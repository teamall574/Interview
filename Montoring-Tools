1-what is Montoring 

>>Collecting, processing, aggregating, and displaying real-time quantitative data about a system, such as query counts and types, error counts and types, processing times, and server lifetimes.
>>The Periodic tracking (for example, daily, weekly, monthly, quarterly, annually) of any activity's progress by systematically gathering and analyzing data and 
information is called Monitoring. like RAM , CPU , Memory , Network satistics or alerts 
>>monitoring on other hand to report or see the overall helath of the system

Why Montoring:-
1-Analyzing long-term trends:-How big is my database and how fast is it growing? How quickly is my daily-active user count growing?
2-Comparing over time and help you to design/tune the system. helps you to rightsize the infra   
3-Alerting
4-project dashboards
5-Retrospective Analysis 
6-what is broken and when and why broken 

2-types of montoring 

1-white box montoring:-Monitoring based on metrics exposed by the internals of the system, including logs, interfaces like the Java Virtual Machine Profiling Interface, or an HTTP handler that emits internal statistics.
>>white box monitoring is the monitoring of application running on the server. monitoring based on exposed by the internals of the system including logs interface
http handler that emits internal statists
2-Block box>>most of the people will focus on black box monitor.block box monitoring refers to the monitoring of server with a focus an area like disk space, memeory usage, 
load average, cpu utilization etc
>>Testing externally visible behavior as a user would see it.

3-what are the four golden things to monitor 

1-Latency:-The time it takes to service a request. It’s important to distinguish between the latency of successful requests and the latency of failed requests. For example, an HTTP 500 error triggered due to loss of connection to a database or other critical backend might be served very quickly; however, as an HTTP 500 error indicates a failed request, factoring 500s into your overall latency might result in misleading calculations. On the other hand, a slow error is even worse than a fast error! Therefore, it’s important to track error latency, as opposed to just filtering out errors 
2-Traffic:-A measure of how much demand is being placed on your system, measured in a high-level system-specific metric. For a web service, this measurement is usually HTTP requests per second, perhaps broken out by the nature of the requests (e.g., static versus dynamic content). For an audio streaming system, this measurement might focus on network I/O rate or concurrent sessions. For a key-value storage system, this measurement might be transactions and retrievals per second.
3-Errors:-The rate of requests that fail, either explicitly (e.g., HTTP 500s), implicitly (for example, an HTTP 200 success response, but coupled with the wrong content), or by policy (for example, "If you committed to one-second response times, any request over one second is an error"). Where protocol response codes are insufficient to express all failure conditions, secondary (internal) protocols may be necessary to track partial failure modes. Monitoring these cases can be drastically different: catching HTTP 500s at your load balancer can do a decent job of catching all completely failed requests, while only end-to-end system tests can detect that you’re serving the wrong content
4-Saturation:-How "full" your service is. A measure of your system fraction, emphasizing the resources that are most constrained (e.g., in a memory-constrained system, show memory; in an I/O-constrained system, show I/O). Note that many systems degrade in performance before they achieve 100% utilization, so having a utilization target is essential.
In complex systems, saturation can be supplemented with higher-level load measurement: can your service properly handle double the traffic, handle only 10% more traffic, or handle even less traffic than it currently receives? For very simple services that have no parameters that alter the complexity of the request (e.g., "Give me a nonce" or "I need a globally unique monotonic integer") that rarely change configuration, a static value from a load test might be adequate. As discussed in the previous paragraph, however, most services need to use indirect signals like CPU utilization or network bandwidth that have a known upper bound. Latency increases are often a leading indicator of saturation. Measuring your 99th percentile response time over some small window (e.g., one minute) can give a very early signal of saturation.
Finally, saturation is also concerned with predictions of impending saturation, such as "It looks like your database will fill its hard drive in 4 hours."

4-What is Prometheus

>>prometheus is a time series database. prometheus is designed to monitor targets servers,databases, standlone virtual machines pretty much everything can be 
monitored with prometheus
>>Prometheus is an open-source linux server montoring tool mainly used for metrics montoring, event montoring, alert montoring etc
>>prometheus uses for own powerful lanaguage "promQL"
>>promotheus uses multiple modes used for graphing and dashboarding support
>>you can create the custom roles and add in prometheus rules you can see in the prometheus this will help you to end complex commands
like what the amount free size memoty and data and cpu utilization
>>like wise you can add the alrets rules also

ex:-
groups:
  - name: custom_rules
    rules:
      - record: node_memory_MemFree_percent
        export: 100*-(100 * node_memory_Memfree_bytes / node_memory_MemTotal_bytes
      - record: node_filesystems_free_percents
        export:
 >>systemctl deamon-reload   -->it wil restart the deamon-sets
        
 record name will take the prometheus site or gui interface it will show the command output like how much free space or cpu utilization accroding to command

5-what is alert in Promethus

>>Alert is used to send notifications to the users or specified persons you can configure if the server fails then you cna send the notifications to user 

6-what is prometheus metrics endpoint

>>Prometheus is a monitoring platform that collects metrics from monitored targets by scraping metrics HTTP endpoints on these targets.

7-what is node exporter

>>node exportes is one of the prometheus exporters which is used to expose servers or systems OS Metrics
>>node exporter helps you to various resources of the system like RAM,CPU Utilzation, Memory Utilization, Disk Space

8-what is Alert manger

>>whenever alerts gone trigger the prometheus should inform to alert manager alert manager will send messages to users
>>The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the 
correct receiver integration such as email, PagerDuty, or OpsGenie. It also takes care of silencing and inhibition of alerts

9-what is observability

>>Observability means assembling all fragments from logs, monitoring tools and organize them in such a way which gives actionable knowledge of the whole environment 
thus creating an insight

Observability=Logging+Montoring+Visualization+Tracing
tools:-
1-dataddog
2-new-relic
3-splunk
4-elastic
5-grafana

10-what is the difference between montoring and observability

>>Monitoring tells you whether a system is working or not
>>Observability lets you ask why its not working

11-what is telemetry

>>Telemetry automatically collects, transmits and measures data from remote sources, using sensors and other devices to collect data. It uses communication systems to transmit the data back to a central location. Subsequently, the data is analyzed to monitor and control the remote system.
>>tel menas remote
>>metron means measure

telmetry data types
1-metrics (cpu and ram)
2-events (system evnets ,application evenets)
3-logs (system logs or network logs or anything)
4-traces (request)
this four types is called MELT

12-what is grafana

>>grafana is a multi-platform open-source analytics and interactive visualization tool it provides charts graph and alert when connected to supported data sources.
it is expandable through a plug-in system

>>dashboard number 14513 is linux
>>dasboard number  14510 is winodws
>>dashboard number 11802 is kubernetes
>>dashboard number 12268 is nginx logs and GEO map dashboard 
>>dashboard number 9614-nginx-ingress-controller
<<dashboard number 1860-node-exporter-full
>>dashboard number 8670-cluster-cost-utilization-metrics
>>dashboard number 13838-kubernetes-overview
>>dashboard number 8588-1-kubernetes-deployment-statefulset-daemonset-metrics
>>dashboard number 4358-elasticsearch
>>dashboard number 139-aws-billing
>>dashboard number 3119 kubernets 
>>dashboard number 10856 kubernets 
>>dashboard number 8685 kubernets 
>>dashboard number 315 kubernets 

1860
405
11074

13-Is it possible to change the port number 

Yes its possible /etc/garafana/grafana.ini

14-What databases work with Grafana?

>>by default grafana installs with sqlite is an embedded database stored in the grafana installtion location 
Grafana was first launched in 2014 and can now connect to a variety of data sources such as MySQL, SQL Server, Graphite, Prometheus, and others. 
This tool is most commonly used to build a time-series data visualization, which illustrates the health or utilization of a resource over time.

15-Is it possible to change grafana database sqlite to mysql 

>>yes its possible it will supports only in this databases postgress or sqlite or mysql 

16-what is grafana latest version

>>9.4.3

17-if we changing one version to another version what should you do 

>>you sholud take the backup of your database in grafana server using mysqldump or dump commands using 

18-What are the features provided by Grafana?

Visualization – Grafana helps us in viewing and understanding our data easily.
Alerting – Grafana is used in supporting numerous number of notification channels such as Slack, Pagerduty, etc.
Annotations – Grafana is used in helping us in annotate or in leaving notes on graphs.
Open Source – Grafana is used in backing an active vibrant community.

19-Is this possible to create the Report in grafana

>>Yes is possible to create the reports in Grafana using OpenNMS Dashboard

20-What is Grafana Cloud?

>>Grafana Cloud is used in observing platform, integrating metrics, tracing and logging with the help of Grafana.It is the best open source observability 
software that consists of Prometheus, Loki and Tempo without installing, maintaining and in scaling our observability stack.

21-What are Grafana Alerts?

>>Grafana Alerts helps in allows in attching rules in our dashboard panels, we can save the dashboard as it extracts the alert rules in a separate alert rule 
storage and schedules it for the evaluation.

Dashboard Alerts consists of 4 components as follows:

Alerting Rule used in creating an alert.
Contact Point used in sending notification when conditions of alerts rule are met.
Notification Policy used in grouping, matching and determining where, how to send notifications.
Silences used in matching and silencing notifications.

22-What are Cloud Logs?

Cloud Loggings helps in storing, searching, analyzing, monitoring and alerting on logging data and events from AWS Web Service and Google Cloud.

23-What is Grafana Dashboard Templates?

>>Grafana Dashboard Templates are used in making our dashboards interactive, we can create dashboard template variables which can be used anywhere in the 
Grafana Dashboard.These variables helps in allowing us in making dynamic changes to the dashboard.

24-What are Cloud Metrics?

>>Cloud Metrics helps in providing insights in our Cloud Computing and also predictive analysis of our system, and in determining when our servers are under load 
and requires provisioning.Cloud Metrics are also used in setting up thresholds through policies which will react in conditions during the cloud operations.
It also allows the cloud in self healing with auto fixing in the event of any operational failures

25-How do you check if the Grafana service is running or not?

You can validate that Grafana is running by checking the service’s status command as given below:

$ sudo systemctl status grafana-server

26-How to configure Grafana, so that dashboards can be viewed without requiring a password?

Make the following small configuration modifications to the default.ini/grafana.ini file to enable anonymous login (Grafana\conf).

27-What Is Graphite Grafana?

Graphite is a monitoring tool yet again. It makes time series data storage and viewing easier. In a data monitoring setup, Graphite is best used as a data source
for Grafana dashboard.
Grafana provides an advanced Graphite query editor that allows us to interact with data using expressions and functions.

28-What is Loki Grafana?

Loki is a Prometheus-inspired horizontally scalable, highly available, multi-tenant log aggregation system. It is intended to be both cost-effective and simple to use.
It uses a set of labels for each log stream rather than indexing the contents of the logs.

29- What Alert Notification Channels does Grafana Support?

Majorly it supports below Alert Notification Channels

Email
Slack
Kafka
Google Hangouts Chat
Microsoft Teams

30-What is difference between Grafana and Kibana?

>>Kibana’s main purpose was to analyze and monitor logs. In the ELK stack, K stands for Kibana. The ElasticSearch team developed Kibana with the goal of 
having a useful tool for monitoring logs. Instead of running Linux commands on the console to identify exceptions in production, simply navigate around and
track the context from the Kibana.

>>Grafana, on the other hand, is developed as a generic monitoring solution that can be used to monitor and analyze pretty much anything. 
This is a very high-level overview of the differences between the two tools.

31-How do you backup Grafana dashboards?

In order to backup your Grafana dashboard you may wish to consider using an open-source tool such as the Grafana backup tool.

This tool uses a Python application to backup not only Grafana dashboards but also backups your settings, folder permissions and alert channel configurations. 
There is also the option to use a managed service such as the platform offered by Logit.io, which offers this service by default as part of our commitment to our users.

32-How do you send data to Grafana?

To forward data to Grafana from different data sources you will need to navigate to “Data Sources” from the configuration menu.

33-What is InfluxDB used for?

>>InfluxDB is a time-series database written in the Go programming language.
>>InfluxDB is used for both the storage and retrieval of time series data and is commonly used by those who wish to perform operations monitoring as well as logs 
and metrics analysis.

34-What can I monitor with Grafana?

Grafana supports graph, singlestat, table, heatmap, and freetext panels, as well as integration with official and community-built plugins (like world map or clock)
and apps that could be visualized, too.

35-What is the difference between Splunk and Grafana?

Grafana is your tool if you need a product only for data visualization without spending a fortune on it. … Splunk is your product if you need a more 
versatile solution that can ingest any kind of data or logs and give you the capabilities to query and search through the data quickly.

36-What are the disadvantages of Grafana?

Cons to Grafana include:

Limitations on dashboard organization and design. Visualization panels are limited to those made available by Grafana Labs and its community. …
No data collection and storage. …
Limited data type visualizations.

37-What is Grafana Enterprise?

>>Grafana Enterprise is a paid version of Grafana that offers capabilities not available in the free source version. Grafana Enterprise gives you access to 
enterprise plugins, which allow you to use your existing data sources in Grafana. This means you can get the most out of your complicated, 
costly monitoring solutions and databases by viewing all of the data in a more efficient and effective manner.

38-What is Grafana Loki and How does it work?

>>Grafana Loki is a log aggregation tool that collects and organizes logs from all of your apps and infrastructure. It offers a unique approach by 
indexing only the metadata of the log lines rather than the complete text

39-Why use Grafana Loki?

Because you may distribute logs in any format, from any source, and through a number of clients, getting started is simple.
With 100 per cent persistence to object storage, you get petabyte-scale, fast throughput, cost-effective & durable storage, and cost-effective & durable storage.
Your log lines can be used to create metrics and alarms.
Ingestion logs have no formatting requirements, providing you more flexibility and the ability to format at query time.
Follow your logs in real-time to see them as they arrive in the system, to have them updated after a set length of time, to see logs for a given day, and so on.
Prometheus, Grafana, and K8s all have native integrations, allowing you to seamlessly move between metrics, logs, and traces.

40-What are Grafana plugins?

1-panel plugins
2-data source plugins
3-app plugins

41-how to save logs in grafana to s3

>>it depends which S3 logging option 70 you choose. If you choose CloudTrail you could export the logs CloudWatch Logs 16 and then import them with our 
Lambda Promtail 136. However, if you choose to store the logs in an S3 bucket we would have to add a Promtail target for S3.

42-What is Graphite DB?

A: Graphite DB is an open-source time-series database. This means that it is able to store time-series data and render graphs based upon this data.

43-what is datadog

>>datadog is a montoring service tool for cloud-scale applications providing monitoring of servers databases tools and services through a SAAS based data analytics
platform
>>it is easy to use
>>providing functionalty
>>it will install an agent in all servers and get the metrics and cpu utilization and events and alrets and network
>>it will check the availbilty and performance and reliability of your application or servers
>>datadog uses go based agent 
>>it will support more than 400 + integartions

44-how to install Prometheus server with direct execution 

>>wget https://github.com/prometheus/prometheus/releases/download/v1.11.0/prometheus-1.11.0.linux-amd64.tar.gz
>>tar xvfz prometheus-1.11.0.linux-amd64.tar.gz
>>sudo mv prometheus-1.11.0.linux-amd64 /usr/local/prometheus
>>sudo ln -s /usr/local/prometheus/prometheus-1.11.0.linux-amd64 /usr/local/prometheus/current
>>vi /etc/systemd/system/prometheus.service
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
ExecStart=/usr/local/prometheus/current/prometheus --config.file=/usr/local/prometheus/current/prometheus.yml
ExecReload=/bin/kill -HUP $MAINPID
Restart=always

[Install]
WantedBy=default.target
>>:wq
>>sudo systemctl start prometheus
>>sudo systemctl enable prometheus

second-type:-

bash sudo vi /etc/yum.repos.d/prometheus.repo
[prometheus] 
name=Prometheus repository 
baseurl=https://packagecloud.io/prometheus-rpm/release/el/$releasever/$basearch 
gpgcheck=1 
enabled=1 
gpgkey=https://packagecloud.io/prometheus-rpm/release/gpgkey
sslverify=1
sslcacert=/etc/pki/tls/certs/ca-bundle.crt
:wq
bash sudo yum makecache
bash sudo yum install prometheus
bash sudo systemctl start prometheus
bash sudo systemctl enable prometheus

third-type:-

>>sudo useradd --no-create-home --shell /bin/false prometheus
>>wget https://github.com/prometheus/prometheus/releases/download/v2.42.0/prometheus-2.42.0.linux-amd64.tar.gz
>>sudo mkdir /etc/prometheus /var/lib/prometheus
>>sudo chown prometheus:prometheus /etc/prometheus /var/lib/prometheus
>>tar -xvzf prometheus-2.42.0.linux-amd64.tar.gz
>>mv prometheus-2.42.0.linux-amd64.tar.gz prometheus

Copy “prometheus” and “promtool” binary and change ownership
>>cp prometheuspackage/prometheus /usr/local/bin/
>>cp prometheuspackage/promtool /usr/local/bin/
>>chown prometheus:prometheus /usr/local/bin/prometheus
>>chown prometheus:prometheus /usr/local/bin/promtool
Copy “consoles” and “console_libraries”
>>cp -r prometheuspackage/consoles /etc/prometheus
>>cp -r prometheuspackage/console_libraries /etc/prometheus
>>chown -R prometheus:prometheus /etc/prometheus/consoles
>>chown -R prometheus:prometheus /etc/prometheus/console_libraries
Configure Prmetheus
vi /etc/prometheus/prometheus.yml

global:
  scrape_interval: 10s

scrape_configs:
  - job_name: 'prometheus_master'
    scrape_interval: 5s
    static_configs:
      - targets: ['SERVER_IP:9090']
:wq

Change the ownership
>>chown prometheus:prometheus /etc/prometheus/prometheus.yml

vi /etc/systemd/system/prometheus.service

[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target
[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/usr/local/bin/prometheus \
--config.file /etc/prometheus/prometheus.yml \
--storage.tsdb.path /var/lib/prometheus/ \
--web.console.templates=/etc/prometheus/consoles \
--web.console.libraries=/etc/prometheus/console_libraries
[Install]
WantedBy=multi-user.target

#start service
systemctl daemon-reload
systemctl start prometheus
systemctl status prometheus

http://SERVER_IP:9090/graph

45-what is promethues querys 

>>node_cpu_seconds_total
>>absent()
>>http_requests_total
>>rate(http_requests_total[5m])[30m:1m]
>>http_requests_total{job=~".*server"}
>>http_requests_total{status!~"4.."}
>>rate(http_requests_total[5m])
>>count by (app) (instance_cpu_time_ns)
>>rate(node_cpu_seconds_total[1m]) * 100
>>celi(rate(node_cpu_seconds_total[1m]) * 100)  -->low 
>>floor(rate(node_cpu_seconds_total[1m]) * 100)  -->high 
>>floor(rate(node_cpu_seconds_total{ mode="idle" }[1m]) * 100)  -->it will show how many cpus you have 
>>avg by (name) (floor(rate(node_cpu_seconds_total{mode="idle"}[1m]) * 100))  -->it will give cpu free space 
celi(100- (100 * node_memory_MemAvailble_bytes / node_memory_MemAvailble_bytes) )  -->it will show the 100 to how much used  

46-when you using ec2_sd-config you want see the system name or labels or tags how you will achive this in Prometheus

>>see videos 61 1:12 mins 
>>you cna achive this using relabel_configs using this will go check you ec2 instance tags it will give you using this snippet

- job name: 'nodes'
  ec2_sd_configs:
    - region: us-east-1
      access_key:
      secret_key:
      port: 9100 
  relabel_configs:
  - source_labels: [_meta_ec2_tag_env]
    target_label: environment  #it will give what label you given this will show that  
  - source_labels: [_meta_ec2_tag_env]
    target_label: name   ##it will giev system name what you created like ex: cart system or frontend System 
>>if you want you filter some servers dont to show this in prometheus  some of the servers dont want montior are any other servers 
  filter:
    name: "tag:project"
    value: ["devlopment-servers"]  

47-what is service discovery in Prometheus

>>In Prometheus, service discovery refers to the automated process of finding and scraping metrics from target hosts that expose Prometheus-compatible endpoints. This eliminates the need to manually 
configure static lists of targets, making your monitoring setup more dynamic and easier to manage.
>>ec2_sd_config automatically monitor you need to give the snippet in Prometheus.yaml file under targets 

- job name: 'nodes'
  ec2_sd_configs:
    - region: us-east-1
      access_key:
      secret_key:
      port: 9100 

this will any node created under us-east-1 automcatically see the nodes you need install the node exporter in all the machines use any script and dont give access key and secret key insted you assign the role
with ec2-read-only-access is enough attach that role Prometheus server    

48-what does prometheus do

>>Prometheus metrics from clients in frequent amount of time as defined scrape config 
>>here client just offers the dataover API Prometheus just collects them 
>>pull can be made by multiple servers 

49-what Prometheus does not do 

>>Raw log and event log is not stored instead you can splunk & elk 
>>Tracing data sampling (instana & jaeger)
>>does not do MachineLearning & Detection 
>>long term stoarge , max of 15 days (use this victoriametrics, thanos )
>>does not support scaling 
>>does not have auth mechanismseither for users or even for nodes also 

50-what Prometheus does 

1-metrics collection and store in time series database 
2-Querying :-enquirying the details  
3-Alerting 
4-Graphing / Trends 

51-what is the difference logging and montoring 

logging:-
1-Capturing Events: Logging involves recording events, activities, and messages generated by applications, services, or systems.
2-Log Entries: Individual records or entries containing relevant information about an event or activity.
3-Log Levels: Different levels (e.g., INFO, WARN, ERROR) to categorize the severity of events.
4-Application logs, server logs, database logs, security logs.
5-Tools:Log management and analysis tools (e.g., Elasticsearch, Logstash, Kibana, Splunk) are used to collect, store, and analyze logs.

montoring:-
1-Proactive Observation: Monitoring involves the continuous observation of a system's health, performance, and behavior.
2-Alerting: Monitors are configured to detect deviations from normal behavior and trigger alerts when predefined thresholds are exceeded.
3-Metrics: Quantitative measurements of system performance (e.g., CPU usage, memory usage, network latency).
4-Alerts: Configured thresholds or conditions that trigger notifications when met.
5-Dashboards: Visualization tools to present key performance indicators and trends
6-Server performance metrics, response times, error rates, resource utilization.
7-Tools:Monitoring solutions (e.g., Prometheus, Grafana, Datadog) collect and analyze metrics, generate alerts, and provide visualization.

difference:-
1-Nature:
Logging: Reactive in nature, used for retrospective analysis and troubleshooting.
Monitoring: Proactive in nature, focused on real-time observation and alerting.
2-Data Type:
Logging: Records detailed information, often in human-readable form, about specific events or transactions.
Monitoring: Emphasizes quantitative metrics, such as response times, error rates, and resource utilization.
3-Use Cases:
Logging: Mainly used for debugging, troubleshooting, and auditing.
Monitoring: Mainly used for real-time visibility, performance optimization, and early detection of issues.
4-Time Frame:
Logging: Captures events and activities over time, usually for post-mortem analysis.
Monitoring: Provides a real-time view of the system's current state and performance.

52-
===============================================Datadog============================================== 
1-what is Datadog 

>>Datadog is multi-cloud and k8s and serverless solution for montoring metrics and events, aggregating logs, visualizing data in dashboard and alerting 
>>Datadog have many pre-build dashboard you can us that like AWS overview it will show the AWS all components you connected are integration accepted 
>>Datado have many Features
1-Integrations
2-Dashboards
3-infrastructure montoring 
4-Log management
5-APM(Application performance montoring)
6-Continous profiles(application performance how much latency)
7-synthetic montoring 
8-network montoring
9-security Monitoring
10-alerts 
11-error tracking 
12-incident management
13-database montoring
14-cloud-montior 

2-what is Integration in Datadog 

>>Integration is used to Integrate with cloud providers for their infra metris (i.e EC2,ELB,DynmoDB,RDS Metrics) or any linux servers 
Integration To AWS :-
- Open the Datadog AWS integration tile. Click the Install button to install this integration.
- Under the Configuration tab, choose Automatically Using CloudFormation. If you already have an attached AWS account, click Add another account first. - If you add another account, give it a different name than the IAM Role you have already registered, because specifying the same name results in access denial.
- Login to the AWS console.
- On the CloudFormation page:
    - Provide your Datadog API key.
    - If you would like to enable Resource Collection (required for some products and features), you must set the CloudSecurityPostureManagementPermissions parameter to “true”.
    - Check the two acknowledgement boxes at the bottom
    - Create a new stack
- Update the Datadog AWS integration tile with the IAM role name and account ID used to create the CloudFormation stack.

3-what are the Integration will support in aws Resources

1-API-Gateway 
2-APP Runner
3-APPStream
4-APPSync
5-Athena
6-Billing
7-Auto-scaling
8-CloudFront
9-Billing 
10-CloudTrail
11-EC2
12-ELB 
13-Elastic Search 
14-EKS 
15-VPC 
16-Stoarge(S3) 
17-Route53
18-WAF 
19-SNS
20-SQS

3-what is Dashboard in Datadog

>>we can create the Dashboard for live and high-resolution dashboard for metrics(CPU & Memory & disk utilization) and events (docker & K8s evenets etc) synchronous mousing across all graphs   
>>Dashboards > Dashboards list

4-what is infrastructure in Datadog

>>infrastructure contain track hosts, containers, pods and serverless functions. quickly visualize your environment, identify outliers, detect usage patterns 

5-what is Infra list Datadog 

>>Infrastructure list shows all of your hosts monitored by Datadog with activity during the last 2 hours(default) and up to 1 week. Search your hosts, or group them by tags

6-what is hostmap in Datadog in Infrastructure

>>Host Maps visualize hosts together on one screen, with metrics made comprehensible through color and shape:
>>Host maap using you can see the how much instance or pods using te cpu or memory usage 

- Host maps enable you to see __distributions of machines in each of your availability zones__ (AZ)
- color of each host is set to represent the percentage of __CPU usage__ on that host, where the color ranges from green (0% utilized) to __orange__ (100% utilized)
__Filter by__ limits the Host Map to a specific subset of an infrastructure. The filter input bar in the top left enables filtering of the Host Map by tags as well as Datadog-provided attributes.

7-what is Configure map in Datadog infrastructure

>>You can see all of your containers together on one screen with customized groupings and filters, as well as metrics made instantly comprehensible through color and shape.

8-what is Live container in datadog under conatiners

>>Datadog Live Containers enables real-time visibility into all containers across your environment.

- monitor the state of pods, deployments
- view resource specifications
- correlate node activity with related logs

9-what is conatiners view in Infrastructure

>>The Containers view includes Scatter Plot and Timeseries views, and a table to better organize your container data by fields such as container name, status, and start time.

10-is it possible to store the logs form datadog to S3 bucket 

>>yes read this article 
https://docs.datadoghq.com/logs/log_configuration/archives/?tab=awss3

5-what is log management in Datadog 

>>in datadog there option availbile log management and aggreagator and visualize logs you can search date and string its very helpful you can check the pod logs and everything 
>>it will provide time series incident also like cloud CloudWatch. you can also patterns  
>>You can collect, analyze, and explore logs from everywhere (service, container, cloud) and can view them in one dashboard.
>>Also, you can __correlate__ logs with metrics (CPU, memory, disk, network etc) and tracing. (AWS Cloud Watch Logs and Metrics can __NOT__ correlate each other)

6-how to Enable Kubernetes Log Collection in Datadog

>>The Datadog Agent can collect logs directly from __container stdout/stderr__ without using a logging driver. 

Datadog Agent deployed by Helm chart can collect logs from containers, and config can be set in yaml.
datadog:
  ## @param logs - object - required
  ## Enable logs agent and provide custom configs
  #
  logs:
    ## @param enabled - boolean - optional - default: false
    ## Enables this to activate Datadog Agent log collection.
    #
    enabled: true

    ## @param containerCollectAll - boolean - optional - default: false
    ## Enable this to allow log collection for all containers.
    #
    containerCollectAll: true
```

Upgrade the chart
```sh
helm upgrade datadog \
  -f overrides.yaml \
  datadog/datadog \
  --set datadog.apiKey=YOUR_API_KEY
```

7-how to Filter, Aggregate, Visualize Logs

>>You can filter by K8s service name, name space, pod name, status, etc from left pane (facets panel, more on later).
>>Based on this, you want to search logs with `Incrementing metric` but want to exclude `acceptance-test`

8-what is Scrub Sensitive Info in Logs before Pushed to Datadog

You can use regex to find patterns (email, credit card, social security #, etc) and srub them from logs.

You specify Datadog log config as k8s annotations at `spec.template.medatada.annotations`:

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: cardpayment
spec:
  selector:
    matchLabels:
      app: cardpayment
  template:
    metadata:
      annotations: # <----- spec.template.medatada.annotations
        ad.datadoghq.com/cardpayment.logs: >-
          [{
            "source": "java",
            "service": "cardpayment",
            "log_processing_rules": [{
              "type": "mask_sequences",
              "name": "mask_credit_cards",
              "replace_placeholder": "[masked_credit_card]",
              "pattern" : "(?:4[0-9]{12}(?:[0-9]{3})?|[25][1-7][0-9]{14}|6(?:011|5[0-9][0-9])[0-9]{12}|3[47][0-9]{13}|3(?:0[0-5]|[68][0-9])[0-9]{11}|(?:2131|1800|35\\d{3})\\d{11})"   // <---- Regex pattern
            }]
          }]          
      labels:
        app: cardpayment
      name: cardpayment
    spec:
      containers:
        - name: cardpayment
          image: cardpayment:latest
```

9-what is log pipeline and how to configure 

>>Pipelines take logs from a wide variety of formats and translate them into a common format in Datadog. Implementing a log pipelines and processing strategy is beneficial as it introduces an attribute naming convention for your organization.

>>With __pipelines__, logs are __parsed and enriched__ by chaining them sequentially through processors. This __extracts__ meaningful __attributes__ from semi-structured text to reuse as __facet__. 
>>Preprocessing of JSON logs__ occurs before logs enter pipeline processing. Preprocessing runs a series of operations based on reserved attributes, such as timestamp, status, host, service, and message.

10-What is Log Index

>>Log Indexes provide fine-grained control over your Log Management budget by allowing you to segment data into value groups for differing retention, quotas, usage monitoring, and billing
>>Index config is about how long to retain what kind of logs based on tags and attributes (i.e. you want to keep logs with `status:error` for 15 days)
>>you can also exclude something 
>>It's critical to exclude some unimportant logs. 
>>For example, if you are using Istio service mesh in K8s cluster, then all the traffic going in and out of pods are intercepted by istio sidecar proxy container. Those istio proxy logs can grow gigantic, so it's important to exclude logs from `service:proxyv2` when indexing.

11-what is Archive Logs

>>Configure your Datadog account to __forward all the logs ingested - whether indexed or not - to a cloud storage__. Keep your logs in a storage-optimized archive for longer periods of time and meet __compliance requirements__ while also keeping __auditability__ for ad hoc investigations, with Rehydration.
>>you can store the logs from datadog to S3 is possible 

12-What is Facets

>>Facets__ are __user-defined tags and attributes__ from your __indexed logs__ to:
- Search upon your logs
- Define log patterns
- Perform Log analytics 
>>The easiest way to create a facet is to add it from the __log side panel__, where most of the facet details—such as the field name or the underlying type of data—are pre-filled and it’s only a matter of double-checking:

6-what is events in Datadog 

>>you can montior evnets from AWS and GCP , AZure ,Docker, K8s and etc. you can filter by user, source, tags, host , status ,protity, and incident 
>>There are more than 100 Datadog integrations support events collection, including Kubernetes, Docker, Jenkins, Chef, Puppet, AWS ECS or Autoscaling, Sentry, and Nagios.
>>For example, if a __docker container__ exits with error, or __K8s pod__ gets terminated due to an error, Datadog will pick up those as events.
>>ec2 terminated
>>helath check failed event in load balancer
>>auto-scaling scaled or scaled in is one event 
>>pod is created is also event 
>>pod is terminated is also event 
>>Kubernetes events are beginning to flow into your Datadog account, and relevant metrics collected by your Agents are tagged with their corresponding cluster level metadata.

7-what is Event Streams

>>The Datadog Events Stream shows an instant view of your infrastructure and services events, to help you troubleshoot issues happening now or in the past.

8-What is StatsD?

>>The easiest way to get your custom application metrics into Datadog is to send them to DogStatsD, a metrics aggregation service bundled with the Datadog Agent
Enable StatsD to collect custom metrics:-
Configure Datadog Helm chart's overrides.yaml:
```sh
dogstatsd:
    port: 8125
    useHostPort: true
    nonLocalTraffic: true
```

Then helm upgrade.

You need to install DogStatsD client library to your code (python, Java, .Net, Go, PHP, etc)

For Python,
```sh
pip install datadog
```

Then
```py
from datadog import initialize, statsd

options = {
    'statsd_host':'127.0.0.1',
    'statsd_port':8125
}

initialize(**options)
```


For [Nodejs](https://docs.datadoghq.com/integrations/node/),
```sh
npm install hot-shots
```

Then in app code,
```js
var StatsD = require('hot-shots');
var dogstatsd = new StatsD();

// Increment a counter.
dogstatsd.increment('page.views')
```

9-What Metrics in K8s Cluster to Monitor?

- https://www.datadoghq.com/blog/monitoring-101-collecting-data/
- https://www.datadoghq.com/blog/eks-cluster-metrics/
- https://www.datadoghq.com/blog/monitoring-kubernetes-performance-metrics/#toc-resource-utilization6

  - node status
    - OutOfDisk
    - Ready
    - MemoryPressure (node memory is too low)
    - PIDPressure (too many running processes)
    - DiskPressure (remaining disk capacity is too low)
    - NetworkUnavailable
  - Desired Pods
  - Current Pods
  - Pod capacity
  - Available Pods
  - Unavailable Pods
- Container and node resource metrics
  ![alt text](../imgs/resource_metrics.png "")
  - Memory requests	
  - Memory limits	
  - Allocatable memory	
  - Memory utilization	
  - CPU requests	
  - CPU limits
  - Allocatable CPU
  - CPU utilization	
  - Disk utilization		
- AWS service metrics

10-- Service Level Indicator (SLI)
- Service Level Objective (SLO)
- Service Level Agreement (SLA)
- Error Budget

7-what is Montiro and alerts 

>>in Datadog alerts based on custom metrics(# of k8s host and pods resource utilization , existance/absense of certain key words in logs or metrics) 
>>you can montior k8s if any usgae happiens we can get the notification you can configure as you want 
>>Metrics can be fetched from existing Dashboard's widgets, or Datadog integration documents (`Data collected` section lists all kinds of metrics)

11-what is Tags

>>Tags are a way of adding dimensions to Datadog telemetries so they can be filtered, aggregated, and compared in Datadog visualizations.
>>The Agent can create and assign tags to all metrics, traces, and logs emitted by a Pod, based on its labels or annotations

Some reserved tags are:
- __host__:	Correlation between metrics, traces, processes, and logs
- __device__:	Segregation of metrics, traces, processes, and logs by device or disk
- __source__:	Span filtering and automated pipeline creation for log management
- __service__:	Scoping of application specific data across metrics, traces, and logs
- __env__:	Scoping of application specific data across metrics, traces, and logs
- __version__:	Scoping of application specific data across metrics, traces, and logs

Why Tags?

> it’s more helpful to look at CPU usage across a __collection of hosts__ that represents a service, rather than CPU usage for __server A or server B__ separately.

12-what are Default Out-of-the-Box Tags Created by Datadog Agent

- __container_id__
- __display_container_name__
- __pod_name__
- __kube_ownerref_name__
- __kube_job__
- __kube_replica_set__
- __kube_service__
- __kube_daemon_set__
- __kube_container_name__
- __kube_namespace__
- __kube_app_name__
- __kube_app_instance__
- __kube_app_version__
- __kube_app_component__
- __kube_app_part_of__
- __kube_app_managed_by__
- __pod_phase__
- __environment__
- __kube_ownerref_kind__
- __kube_deployment__
- __kube_stateful_set__
- __persistentvolumeclaim__
- __kube_cronjob__
- __image_name__
- __short_image__
- __image_tag__

13-what is Unified Service Tagging

>>ties all Datadog telemetry together (hence `unified`), including logs, metrics, infra, through the use of three standard tags: `env`, `service`, and `version`. 
>>With __Unified Service Tagging__, you can 
> View service data __based on environment or version__ in a __unified fashion__ within the Datadog site. Also navigate seamlessly across traces, metrics, and logs with consistent tags
>>This means you can visualize data based on env or version value without switching views. And __correlating metrics, logs, and traces__ are as easy as __clicking tabs on dashboard or right clicking the value__.

--like this you can add tags 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    # this part is for Kubernetes State Metrics
    tags.datadoghq.com/env: "<ENV>"  # <---------- three tags as labels
    tags.datadoghq.com/service: "<SERVICE>"
    tags.datadoghq.com/version: "<VERSION>"

8-what is APM (application performance Monitoring)

>>APM stands for application performance Monitoring and tracing you can scan and monitor application performance(latency, error rates,etc ) by tracing request 
>>you can tarce the status code and http requests and you can see the logs 
>>if you use APM you can see the pod logs and metrics and as wll as tags and network and processes 
>>you can use in kiali and jaeguer in istio like it wil free in k8s when you install  
>>End-to-end request tracing from frontend to database for errors and latency.
>>Notice the use of tags (`service`, `env`, `version`) as labels in K8s deployment yaml (Refer to ch9.3 How to Enable Unified Service Tagging)

There are two ways:
- automatically with the __Datadog Admission Controller__, or
- manually; the application container needs the `DD_AGENT_HOST` environment variable that points to `status.hostIP`


__Automatically__ let [Datadog Admission Controller](https://docs.datadoghq.com/agent/cluster_agent/admission_controller/) inject `DD_AGENT_HOST` and `DD_ENTITY_ID` envs to container to configure DogStatsD and APM tracer libraries into the user’s application containers.

DAC will also inject Datadog standard tags or unified service tags (`env`, `service`, `version`) from application labels into the container environment variables.

```yaml
clusterAgent:
[...]
  ## @param admissionController - object - required
  ## Enable the admissionController to automatically inject APM and
  ## DogStatsD config and standard tags (env, service, version) into
  ## your pods
  #
  admissionController:
    enabled: true # <---------

    ## @param mutateUnlabelled - boolean - optional
    ## Enable injecting config without having the pod label:
    ## admission.datadoghq.com/enabled="true"
    #
    mutateUnlabelled: true  # <---------
```

10-what is live search 

With the __APM Live Search__ you can:
- Monitor that a new deployment went smoothly by filtering on `version_id` of all tags.
- View outage-related information in real time by searching 100% of ingested traces for a particular org_id or customer_id that is associated with a problematic child span.
- Check if a process has correctly started by typing process_id and autocompleting the new process ID as a tag on child spans.
- Monitor load test and performance impact on your endpoints by filtering on the duration of a child resource.
- Run one-click search queries on any span or tag directly from the trace panel view.
- Add, remove, and sort columns from span tags for a customized view.

11-what is Create Custom Retention Filter

>>Ingested and kept for __15 mins__ for live search > Indexed using custom tags in retention filter for __15 days__
>>To ensure all __production errors__ are retained and available for search and analytics for 15 days, create a 100 percent retention filter scoped to `env:prod` and `status:error`:
>>For 30 days, intelligent retention retains:
- A representative selection of __errors__, ensuring error diversity (for example, response code 400s, 500s).
- __High Latency__ in the different quartiles p75, p90, p95.
- All Resources with any traffic will have associated Traces in the past for any time window selection.
- True maximum duration trace for each time window.

12-How to Connect Logs and Traces side-by-side

>>Correlation between Datadog APM and Datadog Log Management is improved by the injection of __trace IDs, span IDs__, env, service, and version as attributes in your logs.

Enable injection with the environment variable `DD_LOGS_INJECTION=true` in k8s deployment yaml
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    tags.datadoghq.com/env: "<environment>"
    tags.datadoghq.com/service: "<service>"
    tags.datadoghq.com/version: "<version>"
spec:
  template:
    metadata:
      labels:
        tags.datadoghq.com/env: "<environment>"
        tags.datadoghq.com/service: "<service>"
        tags.datadoghq.com/version: "<version>"
    spec:
      containers:
        - env:
            - name: DD_AGENT_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: DD_ENV
              valueFrom:
                fieldRef:
                  fieldPath: metadata.labels['tags.datadoghq.com/env']
            - name: DD_VERSION
              valueFrom:
                fieldRef:
                  fieldPath: metadata.labels['tags.datadoghq.com/version']
            - name: DD_LOGS_INJECTION  # <--- set this env to inject trace & span IDs to container. Ref: https://docs.datadoghq.com/tracing/connect_logs_and_traces/nodejs/
              value: "true"
```
9-what is synthetic in Datadog

>>Monitor and test endpoints (HTTP, SSL, DNS, TCP, and ICMP) for SLAs and track performance issues
>>it is like Monitor and test public endpoints SLAs and track performance issues 
>>you can create like acceptance test and endpoint testing and you can cutomize the frequency testing 
>>you can see the uptime and server Responce and helath check and you can breaddkdown latency breakdwon  
>>Synthetic tests allow you to observe how your systems and applications are performing using simulated requests and actions from around the globe. Datadog tracks the performance of your webpages and APIs from the backend to the frontend, and at various network levels (HTTP, TCP, SSL, DNS, and ICMP) in a controlled and stable way,

10-what is network monitoring 

>>you can visualize network traffic flow like kiali dashboard 
>>where one application pod sending another pod 
>>you can see the latency error rate and request persecond

11-how to setup the Datadog in kubernetes cluster

>>when you go the datadog dashboard it will give helm you can insatll then install 
>>when you configure datadog it will configure the demaonset in k8s and it will eploys kube-state-metrics and cluster agent is also added 

helm repo add datadog https://helm.datdoghq.com 
helm repo add stable https://charts.helm.sh/stable 
helm repo update 
helm show values datadog/datadog > overrides.yaml 
helm install datadog -f override.yaml --set datadog.apikey='hjgp0a09a8vb0h8' datadog/datadog 
>>if you want change something in override.yaml then upgarde command you need to execute 
helm upgrade datadog -f  override.yaml --set datadog.apikey='hjgp0a09a8vb0h8' datadog/datadog

>>then go to datadog install kueernetes 
>>then do to dashboard list you will see the kubernetes dashboard you can select all types of dashboard availble 
>>its premade dashboards you can use that kubernetes overview it is very helpful it will show the clusters namespaces nodes service deployments pods containers deamonsets 
>>

12-what is data collected by Datadog Agent 

1-CPU             - System  - All Platforms 
2-Disk            - Disk    - All Platforms
3-Docker          - Docker  - Docker  
4-File Handle     - System  - All Except MAC
5-IO              - System  - All Platforms
6-Load            - System  - All except winodws 
7-Memory          - System  - All Platforms
8-Network         - Network - All Platforms 
9-Uptime          - System  - All Platforms

13-what are the Agents Architecture or Two main components 

>>The two main components to the agent process is 
1-Collector: Collect and checks metrics every 15 seconds 
2-Forwarder: Send payload over Https to Datadog 
thre are two more optional agents that can be enables (this will be insatlled in as side car in datadog angent inside 3/3 pods in single pod )
APM Agnet or (process Agnet): it is a process to collect the traces 
Process Agent: its a process to collect live process information. By default it only collects availbile containers otherwise it is disabled  

14-how much memory and disk or anything will use datadog agent 

>>in Agent test version 
CPU: 0.12 of the CPU used on average 
Memory: 60MB of RAM used 
Network Bandwidth: 86B/s | 260 B/S
Disk: linux 350MB to 400MB depending on the distribution 
      Windows: 260MB  

15-how to save cost in the production avoid collecting logs 

>>Datadog Agent auto-discovers all containers availbile by default. To restrict its discovery permieter and limit data collection to subset of conatiners only include or exclude them through a dedicated configuration
>>you might want to avoid logs from irrelevent containers such as coredns  , efs-csi-node, kube-proxy, and metrics-server in kube-system to save cost 
>>you definitely want to avoid collecting logs rom istio proxy container to save cost 
>>to exclude containers you can use these envs:-
vi override.yaml 
env:   --under you can add exclude or include 
  #- name: DD_CONTAINER_EXCLUDE
   # value: "image:docker.io/istio/proxyv2 kube_namespace:kube-system kuber_namespace:istio-system kube_namespace:prometheus 
  - name: DD_CONTAINER_EXCLUDE_LOGS
    value: "image:docker.io/istio/proxyv2 kube_namespace:kube-system kube_namespace:istio-system kube_namespace:prometheus kube_namespace:grafana kube_namespace:kubernetes-dashboard kube_namespace:jenkins kube_namespace:kube-node-lease kube_namespace:kube-public image:gcr.io/datadoghq/agent image:gcr.io/datadoghq/cluster-agent"
  - name: DD_CONTAINER_INCLUDE_LOGS
    value: "image:us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler"

kubectl get po -n kube-system  -->this things i dont need to montior 

https://www.cloudzero.com/blog/datadog-cost-optimization/   -->read this better understanding 

16-what is the use of agent in datadog installed in k8s 

>>before agent we install datadog in k8s if you 4 server four deaonsets installed in each node if nodes increase deamonsets wil increase this all deamonsets will report to the k8s API-Server it will increase the server load of API-Server 
>>Previously, every worker node in the cluster ran a Datadog Agent (__decentralized__) that collected data from two sources:- kubelet, a local daemon that creates the workload on a node - cluster’s control plane, which consists of the API server, the scheduler, the controller manager, and etcd
>>This led to __increased load__ for API server and etcd when more and more k8s nodes join the cluster.

>>after agent arrival in k8s we install deamonsets and agent all the demaonset report the agnet teh agent report the API-Server it will reduces the burden or traffic of API-server in K8s 
>>Cluster Agent provides a streamlined, __centralized__ approach to collecting cluster level monitoring data. By acting as a __proxy__ between the API server and node-based Agents, the Cluster Agent helps to alleviate server load. It also relays cluster level metadata to node-based Agents, allowing them to enrich the metadata of locally collected metrics.
>>You can see that cluster agent pod is proxying on behalf of other datadog agent pods:

- Alleviate the impact of Agents on API server.
- Isolate node-based Agents to their respective nodes, reducing RBAC rules to solely read metrics and metadata from the kubelet.
- Provide __cluster level metadata that can only be found in the API server__ to the Node Agents, in order for them to enrich the metadata of the locally collected metrics.
- Enable the collection of cluster level data, such as the monitoring of services or SPOF and events.
- Leverage horizontal pod autoscaling with custom Kubernetes metrics. See the guide for more details about this feature.

