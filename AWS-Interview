vpc
ec2
vpc perring
tarnsit gateway
ebs volumes
efs 
s3
nat gateway
iam 
route53
vpc endpoint
cloudfront(CDN)
load-balancer
auto-scaling
elastic beanstalk
subnets
routetable
nacl
secuity groups
sns
elasticIP
AWS-firewall
rds
elasticCache
cloudwatch
lambda
cloudtrail
awsconfig
global acclerator
session manager
site-to-site-vpn
point-to-site-vpn
direct vpn
openvpn
control lower
cloud-formation
coginto
SSM->service manager
service catalogue
ECS
EKS
ECR
AWS FARGATE
SQS
SES
INSpector
parameterstore
secret-manager

APPLICATION-----------------------------------------------| These two is SAAS
DATA------------------------------------------------------|
RUNTIME                              |
MIDDLE-WARE                          |
OPERATING SYSTEM ----                |------>This all is PAAS like MySQL, Azure app service
VIRTUALIZATION      |                |
SERVER              |----------------|------------->This all are IAAS ex:-AWS
STORAGE             |                |
NETWORK-------------------------------

1 - what is IAAS

>>Infrastructure as a service (IaaS) is also known as Hardware as a service. the cloud provider provides the Infrastructure to the developers
like network, storage, virtualization, and servers can managed by the cloud provider
>> This is not a physical thing. it will provide only infrastructure. like VMS, compute, memory, OS, network, storage, firewall. things you have full control over virtual machines
ex:- Virtual machines 

>> Users are managed by the operating system and middleware, and runtime data applications all are managed by developers
>>by using IAAS to develop the application is very hard compared to PAAS
Advantages:-
1-controlling is high because you controlled
2-cost saving
3-scalibility. 
4-Faster time to market
5-expand as you grow
6-tight security
7-anytime any where access
Disadvantages:-
1-no internet no work
2-privacy is limited
3-virtualization depend

2 - what PAAS
 
>> This is not a physical thing. the PAAS will give a platform for your application. for example you have an appliation or java or dot net application
there is no database person in your team you can ask the cloud service provide he will give the database and mantainance. things you dont have full control on virtual machines
>>Platform as a Service (PaaS) is a complete cloud environment that includes everything developers need to build, run, and manage applications—from servers and 
operating systems to all the networking, storage, middleware, tools, and more.

>>the disadvantage you can't customize the things in you can't troubleshoot and changes you can trade-off or compromise. you can customize but limited things only
ex:- Azure sql and App-service it is managed by the Azure you can't control anything
  
3 - what is SAAS

>> This is not a physical thing. it is a readymade product. like Office 365 or google apps or salesforce .it will give full software service. its controling on you it is very limited 
>>SAAS is a pre-existed application that applications are accessible through internet connection and web-browsers like google chrome, gmail, mozila browser, salesforce

4-what is cloud-computing

>>Cloud computing is the on-demand delivery of IT resources over the Internet with pay-as-you-go pricing. Instead of buying, owning, and maintaining physical data centers and servers,
you can access technology services, such as computing power, storage, and databases, on an as-needed basis from a cloud provider like Amazon Web Services (AWS).

5-what is Total upfront,Partial upfront

>>Upfront:-your paying bill first then he will give some discount 
>>Partial upfront:- you can pay 50% of bill and then you can use remaing amount will pay after the month end bill generation time
>>No Upfront:- this is on-demand systems

6-what is 1-tier application

>>In 1-tier architecture, the data is directly provided to the user and that user can directly use the database through the computer. Any changes or updates that are done will reflect 
directly to the database

7-what is 2-tier application

>>Two-tier architecture. In a two-tier architecture, the client is on the first tier. The database server and web application server reside on the same server machine, which is the second tier.
>>This second tier serves the data and executes the business logic for the web application.

8-can you explain 3tier architecture or elaborate them

>> First we have external load balancer and an internal load balancer then the external load balancer is open to the internet
>>through route53 and the internal load balancer is between the app and the web and the app is connected to the database

>> Using public and private subnets

9-what is Front-end 

>>A front-end developer builds the front-end portion of websites and web applications—the part users see and interact with. A front-end developer creates websites and applications using 
web languages such as HTML, CSS, and JavaScript that allow users to access and interact with the site or app
HTML, CSS, javascript, angular, react-js, Boot-strap, vue-js, Django, Swift, nodejs
port number 80 or 443
>> You will see the visual of the application

10-what is backend

>>Back-end development means working on server-side software, which focuses on everything you can't see on a website. Back-end developers ensure the website performs correctly, focusing 
on databases, back-end logic, application programming interface (APIs), architecture, and servers.
java, python, .net, PHP, Nodejs, Go, C#
the port number is 8080 or anything
>>java .net python  working on server-side software, which focuses on everything you can't see on a website

11-what is database

>>Databases often store information about people, such as customers or users. For example, social media platforms use databases to store user information, such as names, email addresses 
and user behavior. The data is used to recommend content to users and improve the user experience
mysql, RDS, Nosql, mongoDB, Oracle, PostgresSQl 
port number is 3306 or any other port 
>>NO-Sql will store the images and data using mongodb

12-how would you know which library is using in code

>>we run before packing the application we can run unit testing and compile the code using maven tool
>>we dont know which librarys using so that why we combine source code + librarys create into war or jar file
>>we can check the pom.xml all the librays and it contains information about the project and configuration details plugins goals and versions we can check only pom.xml
the build toolis 
java: maven  -->pom.xml
Dotnet: MSBuild  -->project.json
node js: NPM   -->package.json
python: pip    -->reqirments.txt           -->this files contains library and dependies list
python: no need to use build tool in python it is not inter-preted language. it will automaticaly install required packages using pip module

13-what is library

>>without library application will not work. library is nothig but its 3rd party application we will use the librarys to make up or development the application is more easy

14-what is this

>>if my project is java i will take maven as build tool
>>if my project is Android i will take graddle as build tool
>> Weservers are nginx, apache2,Caddy,monkey http server which will directly take the traffic which direcly rout to app servers
apache2 -->in ubuntu 80 port
httpd    -->in RHEL 80 port
nginx  -->80 port
>>app servers is an tomcat,JBoss,WildFly,Oracle weblogic
tomcat  -->8080 port 

jenkins   -->8080
sonarqube   -->9000
jfrog    -->8081
Nexus    -->8081

15-How many ways to Access the AWS

>>By using the management console
>>access_kay and secret_key
>>AWS SDKs(software development Kits): using python boto3
>>AWS SSO(single sign-on) using anji@wipro.com
>>AWS Federation like SAML or Okta
>>AWS STS(Security token Service)

16-your unable to SSH into a Linux server what steps would you take to troubleshoot this

>> Check if the server is running and network accessible 
>> You can use the ping command to see if the server is reachable
>> Check if the SSH port is open or not
>> Check the firewall rules
>> Check ssh configuration

17-how much time would take to launch and install

>>Linux: minimum 3 minutes maximum 4 minutes (it requires a minimum of 8 GB storage)
>>Windows: minimum 4 minutes maximum 5 minutes (it requires minimum 30 GB storage)

18-Can we recover the EC2 instance when we have lost the key?

>>Yes, it is possible to recover an EC2 instance when you have lost the key. You can create a new key pair and then use it to connect to the instance.
Alternatively, you can also use other authentication methods such as user name and password, or even IAM roles to access the instance.
However, if you have encrypted the root volume of the instance using the lost key pair, you will not be able to recover the data on that volume.
>> If you lost the private key no longer need to take that form anywhere instead you can create one more keypair if you have the SSM or IAM option then log in to that new instance copy that public key 
pass that key to the old instance using SSM in this place ~/.ssh/authorized_keys 

19-how to restore the login into ec2 if the pem file lost

>>Use a backup or a copy of the original .pem file:

If you have a backup or a copy of the original .pem file stored securely, you can use it to regain access to the EC2 instance.
Locate the backup or copy of the .pem file and follow the standard procedure to connect to the EC2 instance using the private key.

>>Launch a new EC2 instance and attach the original volume:

Launch a new EC2 instance and make sure to select the same availability zone and the same or compatible Amazon Machine Image (AMI) as the original instance.
During the instance launch, you can create a new key pair or choose an existing key pair to associate with the new instance.
Once the new instance is running, stop it and detach its root volume.
Detach the root volume from the original EC2 instance (the one for which you lost the .pem file).
Attach the original volume as a data volume to the new instance.
Start the new instance and access it using the new key pair. You can then mount the original volume and retrieve any necessary files.

>>Use EC2 Instance Connect or Session Manager:

If you have EC2 Instance Connect or AWS Systems Manager Session Manager set up on the EC2 instance, you may be able to regain access without the need for the .pem file.
EC2 Instance Connect allows you to connect to your EC2 instance using the AWS Management Console or the AWS CLI without a key pair. Check if EC2 Instance Connect is enabled for your
instance and follow the instructions provided by AWS. AWS Systems Manager Session Manager allows you to establish a secure shell (SSH) session to your instance through the AWS Management 
Console or the AWS CLI. If Session Manager is enabled for your instance, you can connect to it without needing the .pem file.

20-How to upgrade or downgrade a system with zero downtime?

>>To upgrade or downgrade a system with zero downtime, you can use the following steps:
1-Set up a load balancer in front of your system.
2-Deploy the new version of your system to a new set of servers.
3-Add the new servers to the load balancer and verify that they are healthy.
4-Remove the old servers from the load balancer.
5-Monitor the system to ensure that everything is running smoothly.
By following this process, you can ensure that your system remains available throughout the upgrade or downgrade process.

21-How do you upgrade or downgrade a system with near-zero downtime?

1-Auto Scaling Groups: Use Auto Scaling Groups to manage the desired capacity of your instances. When upgrading or downgrading, you can change the launch configuration or the Amazon Machine Image (AMI) 
associated with the Auto Scaling Group. Auto Scaling Groups automatically replace instances to maintain the desired capacity, minimizing downtime.
2-Blue-Green Deployments: Implement a blue-green deployment strategy, where you maintain two identical environments (blue and green). You direct the traffic to the active environment (blue), 
and when you're ready to upgrade, you switch traffic to the new environment (green). This approach allows for a smooth transition with minimal downtime.
3-Elastic Load Balancers (ELB): Use Elastic Load Balancers to distribute traffic across multiple instances. When upgrading, you can take instances out of service, perform the necessary upgrades, 
and reintroduce them to the ELB. This ensures continuous traffic flow while updating the system.
4-Amazon RDS Read Replicas: If your system involves a database, consider using Amazon RDS Read Replicas. During upgrades, you can promote a read replica to the primary instance with minimal downtime.
This helps maintain database availability during the upgrade process.
5-Amazon EC2 Instance Termination Protection: When upgrading instances in an Auto Scaling Group, enable termination protection for instances you are about to replace. 
This prevents accidental termination of instances during the upgrade process.
6-Amazon Route 53 DNS Failover: If you are implementing a blue-green deployment, use Amazon Route 53 DNS failover to switch traffic between the blue and green environments. 
This allows you to control the traffic flow during the upgrade or downgrade process.
7-AWS Lambda: Use AWS Lambda for serverless computing. If your application architecture includes serverless components, updating Lambda functions can be done without downtime as each function execution
is stateless.
8-Snapshot and AMI Backup: Before making changes, take snapshots of EBS volumes and create AMIs of instances. This provides a backup in case something goes wrong during the upgrade or downgrade process.

>>You can upgrade or downgrade a system with near-zero downtime using the following steps of migration:

Open EC2 console
Choose Operating System AMI
Launch an instance with the new instance type
Install all the updates
Install applications
Test the instance to see if it’s working
If working, deploy the new instance and replace the older instance
Once it’s deployed, you can upgrade or downgrade the system with near-zero downtime.

22-what is the secret_key and access_key

>>Access keys consist of an access key ID and secret access key, which are used to sign programmatic requests that you make to AWS. 
If you don't have access keys, you can create them by using the IAM console

>>AWS access key ID is a form of unique user/account identifier. AWS secret key is like a private key. When AWS CLI sends an API request, 
the payload is signed by generating an HMAC with the secret key as the key

23-Is node.js will supoort backend and frontend

>> Yes node.js will support frontend and backend also
3 tier arch
database:- MongoDB Postgres or MySQL
frontend-web-application:- nodejs (webapp)
backend-end-application:-  NODEJS (app-servers) or java

24-what is virtualization

>>Virtualization uses software to create an abstraction layer over computer hardware that allows the hardware elements of a single computer—processors, memory, 
storage and more—to be divided into multiple virtual computers, commonly called virtual machines (VMs)

25-What are the different types of virtualization in AWS, and what are the differences between them?

>>Hardware Virtual Machine (HVM)
It is fully virtualized hardware, where all the virtual machines act separately from each other. These virtual machines boot by executing a master boot record 
in the root block device of your image.
>>Paravirtualization (PV)
Paravirtualization-GRUB is the bootloader that boots the PV AMIs. The PV-GRUB chain loads the kernel specified in the menu.
>>Paravirtualization on HVM
PV on HVM helps operating systems take advantage of storage and network I/O available through the host.

26-Name some of the AWS services that are not region-specific

>>AWS services that are not region-specific are:

IAM
Route 53
Web Application Firewall 
CloudFront

27-Define and explain the three basic types of cloud services and the AWS products that are built based on them.

--The three basic types of cloud services are:

>>Computing: - These include EC2, Elastic Beanstalk, Lambda, Auto-Scaling, and Lightsail.
>>Storage - These include S3-simple service storage, Glacier, Elastic Block Storage, and Elastic File System.
>>Networking - These include VPC, Amazon CloudFront, Route53

28-what is CIDR 

>>CIDR stands for Classless Inter-Domain Routing. It is a method used to allocate and specify IP addresses and their routing on the Internet. CIDR replaces the older system that used classes A, B, and C to 
define network addresses, providing a more flexible and efficient way to allocate IP addresses.
>>CIDR is used to create a subnet that will reduce the wastage of the IPS 

29-what is VPC

>>Amazon Virtual Private Cloud (Amazon VPC) gives you full control over your virtual networking environment, including resource placement, connectivity, and security. 

>>It is a region-dependent
>>Vpc is a logical boundary
>>vpc id is globally unique
>>you can stop the particular ip one ec2 to another ip through routing table in peering

30-what is Subnet

>>A subnet is a range of IP addresses in your VPC. You launch AWS resources, such as Amazon EC2 instances, into your subnets. You can connect a subnet to the internet, other VPCs,
and your own data centers, and route traffic to and from your subnets using route tables.

31-what is Route tables in AWS

>>A Route table defines how the communication should happen within the subnet and outside of the subnet
>> We can create custom route tables per each subnet design for the network traffic
>> The route table plays a key role when you want to route traffic to 
>> It is also subnet level
-other Vnets
-virtual network gateways
-virtual appliances
-Internet
Use-Cases:-
1-Routing Decisions: The primary purpose of a route table is to make routing decisions for traffic within a VPC. It determines where the traffic within a subnet should be directed. 
This includes local traffic within the VPC and outbound traffic to the internet or other VPCs.
2-Default Routes: Route tables can include default routes, also known as "0.0.0.0/0" routes. These routes define the default path for traffic that doesn't match any specific route in the table.
For example, the default route might point to an Internet Gateway for outbound internet-bound traffic.
3-Internet Gateway Association: When you want your instances in a subnet to communicate with the Internet, you associate the subnet's route table with an Internet Gateway. 
This enables the routing of traffic to and from the internet.
4-Virtual Private Gateway Association: If you have a Virtual Private Network (VPN) or Direct Connect connection to your on-premises network, you can associate a subnet's route table with a
Virtual Private Gateway. This allows traffic to flow between your VPC and your on-premises network.
5-Peering Connection Association: When you have VPC peering connections between multiple VPCs, you can associate a subnet's route table with a peering connection. This allows traffic to flow between 
the peered VPCs.
6-Egress-Only Internet Gateway Association (for IPv6): When using IPv6, you can associate a subnet's route table with an egress-only Internet Gateway. This allows IPv6-enabled instances in the subnet 
to communicate with the internet over IPv6.
7-Custom Routes: Route tables can include custom routes to define specific paths for traffic. This is useful for scenarios where you want to route traffic to specific destinations through particular 
network devices, such as a transit gateway or specific instances acting as routers.
8-Multiple Route Tables per VPC: A VPC can have multiple route tables, and subnets can be associated with different route tables. This allows for flexibility in defining routing policies for different
subsets of resources within a VPC.

32-What is the relation between the Availability Zone and Region?

>>AWS regions are separate geographical areas, like the US-West 1 (North California) and Asia South (Mumbai). On the other hand, availability zones
are the areas that are present inside the regions. These are generally isolated zones that can replicate themselves whenever required.
>>The AWS Cloud spans 99 Availability Zones within 31 geographic regions
>>20 per Region you can request more
>> You can create up to 100 buckets in each of your AWS accounts. If you need more buckets, you can increase your account bucket limit to a 
maximum of 1,000 buckets by submitting a service limit increase.

33-what is nat-gateway

>>A NAT gateway is a Network Address Translation (NAT) service. it is used in a private system to connect over the internet to take patches or updates nat gateway is an intelligent device using 
source destination packets. it will send only traffic through the public instance but not allow outside the traffic to your private instances. when you enable nat gateway you must select the 
public subnet, not the private subnet
 
34-what is NAT-Instance

>>NAT-A NAT (Network Address Translation) instance is, like the same nat-instance gateway but the difference is the amount is big in the nat gateway when compared to the nat instance

35-What are the differences between NAT Gateways and NAT Instances?
 
NAT-Gateway
1 - nat gateway managed by AWS 
2 - nat gateway can't assign security groups
3 - nat gateway highly available
4 - nat gateway bandwidth is up to 45 Mbps
5 - nat gateway performance is good

NAT-Instance
1 - NAT instance managed by you
2 - NAT instance can assign security groups
3 - NAt instance is also highly available
4 - NAT instance bandwidth depends on instance bandwidth
5 - NAT instance performance compared to NAT gateway is average

36-what is the Vpc End-point

>>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services. instance in your VPC does not require a public IP address to communicate the AWS resources
>>S3
>>DynmoDB
>>Also create Interfaces like EkS, Auto-Scaling, Cloud-Trail, Cloud-formation
use cases of endpoints 
1-Replacing NAT gateway with interface endpoints
2-Adding gateways endpoint for s3 intensive Workload
3-private access to saas based applications 

37-what is gateway endpoint

>> Gateway end-point is a gateway that you specify as a target for a route in your table for traffic destined to a supported AWS service. It serves as a bridge between different networks or 
systems and enables communication and data transfer between them. Gateways can be used for various purposes, such as security, monitoring, load balancing, and routing. Examples of 
cloud gateways include API gateways, database gateways, and messaging gateways.
Amazon S3
DynmoDB
>>Must be inside the VPC to be used 
>>Use s3 public IP address
>>Use same S3 DNS names
>>No access from on-permise 
>> Cross-region access not allowed
>>Not billed 
>>Assocaited on a VPC level 
>>if you create DynmoDB or S3 endpoint automatically updated route table 

>> Gateway is a hardware device that gives you entry and exit points in your network
>>gateway end-point is a gateway that you specific as a target for a route in your table for traffic destined to a supported AWS service

38-what is the interface endpoint 

>> It will create an interface to connecting privately through the AWS Interface endpoint securely   
>>An Interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destinated to a supported service 
services:-
Amazon API Gateway 
Amazon AppStream
Amazon APP Mesh 
>>Support most of the AWS Services 
>>It is ENI(Elastic Network Interface) which is attached with a security group 
>>use a private IP address from your VPC to s3 
>>Requires endpoint specific S3 DNS names
>>Allow access from on-premise 
>>Allow cross-region access through VPC peering or Tranist gateway 
>>billed
>>Associated on subnet level 
>> It will create a separate endpoint for service then you can connect the Interface resource or AWS Resources  

39-what is VPC Peering

>>A VPC peering  is a networking connection between two VPCs or two differnet networks. that enables you to route traffic between two different networks or two Vpcs. 
once perring is completed they can communicate with each other as if they are within the same network. There are no charges in VPC peering
>> You can also create the VPC peering one region Mumbai to another region Ohio like this
>> You can also VPC peering one account to another account
Use-cases:-
1-Cross-Account Connectivity: When different AWS accounts need to communicate with each other securely.
2-Isolation of Environments: Organizations often use separate VPCs for different environments, such as development, testing, and production, to isolate resources and environments from each other.
3-Shared Resources: Multiple VPCs within the same AWS account may need to share resources, such as databases, application servers, or other services.
4-Hub-and-Spoke Architecture: Implementing a hub-and-spoke architecture where a central VPC (hub) connects to multiple spoke VPCs.
5-Data Sharing: Sharing data securely between VPCs.
6-Reduced costs: Peering can help to reduce costs by eliminating the need to pay transit fees to an ISP.
7-Improved performance: Peering can improve performance by reducing latency and jitter.
8-Increased reliability: Peering can increase reliability by providing an alternative path for traffic to flow in the event of an outage.

===disadvantages of peering

>> An increase in the VPC will increase in the VPC peerings
>> Each VPC needs a VPG for VPN connections

40-What is a transit gateway?

>>Transit gateway advance layer of VPC peering, in VPC peering if we create 3 or more VPCs we need to interconnect using VPC peering if we increase VPC peering it is
difficult to identify the traffic and get confused in VPC peering. so that we use a transit gateway we create one transit gateway so we can interconnect the
VPC in a region is easy to understand and easy to use. we can interconnect different regions in also easy way. but it is chargeable

>>Transit gateway advance layer of VPC peering. the transit gateway simplifies your network connection and puts an end to complex peering relationships.
Transit Gateway acts as a highly scalable cloud router—each new connection is made only once

41-what is NACL

>>A network access control list (NACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.
You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.

>>when you created vpc automatically create nacl 
>>if anyone want say to change something nacl first ask send a mail 
>>don't do changes in nacl in production environment
>>need to add every rule in inbound and outbound  -->if you add ssh in inbound it will not allow so you need to add outbound also

42-what is security group is stateful or stateless in aws

>>Security groups are stateful—if you send a request from your instance, the response traffic for that request is allowed to flow in regardless of 
inbound security group rules. For VPC security groups, this also means that responses to allowed inbound traffic are allowed to flow out, regardless of 
outbound rules.

43-What is the difference between security group and NACL?

>>Security Group is applied to an instance only when you specify a security group while launching an instance.NACL has applied automatically to all
the instances which are associated with an instance. is the second layer of defense.
>>by default in security group there no deny option it will allow the traffic how you will deny the ip . in that place you can allow specific ip only
>>security-group is stateful
>>if you use security group it will be only server level only. security group only have allow the ips only

>>NACL It is the first layer of defense.
>>NACL only for subnets by default automatically creates a VPC when we create a VPC and NCL is Stateless you need to enable inbound and outbound also if you are enable inbound not 
enabled outboind it will not work and it will sit infornt of security group 
>>if you add one port in the inbound rule you need to add an ephemeral port in the outbound otherwise it won't work
>>it will applied by subnet any server is there in subnet this will apply to that server or subnet level  using NACL.NACL have allow and deny option

44-How many security groups can be attached to an EC2 instance?

>>five

45-where we can attach security groups

>>EC2
>>Load-Balancer  (all)
>>Not for subnets

46-What is the difference between NACL and Subnet ? Explain with a use case ?

>>For example, I want to design a security architecture, I would use a combination of NACLs and security groups. At the subnet level, I would configure NACLs to enforce inbound and
outbound traffic restrictions based on source and destination IP addresses, ports, and protocols. NACLs are stateless and can provide an additional layer of defense by filtering traffic 
at the subnet boundary.At the instance level, I would leverage security groups to control inbound and outbound traffic. Security groups are stateful and operate at the instance level. 
By carefully defining security group rules, I can allow or deny specific traffic to and from the instances based on the application's security requirements.By combining NACLs and 
security groups, I can achieve granular security controls at both the network and instance level, providing defense-in-depth for the sensitive application.

47- What is Amazon EC2?

>>Ec2 Means elastic cloud computing where you are requesting AWS the cloud provider to give a virtual machine or virtual server which is elastic in nature this term is called 
compute that we called EC2
>> Elastic is nothing but scaled up and scaled-down any time is called elastic
ex:-Elastic cloud compute, elastic beanstalk, EKS, ECR , ECS
                                                     (or)
>>Amazon Elastic Compute Cloud (Amazon EC2) is a web-based service that allows businesses to run application programs in the Amazon Web Services (AWS) public cloud.
>>Amazon Ec2 is scalble computing capacity and you can use the images to you cn launch the servers as you need. and its configure security and networking and storage
amazone ec2 helps you to scale up and down handling the requirements

48-what is AMI In AWS

>>An Amazon Machine Image (AMI) is a pre-configured image using this images to launch the virtual servers
>>AMI id is different of every region and every image
>>you can use in production mostly use already created AMIs

49-you are trying to provide a service in a particular region, but you do not see the service in that region. Why is this happening, and how do you fix it?

>>Not all Amazon AWS services are available in all regions. When Amazon initially launches a new service, it doesn’t get immediately published in all the regions. 
They start small and then slowly expand to other regions. So, if you don’t see a specific service in your region, chances are the service hasn’t been published in 
your region yet. However, if you want to get the service that is not available, you can switch to the nearest region that provides the services

50-Mention the different types of instances in  Amazon EC2 and explain its features.

>>General Purpose Instances: They are used to compute a range of workloads and aid in the allocation of processing, memory, and networking resources.
>>Compute Optimized Instances: These are ideal for compute-intensive applications. They can handle  batch processing workloads, high-performance web servers, 
machine learning inference, and various other tasks
>>Memory Optimized: They process workloads that handle massive datasets in memory and deliver them quickly.
>>Accelerated Computing: It aids in the execution of floating-point number calculations, data pattern matching, and graphics processing. These functions are 
carried out using hardware accelerators.
>>Storage Optimised: They handle tasks that require sequential read and write access to big data sets on local storage

51- What do you understand by 'changing' in Amazon EC2?

>>To make limit administration easier for customers, Amazon EC2 now offers the option to switch from the current 'instance count-based limitations' to the new 
'vCPU Based restrictions.' As a result, when launching a combination of instance types based on demand, utilization is measured in terms of the number of vCPUs.

52-What is the difference between a Spot Instance, an On-demand Instance, and a Reserved Instance?

>>Spot instances are unused EC2 instances that users can use at a reduced cost.
>>When you use on-demand instances, you must pay for computing resources without making long-term obligations.
>>Reserved instances, on the other hand, allow you to specify attributes such as instance type, platform, tenancy, region, and availability zone. 
Reserved instances offer significant reductions and capacity reservations when instances in certain availability zones are used.

On-demand Instance
It is cheap for a short time but not when taken for the long term
Spot Instance
It is less expensive than the on-demand instance and can be bought through bidding. 
Reserved Instance
If you are planning to use an instance for a year or more, then this is the right one for you

53-With specified private IP addresses, can an Amazon Elastic Compute Cloud (EC2) instance be launched? If so, which Amazon service makes it possible?

>>Yes. Utilizing VPC makes it possible (Virtual Private Cloud).

54-What Are Some of the Security Best Practices for Amazon EC2?

>>Security best practices for Amazon EC2 include using Identity and Access Management (IAM) to control access to AWS resources; restricting access by only allowing 
trusted hosts or networks to access ports on an instance; only opening up those permissions you require, and disabling password-based logins for instances  launched from your AMI.

55-What are the common types of AMI designs?

Fully Baked AMI
Just Enough Baked AMI (JeOS AMI)
Hybrid AMI

56-What are Key-Pairs in AWS?

>>A key pair is a combination of a public key that is used to encrypt data and a private key that is used to decrypt data. The Key-Pairs are password-protected login credentials for the Virtual Machines
that are used to prove our identity while connecting the Amazon EC2 instances. The Key-Pairs are made up of a Private Key and a Public Key which lets us connect to the instances.

57-what EBS Volumes

>>Amazon Elastic Block Store (Amazon EBS) provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices. You can mount these volumes as devices
on your instances
>>It is designed to be used with Amazon Elastic Compute Cloud (EC2) instances to provide scalable and high-performance block-level storage volumes. EBS volumes are essentially virtual hard drives that 
can be attached to EC2 instances to provide additional storage space.it is permanent storage/persistence. it can be increased to a max of 16TB you can't decrease the ebs
>>it is availble in SSD and HDD

Use-Cases:-
1-Operating System and Application Storage: EBS volumes are commonly used to store the operating system, applications, and other software required by EC2 instances.
2-Databases: EBS is frequently used to store database files, providing the required storage for relational databases like MySQL, PostgreSQL, Oracle, and Microsoft SQL Server.
3File Systems: EBS volumes can be used as the storage backend for file systems. For example, you can attach EBS volumes to EC2 instances to create a scalable and reliable file system.
4-Data Backup and Disaster Recovery: EBS snapshots are often used for backup and disaster recovery purposes. Snapshots can be used to create copies of volumes, and these copies can be used to restore data
in case of data loss or system failures.
5-Development and Testing Environments: EBS volumes are valuable in development and testing scenarios where you need scalable and flexible storage for software development, testing, and debugging.
6-Media Storage: EBS volumes can be used to store and retrieve media files, such as images, videos, and audio files, providing a scalable and high-performance storage solution.
Dis-Advanatges:-
1-Performance Variability:EBS performance can be variable, especially for the General Purpose (gp2) and Throughput Optimized (st1) volumes. While Provisioned IOPS (io1) volumes provide predictable performance,
they can be more expensive.
2-Cold Start Latency:When launching a new EBS volume or restoring a volume from a snapshot, there can be an initial latency (cold start) as the data is loaded or provisioned.
This might impact the performance for a short period.
3-Pricing Model:EBS costs can accumulate, particularly with Provisioned IOPS volumes or snapshots. Users should carefully consider the pricing model, especially for high-performance workloads,
to avoid unexpected costs.
4-Region Dependency:EBS volumes are tied to a specific AWS region. If you need to move data between regions or have a multi-region architecture, you may need to use mechanisms like AWS DataSync 
or third-party tools.
5-Snapshots Are Region-Specific:EBS snapshots are region-specific. If you want to use a snapshot in another region, you need to copy it to the desired region, which might incur additional costs.
6-Volume Size and Type Changes:While you can modify the size of an EBS volume, changing the volume type (e.g., from gp2 to io1) requires creating a new volume and migrating data, which can involve downtime.
7-Backup and Restore Complexity:Although EBS snapshots are a powerful backup tool, managing and restoring multiple snapshots or dealing with complex backup strategies might require additional scripting
or management tools.
8-Limited Granularity for Volume Modifications:Certain modifications to EBS volumes, such as changing the volume type or encryption status, may require creating a new volume and copying data.
There are limitations on the granularity of modifications.
9-Dependency on EC2 Instances:EBS volumes are associated with specific EC2 instances. If an instance is terminated, the EBS volume must be manually detached and reattached to another instance.
10-ebs volume you can apply only one ec2 system
11-ebs volume you can apply only one ec2 system and it should be same availbility zone

advantages:-
>>you can diretly mount to the system 
>>it can shared between multiple servers and multiple availbility zones

58-How to decrease EBS volume.

>>Unfortunately, it is not possible to directly decrease the size of an existing EBS volume in Amazon Elastic Block Store (EBS). However, you can effectively reduce the amount of storage space used
by an EBS volume by creating a new smaller volume and migrating the data from the old volume to the new one. Here's a step-by-step guide on how to perform this process:
1-Create a Snapshot of the Old Volume: Before making any changes to the existing EBS volume, it's crucial to create a snapshot. This snapshot serves as a backup copy of the volume's data, allowing
you to revert to the original state if needed. To create a snapshot, navigate to the EBS console in the AWS Management Console and locate the volume you want to reduce. Click on the "Actions" button and
select "Create Snapshot." Assign a descriptive name to the snapshot and click "Create" to proceed.
2-Create a New Smaller Volume: Once the snapshot is complete, proceed to create a new EBS volume with the desired smaller size. Ensure the new volume is created in the same Availability Zone as the
original volume. To create a new volume, go to the EBS console, select "Volumes" from the left-hand menu, and click on the "Create Volume" button. Specify the desired volume size, select the same
volume type as the original volume, and choose the appropriate Availability Zone. Click "Create Volume" to finalize the process.
3-Attach Both Volumes to the Instance: To transfer the data from the old volume to the new one, you'll need to attach both volumes to the same EC2 instance. Identify the EC2 instance that is 
currently running the old volume. Locate the "Instances" section in the AWS Management Console and select the particular instance. Click on the "Actions" button and choose "Attach Volume." 
In the pop-up window, select the old volume and click on "Attach." Repeat the process to attach the newly created smaller volume to the same instance.
4-Transfer Data between Volumes: Once both volumes are attached to the instance, you can initiate the data transfer process. Log in to the EC2 instance and use a file system cloning tool to copy
the data from the old volume to the new one. Popular cloning tools include rsync, dd, and Clonezilla.
5-Detach and Delete the Old Volume: After successfully transferring the data, you can detach the old volume from the instance. This will make the old volume unmountable and free up the associated 
storage space. To detach the old volume, navigate to the EC2 console, select the instance, and click on the "Actions" button. Choose "Detach Volume" and select the old volume to detach it.
Once detached, you can safely delete the old volume from the EBS console.

59-Types of EBS-Volumes

===general purpose ssd gp2
>>it is cheaper compared to provisioned iops
>>it is high speed gp2  performance
>>minimum 1GB to you can extend volume upto 16TB
>>Minimum IOPS 100
>>Maximum IOPS 16000
>>if you taken 1GB you will get 3IOPS          -->input/output operations per second
>>you can't change iops you should use aws given iops

===gp3
>>it is also costly to gp2
>>minimum 1GB to you can extend volume upto 16TB
>>Minimum IOPS 3000
>>Maximum IOPS 16000

====IO1 Provisioned IOPS
>>it is costly compared to gp2 
>>it is also high spped performance
>>minimum 4GB to you can extend volume upto 16TB
>>Minimum IOPS 100
>>Maximum IOPS 64000
>>if you taken 1GB you will get 50IOPS
>>you change iops what you want you can modify the iops details

====IO2 Provisioned IOPS
>>it is costly compared to gp2 and IO1
>>it is also high spped performance
>>minimum 4GB to you can extend volume upto 64TB
>>Minimum IOPS 100
>>Maximum IOPS 256000
>>if you taken 1GB you will get 1000IOPS
>>you change iops what you want you can modify the iops details

47 -===example for iops gp2&IO1

DB requirment given by client 10TB he needs 100000 IOPs using do storage

gp2:
2 x 5120 = 10TB  30720 iops you will get
2 x 5120 = 10TB  30720 iops you will get
2 x 5120 = 10TB  30720 iops you will get
1 x 2500 = 2.5TB 7000 iops you will get 
>>estimate cost of your monthly bill ($3650.90)
>>con: max 250mb/sec

===if you need 100000iops you need to increase gp2 instance but IO1 you can get easily

IO1
2 x 5120 = 100000 iops with each volume 50000iops
requirement is fullfield  but it is costly compared to gp2

eatimate cost of monthly bill ($8558)
con:max throughput 781mb/sec

===cold HDD and optimized HDD and Magnetic (standard)

>>remaining all drives is slow and it is cheaper 
>>it is used for storage purpose or testing servers purpose
>>when you cant use data regularly then you can use this

where we can sql databese we can use ebs volumes explaining beloow

>>windows or linux we can gp2 volume
>>system databases we can gp2 volume
>>user database we can use IO1 volume
>>database transaction logs we can use IO1 volume 
>>tempDB we can use we can use instance store or standard
>>page file i virtual memory we can use instance store or standard
>>backups  we can use magnetic or standard

60-What is an Instance store

>>instance store is temporary storage data will be deleted if server reset
>> It cannot be increased or decreased
>>only ssd type is available
>> When you use instance storage while launching ec2 if the server restart or reset the data will be loose

61-what is EFS and what is the use case of it

>>Amazon Elastic File System is a cloud storage service provided by Amazon Web Services designed to provide scalable, elastic, concurrent with some restrictions, and encrypted file storage for 
use with both AWS cloud services and on-premises resources.
>>After you create an existing file system (EFS), you can't change its encryption setting. This means you can't modify an unencrypted file system to make it encrypted. Instead, you must: 
Create a new Amazon EFS file system with encryption turned on
>>Amazon Elastic File System (EFS) is a scalable and fully managed file storage service provided by Amazon Web Services (AWS). It is designed to provide scalable and shared file storage that 
can be accessed concurrently by multiple Amazon EC2 instances. EFS uses the Network File System version 4 (NFSv4) protocol, making it compatible with a wide range of Linux-based applications and workloads.
>> The storage limit 8PB
>>it can be shared between multiple servers and multiple regions
Advantages:-
1-Elasticity and Scalability:EFS automatically scales storage capacity up or down as you add or remove files, providing a high level of elasticity.
It can grow to petabyte scale, accommodating changing storage requirements without manual intervention.
2-Shared File System:EFS allows multiple Amazon EC2 instances to access the same file system concurrently. This makes it suitable for applications that require shared access to data.
3-Compatibility:EFS supports the Network File System version 4 (NFSv4) protocol, making it compatible with a wide range of Linux-based applications and workloads.
4-Managed Service:AWS takes care of the operational aspects of EFS, such as hardware provisioning, software configuration, patching, and maintenance. This reduces the administrative overhead for users.
5-Performance:EFS provides low-latency performance for a broad spectrum of workloads. It can handle a large number of concurrent connections and is suitable for various use cases, 
including big data analytics, media processing, and content management.
6-Security:EFS allows you to control access to your file systems using AWS Identity and Access Management (IAM) roles and policies. You can also use POSIX permissions to control access at 
the file and directory levels.
7-Integration with Other AWS Services:EFS can be easily integrated with other AWS services. For example, it can be used as shared storage for containers running on Amazon Elastic Container Service (ECS)
or Amazon Elastic Kubernetes Service (EKS).
8-Backup and Restore:EFS supports AWS Backup, allowing you to create and manage backups of your file systems. This makes it easier to implement backup and restore strategies for your data.
9-Regional Redundancy:EFS file systems can be configured for regional redundancy, ensuring high availability. This means that your data is stored across multiple Availability Zones within a region.
10-Cost-Effective:EFS offers a pay-as-you-go pricing model, where you pay only for the storage you use. It eliminates the need for upfront investments in hardware and allows you to scale your storage
infrastructure based on your actual needs.

Uses cases:-
>>you can directly mount the system to ec2 instances
>>it is a share folder for network in a region
>>it is network file system  
>>it is unix or linux supported
>>it can shared between multiple servers and multiple regions
>>the storage limit 8PB

62-what is the use cases of EFS

1-Content Management Systems (CMS):EFS can be used to store and share assets, templates, and other files across multiple instances in a CMS environment. 
This is useful for websites and applications that rely on a centralized repository for managing content.
2-Web Serving:EFS is well-suited for web serving architectures where multiple web servers need access to the same set of files. It provides a scalable and shared storage solution for hosting web content.
3-Development and Build Environments:In development environments, EFS can be used to store source code, libraries, and build artifacts. Multiple development instances can access and modify these 
shared files, facilitating collaboration and consistency.
4-Big Data Analytics:EFS is suitable for big data workloads where multiple EC2 instances in a cluster need access to shared data sets. It provides a common storage platform for distributed processing
frameworks like Apache Spark or Apache Hadoop.
5-Containers and Microservices:EFS can serve as a shared storage backend for containers running on Amazon Elastic Container Service (ECS) or Amazon Elastic Kubernetes Service (EKS). 
This enables multiple containers to access shared data, supporting microservices architectures.
6-Home Directories and User Files:EFS is suitable for scenarios where multiple users or instances need access to common home directories or user-specific files. This can be beneficial in environments
where users require consistent access to their files across different instances.
7-Media and Entertainment Workflows:EFS is well-suited for media processing and entertainment workflows where large files, such as videos or graphics, need to be shared and processed by multiple instances 
simultaneously.
8-Log and Data Storage for Applications:Applications that generate logs or require persistent data storage can leverage EFS for shared file storage. This is beneficial for scenarios where logs need to be
centrally stored and analyzed.

63-what is EFS life cycle manager.

>>Amazon EFS lifecycle management uses an internal timer to track when a file was last accessed, and not the POSIX file system attributes that are publicly viewable. Whenever a file in Standard or
One Zone storage is accessed, the lifecycle management timer is reset.
>>Amazon EFS lifecycle management automatically manages cost-effective file storage for your file systems. When enabled, lifecycle management migrates files that have not been accessed for a set 
period of time to the EFS Standard–Infrequent Access (Standard-IA) or One Zone–Infrequent Access (One Zone-IA) storage class, depending on your file system. You define that period of time by using 
the Transition into IA lifecycle policy.
types of storage classes available in EFS:-
1-standard
2-standard-IA
3-onezone-IA 

64-what is the difference between EFS and EBS 

EBS:-
1-The file storage type is block storage
2-it is attached to single instance only 
3-limited Scalability to attach or incerse the volume 
4-it works 	Boot volumes, data storage
5-it more expensive compared to EFS 
6-it can be increased to max of 16TB you can't decreased the ebs
7-so many types of stoarge classes availble  
8- It is commonly used as block storage and is attached to EC2 instances using block-level protocols. EBS volumes are accessed within a single Availability Zone and can only be attached to 
one EC2 instance at a time.
EFS:-
1-The file storage type is file storage
2-it is attached to multiple instance only 
3-Highly scalable
4-it works Shared file systems, large data storage
5-less expensive compared to EBS 
6-It supports 8PB 
7-limted stoarge classes available only 
8-It uses the Network File System version 4 (NFSv4) protocol, allowing multiple EC2 instances to access the file system concurrently. EFS supports shared access across multiple Availability Zones 
within a region.

65-what is the Advanatges of Storage classes

1-Cost Optimization:Storage classes allow you to choose the right balance between performance and cost. If you feel your not frequently accessing then you can create the life cycle move to another
cheap stoarge 
standard 
standard-IA
One-Zone-IA 
Glacier
Glacier Deep Archive
Intelligent-Tiering
2-Performance Tiers: Speed based on the selecting the stoarge class if you select the standard then it will fast if you selecct the one-Zone-IA it low and glacier is to much slow 
3-Durability: Storage classes often have different durability characteristics. For example, Standard storage in Amazon S3 has 99.999999999% (11 9's) durability,
4-Lifecycle Management: there lifecycle management available you can move your data wheneve you wnat 
5-Built-In Versioning Support: in this storage class there option called versioning it will help you keep data size low and if any case you deleted some file the version will help you 
6-Data Archiving and Compliance:Storage classes like Amazon Glacier and Amazon Glacier Deep Archive are designed for long-term data archiving and compliance. These storage classes offer extremely
low-cost storage for data that is rarely accessed but needs to be retained for compliance reasons.
7-Multi-Tiered Storage Strategies:By using multiple storage classes and lifecycle policies, you can implement multi-tiered storage strategies. This enables you to optimize storage costs based on the
changing access patterns of your data over time.
8-Integrated Services:Storage classes often integrate with other services and features provided by the cloud storage platform. For example, Amazon S3 Intelligent-Tiering integrates with the S3 Inventory
and S3 Analytics features to provide insights into data access patterns.

66-What is Amazon S3? 

S3 is short for Simple Storage Service, and Amazon S3 is the most supported storage platform available. S3 is object storage that can store and retrieve any amount
of data from anywhere. Despite that versatility, it is practically unlimited as well as cost-effective because it is storage available on demand. In addition to 
these benefits, it offers unprecedented levels of durability and availability. Amazon S3 helps to manage data for cost optimization, access control, and compliance. 

>> We can store log files 
>> Maximum uploading one file size upto 5TB
>>it is objeted level stoarge
>> It is the global unique name
>> You can upload files 3types 1 - cli and 2 - aws console 3 - application through API or Jenkins
>> Mostly upload through API or application through
>>s3 bucket is not global why means when you create a bucket it asks for the region but it will show all the regions
>>s3 availability is 99.9 uptime
>>s3 durability is 99.999999 it will not go delete 
>> When you upload a file it will save in 3 az but it will not show that saving
>> It will charge when you upload and storage and when you access through the internet will cost 
>> You can enable version control

---storage types
>> If frequently using data means use this (standard)
>> If we can't access frequently that old data we can use this (standard, glacier)
>>temporary log files you can use this (One Zone-IA)

67-S3 life-cycle

>> You can create the bucket using create the lifecycle then which will save in the first 30 days standard if not frequently accessed that you can move that data into
standard-IA then this data also not frequently using then you can move into One-Zone-IA then move to S3-glacier it is permanent data storage for long-term
if you want to delete you can specify also after 1 or 2 or 10 years after also

68-what is S3 Replication

>> Transfer the data from one bucket to another bucket
>>craete bucket anji256 on that time you should enable the replication
>> It is use for mostly log-purpose of saving

=== How to add replication
>>click on add rule >>enable version
>>select bucket anji159
>> Select the storage type glacier or anything
>> Click on next done
>>if you upload data in the bucket of anji256 the data will save in the bucket of anji159

69-How many S3 buckets can be created?

Up to 100 buckets can be created by default

70-S3 bucket storage types

1-standard
2-standard I-A
3-onezone I-A
4-S3 Intelligent-Tiering,
5-S3 glacier

71-What is Glacier

>> S3 glacier cheap storage
>> For long-term backup, we can use this
>> If you stored data in Glacier if retrieving it  will charge it's not for regular purposes it's for longterm
>> You can take glacier backup using third-party tools fast glacier, altro, veritas, Netbackup,networker, Aavamar, Tivoli
>> Glacier is called the vault

72-can I store my EBS volume in an s3 bucket

>>When you create an EBS snapshot, it's automatically stored in an Amazon S3 bucket that's managed by Amazon Web Services (AWS). You can copy snapshots
within the same AWS Region, or from one Region to another. However, you can't copy snapshots to the S3 buckets that you manage.

73-How can I grant a user access to a specific folder in my Amazon S3 bucket?

>>If your IAM user and s3 bucket are present in the AWS account then you can grant the user access to a specific bucket folder using the IAM policy

You can add the IAM policy to individual IAM users or you can attach the IAM policy to an IAM role that multiple users can switch to.
If the IAM identity (user or role) and the s3 bucket belong to different AWS accounts, then you  must grant access to both the IAM policy and the bucket policy

74-What does bucket policy mean?

>>With Amazon S3 bucket policies, you can secure access to objects in your buckets, so that only users with the appropriate permissions can access them. 
You can even prevent authenticated users without the appropriate permissions from accessing your Amazon S3 resources.

75-can I copy my AMI into the s3 bucket?

>>You can store an Amazon Machine Image (AMI) in an Amazon S3 bucket, copy the AMI to another S3 bucket, and then restore it from the S3 bucket.

76-what are the types of encryption in s3

>> Using AWS KMS policy using create the key assign to users then it will use encrypt the data 
two types of encryption there AWS KMS policies

1-symmetric key: the same key is used to encrypt and decrypt 
2-asymmetric key: different keys used to encrypt and decrypt

Server-side encryption – Amazon S3 encrypts your objects before saving them on disks in AWS data centers and then decrypts the objects when you download them. ...
Client-side encryption – You encrypt your data client-side and upload the encrypted data to Amazon S3.

77-Do you prefer to host a website on S3? What’s the reason if your answer is either yes or no?

>>Yes, it can be a good option to host a website on S3. Hosting a website on S3 is a cost-effective solution for static websites that don’t require server-side processing.
S3 is a reliable and scalable storage service that offers high availability and durability.
>>However, hosting a dynamic website with server-side processing on S3 is not recommended as it lacks the necessary computing resources and scalability to handle such websites.
In such cases, it’s better to use other AWS services like EC2 or Elastic Beanstalk.

78-which is faster storage EBS or S3

EBS: EBS provides block-level storage volumes that can be attached to EC2 instances. It offers low-latency, high-performance storage optimized for transactional workloads and applications
that require frequent read/write access to data. EBS volumes are typically used as primary storage for applications running on EC2 instances.

S3: S3, on the other hand, is an object storage service designed for storing and retrieving large amounts of unstructured data. It is highly scalable and durable, offering high availability
and durability for data. S3 is accessed over the network and is well-suited for use cases such as data backup, data archiving, static website hosting, and content distribution.
>>The performance of EBS and S3 can vary based on several factors, including the storage class, instance type, network connectivity, workload characteristics, and access patterns. 
However, in general, EBS is designed for low-latency access and is optimized for performance-sensitive workloads, while S3 is optimized for scalability, durability, and cost-effectiveness
at the expense of slightly higher latency.
>>It's important to consider the specific requirements of your application or use case when choosing between EBS and S3. If your application requires low-latency access to storage and
frequent read/write operations, EBS is usually the more suitable choice. On the other hand, if you have large volumes of unstructured data, require high scalability, or need to 
store data for long-term archival purposes, S3 is a better fit.

In summary, the "faster" storage option between EBS and S3 depends on your specific workload and requirements. It is recommended to analyze your application's needs and the characteristics
of the storage services to make an informed decision.

79-Your application needs to access AWS services, such as S3 securely within your VPC. How would you achieve this?

>>To securely access AWS services within the VPC, we can use VPC endpoints. VPC endpoints allow instances in the VPC to communicate with AWS services privately, without requiring
Internet gateways or NAT gateways. 
We can create VPC endpoints for specific AWS services, such as S3 and DynamoDB, and associate them with the VPC. 
This enables secure and efficient communication between the instances in the VPC and the AWS services.

80-S3 buckets offer several advantages, including:

Durability and availability: S3 provides high durability and availability for your data.
Scalability: You can store and retrieve any amount of data without worrying about capacity constraints.
Security: S3 offers multiple security features such as encryption, access control, and audit logging.
Performance: S3 is designed to deliver high performance for data retrieval and storage operations.
Cost-effective: S3 offers cost-effective storage options and pricing models based on your usage patterns.

81-hat is S3 Event Notifications and Triggers

>>S3 event notifications allow you to configure actions when specific events occur in an S3 bucket. For example, you can trigger AWS Lambda functions, send messages to 
Amazon Simple Queue Service (SQS), or invoke other services using Amazon SNS when an object is created or deleted.

82-you have a VPC with a public subnet and private subnet. Instances in the private subnet need to access the internet for software updates. How would you allow internet access for
instances in the private subnet?

>>To allow internet access for instances in the private subnet, we can use an Internet gateway or a NAT instance
>> We would place the NAT Gateway/instance in the public subnet and configure the private subnet route table to send outbound traffic to the NAT Gateway/instance. This way instances 
in the private subnet can access the internet through the NAT Gateway/instance.

83-you have launched EC2 instances in your VPC, and you want them to communicate with each other using private IP addresses. What steps would you take to enable this communication?

>>By default, instances within the same VPC can communicate with each other using private IP addresses. To ensure this communication, we need to make sure that the instances are launched in the same VPC 
and are placed in the same subnet or subnets that are connected through a peering connection or a VPC peering link. 
Additionally, we should check the security groups associated with the instances to ensure that the necessary inbound and outbound rules are configured to allow communication between them.

84-your organization has a VPC with multiple subnets you want to restrict outbound internet access for resources in one subnet but allow outbound internet access for resources in 
another subnet how would achieve this?

>>To restrict outbound internet access for resources in one subnet we can modify the route table associated with that subnet in the route table we can remove the default route (0.0.0.0/0)
that points to an internet gateway
>>This would prevent resources in that subnet from accessing the internet for the subnet where outbound internet access is required we can keep the default route pointing to the Internet gateway

85-you have been assigned to design a VPC architecture for a 2-tier application. the application needs to be highly available and scalable how would you design the VPC architecture

>>In this scenario, I would design a VPC architecture in the following way.

I would create 2 subnets public and private the public subnet would contain the load balancer and be accessible from the internet. the private subnet would host the application servers.
I would distribute the subnets across multiple Availability Zones for high availability. Additionally, I would configure auto-scaling groups for the application servers.
1-disatrey recovery strage 
2-multi availbilty zone 
3-multi region
4-blue-green deployment 

86-Your organization requires an isolated environment within the VPC for running sensitive workloads. How would you set up this isolated environment?

>>To set up an isolated environment within the VPC, we can create a subnet with no internet gateway attached. 
>>This subnet, known as an "isolated subnet," will not have direct internet connectivity. We can place the sensitive workloads in this subnet, ensuring that they are protected from 
inbound and outbound internet traffic. 
>>However, if these workloads require outbound internet access, we can set up a NAT Gateway or NAT instance in a different subnet and configure the isolated subnet's route table to
send outbound traffic through the NAT Gateway/instance.

87-You want to implement strict network access control for your VPC resources. How would you achieve this?

>>To implement granular network access control for VPC resources, we can use Network Access Control Lists (ACLs). NACLs are stateless and operate at the subnet level. We can define inbound and outbound 
rules in the NACLs to allow or deny traffic based on source and destination IP addresses, ports, and protocols. By carefully configuring NACL rules, we can enforce fine-grained access control for 
traffic entering and leaving the subnets.

88-You have a private subnet in your VPC that contains a number of instances that should not have direct internet access. However, you still need to be able to securely access these 
instances for administrative purposes. How would you set up a bastion host to facilitate this access?

>>To securely access the instances in the private subnet, you can set up a bastion host (also known as a jump host or jump box). The bastion host acts as a secure entry point to your 
private subnet. Here's how you can set up a bastion host: Create a new EC2 instance in a public subnet, which will serve as the bastion host. Ensure that this instance has a public IP 
address or is associated with an Elastic IP address for persistent access. Configure the security group for the bastion host to allow inbound SSH (or RDP for Windows) traffic from your
IP address or a restricted range of trusted IP addresses. This limits access to the bastion host to authorized administrators only. Place the instances in the private subnet and configure
their security groups to allow inbound SSH (or RDP) traffic from the bastion host security group.SSH (or RDP) into the bastion host using your private key or password. From the bastion host,
you can then SSH (or RDP) into the instances in the private subnet using their private IP addresses.

89-how to encrypt the unencrypted AMI

>>A Launch Configuration is a configuration template that specifies the parameters for launching instances in an ASG. It includes details such as the EC2 instance type, AMI ID, 
security groups, key pairs, block device mappings, and user data scripts. When creating an ASG, you associate it with a Launch Configuration. If the desired capacity of the ASG changes, 
the ASG automatically creates new instances based on the Launch Configuration settings

90-How have you ensured high availability and fault tolerance for an AWS application?

>>In a previous project, we implemented AWS Elastic Load Balancers and Auto Scaling Groups to ensure that our application could handle sudden spikes in traffic and maintain high availability.
We also set up Amazon CloudWatch to monitor the performance of the application and alert us in case of any failures, allowing us to quickly respond and ensure fault tolerance.

91-Can you explain a time when you had to troubleshoot an issue with an AWS application?

>>In my previous role, we had an issue with an EC2 instance not responding to incoming traffic. After reviewing the logs, we discovered that the security group rules were misconfigured,
causing the instance to reject incoming traffic. We adjusted the security group rules to allow incoming traffic, and the issue was resolved.

92-how NAT-Gateway will work

>> When we use to launch the nat-gateway in the public subnet then we can add it to private route tables

93-Can S3 Be Used with EC2 Instances, and If Yes, How?

Amazon S3 can be used for instances with root devices backed by local instance storage. That way, developers have access to the same highly scalable, reliable, fast,
inexpensive data storage infrastructure that Amazon uses to run its own global network of websites. To execute systems in the Amazon EC2 environment, 
developers load Amazon Machine Images (AMIs) into Amazon S3 and then move them between Amazon S3 and Amazon EC2.
>>Amazon EC2 and Amazon S3 are two of the best-known web services that make up AWS

94-What is the maximum limit of elastic IPs anyone can produce?

>>A maximum of five elastic IP addresses can be generated per location and AWS account.

95-what is a storage gateway and what types

>>AWS Storage Gateway is a set of hybrid cloud storage services that provide on-premises access to virtually connect the cloud storage.

>>volume-gateway:- A Volume Gateway provides cloud-backed storage volumes that you can mount as Internet Small Computer System Interface (iSCSI)
devices from your on-premises application servers.

>>File-gateway:- File Gateway allows your existing file-based applications or devices to use secure and durable cloud storage without needing to be modified. 
With S3 File Gateway, your configured S3 buckets will be available as Network File System (NFS) mount points or Server Message Block (SMB) file shares.

>>Tape-gateway:-Tape Gateway enables you to replace using physical tapes on premises with virtual tapes in AWS without 
changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on premises for low-latency data access

96-What is auto-scaling?

>>Auto scaling groups must not be used for database
>> Auto scaling groups are used mostly used web and app servers because of static content
>>Auto-scaling is a function that allows you to provision and launch new instances whenever there is a demand. It allows you to automatically increase or decrease 
resource capacity in relation to the demand.Cloud Architect Master's
-----types
1-Vertical Scaling:-
>>t2.micro to t4.medium 
>> It needs machine shutdown or downtime to increase the size of the specification

2-Horizontal Scaling:-
>>t2.micro to t2.micro 
>> If you want to increase the system you need to use horizontal scaling

97-where we can use cloud watch mostly in AWS

1-Auto-scaling --scaling policies
2-

97-How do you set up a system to monitor website metrics in real-time in AWS?

>>Amazon CloudWatch helps you to monitor the application status of various AWS services and custom events. It helps you to monitor:

State changes in Amazon EC2
Auto-scaling lifecycle events
Scheduled events
AWS API calls
Console sign-in events
amazon cloud watch

98-On an EC2 instance, an application of yours is active. Once the CPU usage on your instance hits 80%, you must reduce the load on it.What strategy do you use to complete the task?

>>It can be accomplished by setting up an autoscaling group to deploy additional instances, when an EC2 instance's CPU use surpasses 80% and by
allocating traffic across instances via the creation of an application load balancer and the designation of EC2 instances as target instances.

99-what is describe menas vpc

>>it will show the entire vpc details of vpcs

100-what Interface endpoint

>>An interface VPC endpoint allows you to privately connect your Amazon VPC to supported AWS services. 
Interface VPC endpoints also connect to endpoint services hosted by other AWS customers and partners and AWS Marketplace partner services.

101-What is load balancer and how it works? and types of load balancer

>>A load balancer acts as the “traffic cop” sitting in front of your servers and routing client requests across all servers capable of fulfilling those requests 
in a manner that maximizes speed and capacity utilization and ensures that no one server is overworked, which could degrade performance. 
>>load balancer is region dependent
>>you can add multiple target groups into load balancer

1- Network load balancer
>>A load balancer serves as the single point of contact for clients. The load balancer whenever the first system not availble then distributes incoming traffic across
multiple targets, such as Amazon EC2 instances.This increases the availability of your application. You add one or more listeners to your load balancer.
>>it is an layer 4 in transpot layer 
>>it is use TCP and UDP protocals
>>direct traffic based on ip address
>>you can add multiple target groups into load balancer
>>Network, or Layer 4, load balancers can provide faster responses,
>>the deregistation delay is 300 seconds
>>it will check every 30 seconds for health checks if you cant reach the server then decalre as not reachable then forward the request to another server
>>if you use one port or listner like 80 you can't use that port onemore time
>>you can save your load balancer logs in s3 
>>it will support path based routing but you need to every time differnt ports or different listners everytime
>>it is region dependent
>>by default cross-zone load balancer is not-enabled you should enable if the required happens
>>it is suitable for single application or easy application

---What is an internal load balancer?
>>An internal (or private) load balancer is used where private IPs are needed at the frontend only. Internal load balancers are used to load balance traffic inside
a virtual network.A load balancer frontend can be accessed from an on-premises network in a hybrid scenario

---what is external load balancer
>>The external load balancer is used to route external HTTP traffic into the cluster

2- Application load balancer
>>A load balancer serves as the single point of contact for clients. The load balancer whenever the first system not availble then distributes incoming traffic across
multiple targets, such as Amazon EC2 instances.This increases the availability of your application. You add one or more listeners to your load balancer

>>it is layer 7 in application layer
>>it uses https and http
>>it is only for web-applications
>>the deregistation delay is 300 seconds
>>you can save your load balancer logs in s3 
>>you can use one listner or port number of times
>>it is region dependent
>>by default cross-zone load balancer is enabled
>>it will check every 30 seconds for health checks if you cant reach the server then decalre as not reachable then forward the request to another server
>>it will support path based routing or url based routing
>>you can transfer the traffic using paths based in application load balancer or  you can use path based routing
>>you can create the multiple target group like home , movies, shows then use the path based routing or url based routing
>>it is used or suitable for complex applications 

102-what is the differnce between application load-balncer and network load-balancer and gateway load-balancer step by step comparision

1-Network-Load-Balancer:-
>>Operates at the transport layer (Layer 4) of the OSI model. It focuses on routing traffic based on IP protocol data.
>>Suited for scenarios where low-latency and high-throughput are essential, such as TCP/UDP-based applications.
>>Primarily works at the transport layer, supporting TCP, UDP, and TLS.
>>Performs health checks at the transport layer, ensuring the health of the underlying instances.
>>Employs algorithms such as Least Outstanding Requests and Round Robin.
>>Offers features like source IP preservation, allowing the backend servers to see the original client IP addresses
>>by default cross-zone load balancer is not-enabled you should enable if the required happens
>>it is suitable for single application or easy application
>>>>it will support path based routing but you need to every time differnt ports or different listners everytime

2-Application Load-Balancer:-
>>Operates at the application layer (Layer 7) of the OSI model. It can make routing decisions based on content, such as HTTP/HTTPS headers.
>>Ideal for applications that require advanced routing, content-based routing, and support for multiple protocols. Commonly used for web applications.
>>Supports application layer protocols such as HTTP, HTTPS, and WebSocket.
>>Supports health checks at the application layer, checking the health of the individual services.
>>Uses advanced algorithms like Least Outstanding Requests, Least Connection Count, and Round Robin.
>>Provides features like SSL termination and Web Application Firewall (WAF) to enhance security.
>>by default cross-zone load-balancer is enabled
>>it is used or suitable for complex applications 
>>it will support path based routing or url based routing

3-Gateway-Load-Balancer:-
>>Primarily designed for managing and distributing traffic at the edge of the network, often used in scenarios involving multiple virtual private clouds (VPCs).
>>Used in edge network scenarios where multiple VPCs need to be interconnected efficiently, making it suitable for hybrid cloud architectures.
>>Provides flexibility to handle various protocols at the edge of the network.
>>Incorporates health checks for the connected VPCs and services.
>>Typically employs algorithms suitable for global load balancing, considering factors like proximity and latency.
>>May include security features relevant to edge network scenarios, but specifics depend on the implementation.

103-If any one asking the my load balancer is in public my subnets is private can use this subnets to connect through internet 

>>yes you can connect through internet by using the load-balancer must be public .like when you create the load-balancer it will create in only public not private
you must put app and db servers in private only. you can connect or download patches using nat gateway

>>you can enable in load balancer to connect the particular ip only if your client asked to connect the your website through ip you can give this ip and 
and add that client ip in your load balancer only that client will connect

104-what is cross-zone load-balancer

>>The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer 
node distributes traffic across the registered targets in all enabled Availability Zones

>>you have load balancer and 1a availbility zone 4 servers and 1b availbilty zone have 2 servers if the traffic will then it will distribute 50 persent to 1a
and 50 persent 1b then it will get over trafficed by 1b so on that time you can use the cross-zone load balancer then it will distribute same amount of traffic to every servers in availbilty zones

105-what is ssl-off loading or termination

>>SSL termination refers to the process of decrypting encrypted traffic before passing it along to a web server.
            or
>>An SSL connection sends encrypted data between an end-user's computer and web server by using a certificate for authentication. SSL termination helps speed the decryption process and reduces 
the processing burden on backend servers

106- what is round-robin load balancer              -->video 17 40mins

>>Round-robin DNS is a technique of load distribution, load balancing, or fault-tolerance provisioning multiple, redundant Internet Protocol service hosts, 
e.g., Web server, FTP servers, by managing the Domain Name System's (DNS) responses to address requests from client computers according to an appropriate 
statistical model        (or)
>>Round robin load balancing is a simple way to distribute client requests across a group of servers. A client request is forwarded to each server in turn.
How does it works:
>>What Is Round-Robin Load Balancing? Round‑robin load balancing is one of the simplest methods for distributing client requests across a group of servers. 
Going down the list of servers in the group, the round‑robin load balancer forwards a client request to each server in turn.

107-what is target group in load balancer

>>collections of machines and also specified protocols and ports to specified instances
         or
>>A target group tells a load balancer where to direct traffic to : EC2 instances, fixed IP addresses; or AWS Lambda functions, amongst others. When creating a 
load balancer, you create one or more listeners and configure listener rules to direct the traffic to one target group

108-what is the use of health checks in aws

>>Health checks are a way of asking a service on a particular server whether or not it is capable of performing work successfully. 
Load balancers ask each server this question periodically to determine which servers it is safe to direct traffic to.

109-what is deregistration delay

>>Elastic Load Balancing stops sending requests to targets that are deregistering. By default, Elastic Load Balancing waits 300 seconds before completing the 
deregistration process,which can help in-flight requests to the target to complete
         or
>>Connection draining allows you to specify a maximum time (between 1 and 3,600 seconds and default 300 seconds) to keep the connections alive before reporting the
instance as de-registered. The maximum timeout limit does not apply to connections to unhealthy instances.

110-What is AZ affinity?

>>The AZ Affinity architectural pattern reduces the number of times an AZ boundary is crossed. In the example system we looked at in Figure 1, 
AZ Affinity can be implemented with two changes. First, the ALB is replaced with a Network Load Balancer (NLB).

111-what is gateway load balancer

>>Gateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. It gives you one gateway for distributing traffic
across multiple virtual appliances while scaling them up or down, based on demand.

112-What is a load balancer? Give scenarios of each kind of balancer based on your experience.

>>A load balancer is a device or service that distributes network traffic across multiple servers to prevent any one server from becoming overwhelmed. There are three main types of load balancers:

1-Application Load Balancers (ALBs) distribute traffic based on content at the application layer (HTTP/HTTPS).
2-Network Load Balancers (NLBs) distribute traffic at the transport layer (TCP/UDP).
3-Classic Load Balancers (CLBs) distribute traffic across multiple EC2 instances and can be used with both HTTP and HTTPS protocols.

113-How have you ensured high availability and fault tolerance for an AWS application?

>>In a previous project, we implemented AWS Elastic Load Balancers and Auto Scaling Groups to ensure that our application could handle sudden spikes in traffic and maintain high availability.
We also set up Amazon CloudWatch to monitor the performance of the application and alert us in case of any failures, allowing us to quickly respond and ensure fault tolerance.

114-what is ASG and launch config

>>ASG:-ASG stands for Auto Scaling Group. It is a feature provided by Amazon Web Services (AWS) that automatically adjusts the number of instances in a group based on the specified 
scaling policies. ASGs are commonly used to maintain the availability, performance, and cost-efficiency of applications running on EC2 instances.
Key features of Auto Scaling Groups include:
1-Automatic Scaling: 
2-High Availabilty:
3-Elasticity
4Integration with load-balancer
5-Integration with launch configurations

>> launch-config launch configuration is a template that an EC2 Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the 
instances such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping.

115-can I add multiple target groups in a load balancer

>>yes you can add multiple target groups. for example your created two servers server-1, server-2 your created server-1 in tg-1 target group and then created tg-2 target add server-2 then
create the application load balancer and the listeners tg-1 target group it will add 100% traffic to server-1. you can goto the load-balancer in listeners go to add rules edit 
load-balancer then add server-2 or target group and add 50% of traffic to server-1 and server-2 this will highly available

116-what is Route53 and what is the uses

>>Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web servic. Rout53 using we can host the domians.Route53 is global. 
route53 using you can do one region to another region load balancing.route53 using you can do multiple region load-balancing.  
>>Route53 provides DNS as a Service DNS->stands for Domain name service
>>DNS service is the one that maps your domain name with ip address or maps dns is that resolves your domain to the ip address if you assign the domain to the load-balancer
its resolve the domian name to the load-balancer
>>DNS keeps a lot of records
>>we use mostly routing policys simple, latency, falover, weighted

1-failover :- if we create two load balancer in differnt regions and create the helath checks in route 53 if we selectd the failover routing policy then we can 
create the same name of the website like www.anji.com is primary in mumbai region and www.anji.com is secondary in us-east-1 .whenever primary fails 
it will go secondary directly without checking the pings

2-weighted :- if we create weighted route policy route 53 it is like failover but the thing is you can mention the traffic which load balancer need to 
accessing the apllication over internet if we give 0 menas you can't access the application on that load balancer if we give 255 then we can access the 
loab balancer. the use case is we want maintaines on particular region you can diver the traffic into sepearte the specific load balancer
			(or)
Weighted round-robin:-two regions servers are the then we can divide the traffic based on priority based we can divide the traffic 
>>Weighted routing allows you to associate multiple resources with a single domain name (example.com) or subdomain (acme.example.com) and choose the amount of traffic routed for each resource.
This can be useful for a variety of purposes, including load balancing and testing new versions of software.

3-geo-DNS:-Geolocation routing can be used to send traffic to resources based on the geographical locations for example you may have presence in europe and Asia now you want user in the
Asia to be served in the Asia and rhose europe to be served by servers in europe
ex:-you can localize yourcontent present some or all of your website in the language of your user

4-latency:- latency is used same is the previous failover or weighted routing policy the differnt is you can mention to the particular in region if any one 
near by india that user will connect to the india based load balancer and if any us then that user will connect the us load balancer its region based. 
it is used to decrease the latency in your application to the users

117-what is traffic policies in route53

>>it will give the graphical representaion of dns records

118-What is the difference between AWS latency-based routing and geo DNS

>>Amazon maps-out typical latency between IP addresses and AWS regions. Choose Latency-based Routing to have the fastest response. Geolocation maps the IP addresses to geographic locations. 
This permits rules like "send all users from Côte d'Ivoire to the website in France", so they see a language-specific version

119-What is dns how it works

>DNS used to identify computers, services, and other resources reachable through the Internet or other Internet Protocol
>>dns transfers human readble domain names for (example www.amazon.com)to machine  readble to ip address for example (192.168.0.24)
>>in your real time your isp will give dns address
>>most orgnazation disable ping to stop the ddos attacks by hackers
>>rembering name is easy but number are diffcult that we using dns names

120-what is hostname resolution 

>>HostName resolution is the process of associating names and IP addresses, and it's one of the most essential services on a network. 
People understand descriptive names, but network communications require difficult-to-remember addresses

121-how to create ssl-certifictae in aws

>>go to aws certificate manager create your domain name anji.com it will ask the email or domain validation then go to domain it will give name and cname
and create the record set in your route53 and create a record set the giev name and select the cname and it will give you certificate then go to your load balancer 
and add listener select the port and certficate then you good to go then delete the 80 port number that it . it take 10 to 15 minutes and to avoid the security instrucations
								(or)
>>you can create the certifictaes using third-party also using csr generator it will ask your domain name and give you public and private key then go to comodsslstore then you can create the certificate
								(or)
>>digcert, entrust provide ssl certficate
>>you need to create private.key and .csr file in your system and give that keys into authority digcert entrust they will give 3 files certification.crt and private.key and ca_bundle.crt
then you configure this certifictaes to webserver

122-what is ssh

>>the ssh protocal (also referred to as secure shell) is a method for secure remote login from one computer to another computer.it provides the secure connection
to network. it provides several alternative options for strong authntication, and it protects the communications security and integrity with strong encryption

123-what is Subdomain

>>A subdomain is a prefix added to a domain name to separate a section of your website. Site owners primarily use subdomains to manage extensive sections that
require their own content hierarchy, such as online stores, blogs or support platforms. Subdomains function as a separate website from its domain

124-what is mx record 

>>An MX record specifies the names of your mail servers and, if you have two or more mail servers, the priority order. Each value for an MX record contains
two values, priority and domain name. An integer that represents the priority for an email server.

125-what is simple record in aws

>>Simple routing lets you configure standard DNS records, with no special Route 53 routing such as weighted or latency. With simple routing, you typically 
route traffic to a single resource, for example, to a web server for your website. You can use simple routing policy for records in a private hosted zone.

126-what is cname records in aws

>>cname is also called canincal name or alias 
>>CNAME records are Directs one name to another name or Points domain to Another Domain www.anji.com to sampi.anji.com
>>so when you move your servers you only have to change one set of records

127-what is A name record in aws

>>A known as Host records it converts host to ip address 
>>A Records are IPv4
>>AAAA Records are IPv6
>>Amazon Route 53 offers a special type of record called an 'Alias' record that lets you map your zone apex (example.com) DNS name to the DNS name for your
ELB load balancer (such as my-loadbalancer-1234567890.us-west-2.elb.amazonaws.com).

128-what is the differnce between A and CNAME record

>> the difference between CNAME and A record in a traditional NameServer is that only CNAME can be used to point to a different subdomain and A record must 
point to a valid IP address. Since the AWS Route53 allows you to do use a subdomain with either A Rec or CNAME

129-what is NS record

>>>>it is name server records when you creating or hosting website it will create the name server reocrds that will be fault tolerance purpose if any one goes down your will not impact why 
because reaminig will run right
>>The NS records and A records are both used to provide DNS services. The difference between them is that the former define the servers that provide
DNS services for a domain name while the A records translate hostnames (such as www, FTP, mail) into IP addresses.

130-Is it possible to create one webserver under we can create 3 domains

>>yes its possible you can 3 folders like anji.com sampi.com vinod.com under deploy your code each domain code route the traffic based on DNS 
then you will access one-webserver using access 3 differnet domains in one ec2 system

131-what is reverse proxy

>>A reverse proxy is a server or service that sits between client devices and backend servers. When a client makes a request, it is forwarded to the reverse proxy, which then acts as 
an intermediary and routes the request to the appropriate backend server. The response from the server is then returned to the client through the reverse proxy. The client is unaware of
the backend server and communicates only with the reverse proxy.

132-what is mod_proxy 

>>mod_proxy and related modules implement a proxy/gateway for Apache HTTP Server, supporting a number of popular protocols as well as several different load-balancing algorithms. 
Third-party modules can add support for additional protocols and load balancing algorithms.

133-what is the difference between reverse proxy and mod_proxy

>>In summary, a reverse proxy is a general concept referring to a server or service that acts as an intermediary between clients and backend servers. On the other hand, mod_proxy is a 
specific module within the Apache HTTP Server that enables reverse proxy functionality and provides additional features and configurations for proxying requests.

134-what is Cloud-Native

>>Cloud native is the software approach of building, deploying, and managing modern applications in cloud computing environments. Modern companies want to build highly scalable, flexible,
and resilient applications that they can update quickly to meet customer demands

Benefits
>>Increase efficiency
>>Reduce Cost
>>Ensure Availability
>>Faster development
>>Platform independence
>>Cost-effective operations

EX:-Docker kubernetes

135-what is firewall

>>firewall is allow or disallow the traffic in your network
>>firewall is security group it is used to control the incoming and outgoing traffic

>>firewall are two types

1-stateless
>> It is not a two-way handshake
>>Stateless firewalls make use of a data packet's source, destination, and other parameters to figure out whether the data presents a threat. These parameters have to be entered by either
an administrator or the manufacturer via rules they set beforehand

2-stateful firewall
>> It is a two-way handshake  process
>>A stateful firewall inspects everything inside data packets, the characteristics of the data, and its channels of communication. Stateful firewalls examine the behavior of data packets,
and if anything seems off, they can filter out the suspicious data. Also, a stateful firewall can track how the data behaves, cataloging patterns of behavior. 

>> It is used to allow one side no need to allow the other side will remember and send the packets
>> A stateful firewall remembers the connection details it will give a replay

3-In-Bound rule

>> Inbound firewall rules protect the network against incoming traffic, such as disallowed connections, malware, and denial-of-service (DoS) attacks

4-Out-Bound rule

>>Outbound firewall rules protect against outgoing traffic, originating inside a network.

136-What is a DDoS attack, and what services can minimize them?

>>Distributed denial of service DDOS The main intention is to make it unavailable over the internet on a website or some resources. the hacker sends the multiple
requests into website is going unavailble over the internet

AWS Shield
AWS WAF
Amazon Route53
Amazon CloudFront
ELB
VPC

137-What is whitelisting an IP address?

>>IP whitelisting is when you grant network access only to specific IP addresses. Each employee (or approved user) shares their home IP address with the 
network administrator, who then enters their IP address on a “whitelist” that grants them network access.

138-What is the advantage of using a “reverse proxy server”?

>>The reverse proxy server can hide the presence and characteristics of the origin server. It acts as an intermediate between internet cloud and web server.
It is good for security reason especially when you are using web hosting services.

139-what is Baston Host

>>Bastion Hosts are a special-purpose instanxe that provides seccure access to linux instacnes located in private subnets
>>Act as a jump server to access other instances in private subnets
>>protect instances in private subnets from potential attacks 

140-How Bastion Host work

>>Ec2 instances in private subnets are protected by security group and NACLs, only allow secure connection from Bastion Host

>>Security Group of Bastion Host only allow SSH from trusted IP Address
>>use ssh agent forwarding on a client do not store private key on bastion host

141-without password how can access linux machines

>>you need to create the sshkey it is uses in rsa alogirthum it will create 2 files public key file named . ssh/id_rsa. pub and a private key file named . ssh/id_rsa.
you can add that public key into the remote server you must enable the password less authentication in remote servers
>>vi /etc/ssh/sshd_config  
passwordauthentication yes
permitrootlogin yes
#passwordauthentication no  -->you need comment this 
>>scp -r /home/anji.tx  2.54.25.2:/root/anji    -->it will send the file  
two ways to send the file password less like add your ssh keys in destination server or password authentication need to give password

>>ssh-keygen
>>press enter
>>press enter
>>ls -a
>>cd .ssh
>>ls
>>ssh-copy-id anji@172.31.38.167   -->it will ask passwd give next time it will not ask do same in remaing systems

142-how you can see if the port is opened or not in linux

>>>>netstat -a | grep -i 8000     -->to check the port opening and listening
>>telnet "ip" port

143-What are the kinds of security attacks that can occur on the cloud? And how can we minimize them?

>>The kinds of security attacks that can occur on the cloud include DDoS attacks, phishing attacks, data breaches, and insider attacks. To minimize them, it is important to use
strong authentication methods, implement proper access control and monitoring, encrypt data, keep systems up to date with security patches, and regularly conduct security audits and training for employees.

144-What is the difference between the Amazon Rds, Dynamodb, and Redshift?

Amazon RDS: Relational database service that allows you to set up, operate, and scale a relational database in the cloud easily.

DynamoDB: Fully-managed NoSQL database that provides fast and predictable performance with seamless scalability.

Redshift: Data warehousing service that provides fast querying and analysis of data using SQL and business intelligence tools.

145- Explain the difference between elasticity from scalability.

>>Elasticity: The ability of a system to handle an increase in the workload by simply adding hardware resources when demand rises, and rolling back scaled resources when there’s no demand.

>>Scalability: the ability of a system to increase the hardware resources for handling an increase in demand. It can be achieved by either increasing the hardware specs or increasing the number of processing nodes.

146-what is latency 

>> Latency is nothing but the time taken by request to reach the application and for the application and the server to send the response back to you
If there is a delay that is high latency
if there is no delay that is low latency

147-what are tags in what is the use of tags?

>>A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value. For each resource, each tag key must be unique, and each tag key can have only 
one value. You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level.

148-why AWS block port 25 in outbound ports

>>Amazon Web Services' EC2 instances are throttled on port 25 by default as a spam prevention measure. This can cause connection issues when attempting to use SMTP to relay emails 
through Postmark in your EC2 instance

149-you can block the certain ip and some countries using AWS WAF 

>>Yes its possible to block certain region

150- what is operating system

>>it is an user interface between user and hardware is called operating system

151-Can I Retrieve Data from a Deleted ELB or Ec2 Instance?

>>Metrics data for deleted ELBs and decommissioned EC2 instances are still available. MetricsData associated with a terminated instance will be removed when the Instance is shut down. 
In contrast, MetricsData associated with your account will remain on the MetricInfraService until you delete it. This information can be used for diagnosing problems with your ECS cluster or ELB.

152-If I disable monitoring for an Ec2 instance, will I lose the metrics data?

>>No, the metrics data will not be lost.
>>If you disable monitoring for an EC2 instance, the metrics you've collected are still safely preserved in a SQL database as part of the AWS CloudTrail logs.

153-What is RDS AWS used for?

>>Amazon RDS managed by Amazon and it makes it easy to set up operate and scale a relational database in the cloud it provides cost-efficient and resizable capacity
while automating time-consuming administration tasks. such as hardware provisioning database setup patching and backups
>>Amazon RDS is available on several database instance types optimized for memory performance I/O provides familiar databases like aurora, PostgreSQL, MySQL, MariaDB, Oracle database, SQL server

>>RDS is structured database
>>RDs is platform as service(paas)
>>you can create read replcia in another region also us-east-1a to ap-south-1a
>>if you createing an database automatically it will create two databases in a region but they don't know
it will connect in primary database ,the primary will sync to secondary database ,if in case primary not availble then secondary will connect 
>>if the primary available zone damaged not available then it will create onemore database in availble region like us-east-1c
>>you will connect only primary database
>>you can't access secondary why because where the database is there don'tknow

154-what is the rds failover time

>>60-120 seconds

155-rds database tpes

1-master(primary)
2-read(secondary)replica       -->you can create read replica

>>some tester has come and asked we test the rds you're not giving the primary you dont
know how to access the secondary that time you can use the read replica
>>read replica just copy a primary you can put another region also
>>in read replica you can't write on the database its only read option

156-Will your standby RDS be launched in the same availability zone as your primary?

>>No, standby instances are launched in different availability zones than the primary, resulting in physically separate infrastructures. This is because the entire purpose
of standby instances is to prevent infrastructure failure. As a result, if the primary instance fails, the backup instance will assist in recovering all of the data.

157-How would you address a situation in which the relational database engine frequently collapses when traffic to your RDS instances increases, given that the RDS instance replica
is not promoted as the master instance?

>>A larger RDS instance type is required for handling significant quantities of traffic, as well as producing manual or automated snapshots to recover data if the RDS instance fails

158- What is SQL Server in simple words?

>>SQL Server is a database server by Microsoft. The Microsoft relational database management system is a software product which primarily stores and retrieves data
requested by other applications. These applications may run on the same or a different computer

159-What is AWS redshift used for?

>>it is type of database it belong to postgress sql.it used for data ware-housing purpose
>>Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes,
using AWS-designed hardware and machine learning to deliver the best price performance at any scale

160-what is elastiCahe in aws

>>elastcahe is not regular satabase it is web-service that makes easy to launch manage and scale distributed in memory cache in the cloud
it is used to improve your website or web-application performance. it will cache the services in that memory it will increase the better performance
>>it like ec2-service
 
two types:-
1-memcahed 
2-redis

161-What is DocumentDB used for?

>>it like mongo database
>>A document database is a type of nonrelational database that is designed to store and query data as JSON-like documents. Document databases make 
it easier for developers to store and query data in a database by using the same document-model format they use in their application code.

162-how do you backup rds server

>>you need to create first option group and select the version and type of your database and then create your option group. then go to s3 create bucket and role
for s3 and then go that group attach the role and bucket and select the backup and restore it will 

163-what is snapshots

>>Snapshots are a point-in-time copy of your data, and can be used to enable disaster recovery, migrate data across regions and accounts, and improve backup compliance. 

164-how to make rds highly available

>>multi AZ deployment high availbilty & fail over for disaster recovery.
>>read replicas provide high performance for read heavy workloads
>>automatic database software updates
>>easy to scale compute memory stoarge
>>automated backups and manula backups
>>encryption at rest (AWS KMS) andd transit SSL

165-how to make rds highly availble

>>you can create the one RDS-Database then created the read replicas in 2 more availbilty zones then and also your servers are in different availbility zones like 
az1->RDS(read-write) az2->RDS(read only), az3->RDS(read only) if the any traffic will go az2 then it will take data from az2->RDS only if any want write then it will az1->RDS
why because it will have only read and write permisssion this will make your RDS highly availble and fast your database

166-How have you implemented disaster recovery for an AWS application?

>>In a previous project, we implemented disaster recovery for an AWS application by using AWS Route 53 to route traffic to a secondary region in case of a failure in the primary region. 
We also set up Amazon S3 cross-region replication to ensure that the data was replicated to the secondary region in real-time. Additionally, we created automated backups of the
application data using Amazon S3 and Amazon Glacier to ensure that we could quickly restore the data in case of a disaster.

167-Can you describe a time when you had to troubleshoot a performance issue in an AWS application?

>>In a previous project, we had an issue where the application was slow to respond to user requests. After reviewing the logs, we identified that the application was experiencing high CPU
usage. We then analyzed the application’s resource utilization using Amazon CloudWatch and identified that the database queries were taking longer than expected. We optimized the database
queries and improved the performance of the application.

168-Can you explain a time when you implemented security measures for an AWS application?

>>In my previous role, we implemented several security measures for an AWS application, including setting up Multi-Factor Authentication (MFA) for all AWS accounts, enabling AWS CloudTrail
to monitor and log all API activity, and encrypting data at rest using AWS Key Management Service (KMS). We also regularly conducted vulnerability assessments and penetration testing to
identify and mitigate potential security risks.

169-How would you use AWS S3, CloudFront, and Route 53 for a scalable and highly available static website?

>>AWS S3, CloudFront, and Route 53 can be used together to build a scalable and highly available static website. Here’s an overview of the process:

>>Store the website files in an Amazon S3 bucket and make the files publicly accessible.

>>Create a CloudFront distribution, which is Amazon’s global content delivery network (CDN), and configure it to use the S3 bucket as its origin. This allows CloudFront to serve the 
website files from edge locations around the world, ensuring fast and low-latency access for visitors.

>>Use Amazon Route 53, the highly available and scalable DNS service, to associate a custom domain name with the CloudFront distribution, so visitors can access the website using a 
familiar and memorable domain name.

>>Configure Route 53 to use the health checks and failover features to ensure that the website is always available, even if one of the edge locations becomes unavailable. Finally, 
use S3’s versioning and lifecycle policies to automatically store multiple versions of the website files and to transition older versions to less expensive storage options over time.

170-Can you describe a scenario in which you would use AWS Elastic Beanstalk to deploy an application?

>>AWS Elastic Beanstalk is a fully managed service offered by Amazon Web Services that allows developers to easily deploy, run and scale web applications. 
The service is ideal for situations where a developer has created a web application using a popular framework, such as Ruby on Rails, Node.js, or Java, and is looking to quickly deploy
the application to a production environment with minimal overhead. AWS Elastic Beanstalk takes care of provisioning the necessary resources, such as EC2 instances, load balancers, 
databases and security groups, and sets up a fully functional environment for the application. This eliminates the need for the developer to manage the underlying infrastructure, 
freeing them up to focus on the development and maintenance of the application itself. Monitoring the health of the application can be done using AWS Elastic Beanstalk’s built-in monitoring 
and reporting features, and updates and changes can be made as needed. This makes AWS Elastic Beanstalk a simple, streamlined solution for deploying and managing web applications, 
particularly well-suited for developers who want a straightforward way to handle the deployment and management of their applications.

171-How would you implement a disaster recovery solution in AWS using RDS, EC2, and S3?

>>Create an Amazon RDS instance in a primary region, and configure automatic backups to Amazon S3.
>>Create an Amazon EC2 instance in the same primary region
>>Create an Amazon S3 bucket to store data backups, and configure the RDS instance to store backups in the S3 bucket.
>>Set up an Amazon EC2 instance in a secondary region and install the necessary software to access the S3 bucket.
>>Create an Amazon RDS instance in the secondary region and configure it as a replica of the primary RDS instance.
>>Configure the secondary RDS instance to automatically fail over to the primary RDS instance in case of a disaster in the secondary region.
>>Regularly test the failover process to ensure it is working as expected.
>>Use Amazon S3 versioning and object lifecycle management policies to retain backups for a desired period of time.
>>Use Amazon CloudWatch to monitor the RDS instances and EC2 instances, and set up alarms to trigger notifications or automated actions in case of a disaster.

172-what Required to connect the MySQL or RDS or any databse

>>The account that is used to connect to the MySQL database must have at least CONNECT permissions. This enables SSMA to obtain metadata from schemas owned by the connecting user.
1-connection permission any network or locally
2-mysql endpoint
3-port number should be open
4-username and password
5-mysql workbench or jdbc connection

173-Define Snapshots in Amazon Lightsail?

>>A snapshot is a base feature for creating backups of your EBS volumes. A snapshot takes a copy of the EBS volume and places it in Amazon S3, where it is 
stored redundantly in multiple Availability Zones. The initial snapshot is a full copy of the volume; ongoing snapshots store incremental block-level changes only
>>and can be used to enable disaster recovery, migrate data across regions and accounts, and improve backup compliance

174-what is AWS Backup

>> AWS Backup features .AWS Backup is like snapshots. in every organzation need to take a backup some of the particular resources with using of aws backup take some of major 
resources in aws backup automatically or daily backup no need to write script you can do automated backup on time bases like you can schedule and store the backup in s3 directly
>>your specify the some of the resource in aws to take backup on AWS Backup service. on which time you need . you can take partcular time take backup between
partcular  resource to take the backup. its nothing but snapshots

175-what is backup vault

>>its like glacier vault where we can storing data or saving the data is called backup vault

176-how many types of backup

>>two types
1-automatic
two types
a-daily backup
b-transactionlogs
2-snapshots

two types of RDS backup 

1-Automated backups:- it will store in s3
>>RDS automatically create backs up your database and transaction logs
>>RDS automcatically create a storage volumes snapshots of your DB instances backuing up the entire DB intsance
>>Automated backups occur daily during the preferred nackup window
>>Automated backups are kept for the Backup Retention period (0 to35 Days)
>>you can restore the DB instances to specify point in time during the bcakup retention period
creating a new DB instances

2-Database snapshots

>>Database snapshots are user-initiated backups of the DB instance stored in s3
>>DB snapshots are kept intil explicitly delete them
>>you cna creae a DB instance by restoring from DB snapshots
>>the first snapshots contains the data for the full DB instance subsequent snapshots of the
same DBinstance are incremenetl
>>ypu can copy a sanpshot in the same region acrss regions and across acounts
>>you can directly resore a DB instance from unencrypted a shared snapshot

177-how many days take AWS-backup can retrive

>>minimum 35 days

178--what is cloudwatch

>>Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, 
and on-premises applications and infrastructure resources and also analyze the logs by using iam role
 
1-cpu Utilzation
2-Memory utilization
3-harddisk
4-application montoring

2-types of montoring in AWS-Cloudwatch

1-Basic Montoring
>>it comes by default
>>it is free of cost
>>every five minutes logs will be updated 

2-Detailed monitoring
>>you need enable 
>>and its not free of cost
>>every one minute logs will updated 

179--What is an AWS CloudWatch Alarm?

>>An AWS CloudWatch Alarm is a monitoring and management solution for AWS resources. It enables the configuration of alarms based on individual indicators or as part of a larger monitoring plan.
>>you can set the alarm you set sns then it whenever it will trigger you will get an notification
>>you can also billing to the alarm like if we bill some amount then it will trigger 

180-types of alarm in cloudwatch 

1-Ok -The metric is below the threshold
2-Insuffiecnt data -Not enough data available for the metric to determine the alarm state.
3-ALARM — When the metric crosses the threshold.

181-What are Cloudwatch Logs?

>>CloudWatch Logs is a tool that helps us monitor and troubleshoot our systems and applications. It uses our already-established systems, applications, and custom log files. 
In addition, it allows us to monitor our logs in real-time for particular phrases, values, or patterns.

182-How Long Do Cloudwatch Logs Store My Data?

>>Your information will be kept in Cloudwatch Logs for up to 7 days. After that point, only the most current logs will be kept, with older ones automatically deleted to save space.

183-what is the AWS CloudWatch Agent?

>>The AWS CloudWatch Agent is software that can be deployed to any server, whether hosted on Amazon EC2, Amazon ECS, or on-premises. Using the AWS CloudWatch Agent, you can quickly gather 
and send system-level performance information from specific AWS resources.

184-What are three things you can do in CloudWatch?

>>Real-time monitoring is available for AWS resources and apps.
>>Create alerts that will respond automatically to changes in your AWS resources.
>>AWS CloudWatch Logs allows you to see and search log data.

185-What Types Of Cloudwatch Alarms Can Be Created?

>>Any CloudWatch measure can be displayed via an alarm you set up. You can also set off an alert based on metrics specific to your application or system.

186-What Actions Can We Take From a Cloudwatch Alarm?

Share an email with the team.
Schedule a Cron task to run.
Share on Slack or HipChat.
Run a script on this machine, such as one that imports data into a spreadsheet.

187-What Types Of Cloudwatch Alarms Can Be Created?

A warning about a specific resource (e.g., a virtual machine).
A regular notification for a specific metric (e.g., CPU usage).
A user-defined alarm (for example, if your application runs slowly, the alarm will trigger when the CPU reaches 80% and notify you immediately).
A personalized alarm based on user-defined rules.

188-What Statistics Can I View And Graph In Cloudwatch?

>>Disk I/O Activity
>>Memory Share 
>>CPU Usage
>>Memory Usage
>>Network Interface Usage.

189-What is the data retention period in CloudWatch?

>>You can change the log data retention setting for CloudWatch logs. By default, logs are kept indefinitely and never expire. You can adjust the retention policy for each log group, 
keeping the indefinite retention, or choosing a retention period between 10 years and one day.

190-What is the difference between CloudWatch and CloudTrail?

>>CloudWatch is a tool provided by Amazon Web Services that monitors and reports on the performance of various AWS resources and services.
>>CloudTrail, on the other hand, is a log of all actions in your AWS environment since it was first launched.

191-What is CloudWatch events used for?

>>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. 
Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams.

192-What services can be used to create a centralized logging solution?

>>The essential services that you can use are Amazon CloudWatch Logs, store them in Amazon S3, and then use Amazon Elastic Search to visualize them.
You can use Amazon Kinesis Firehose to move the data from Amazon S3 to Amazon ElasticSearch.
>>if your service is stopped are anything happens you can create a rule to trigger your script and send the notifications
(like if your lamabda function to stop the start and specific time then youy can use the rules it will be changed)

193-How do you set up a system to monitor website metrics in real-time in AWS?

>>Amazon CloudWatch helps you to monitor the application status of various AWS services and custom events. It helps you to monitor:

State changes in Amazon EC2
Auto-scaling lifecycle events
Scheduled events
AWS API calls
Console sign-in events
amazon cloud watch

194-What are the native AWS Security logging capabilities?

>>Most of the AWS services have their logging options. Also, some of them have an account level logging, like in AWS CloudTrail, AWS Config, and others.
Let’s take a look at two services in specific:

1-AWS CloudTrail:
>>cloudtrail is used to see th login logut details and activitesits not give detailed information
>>cloudtrail is data will save only 90 days, so we can import that into aws s3 or any centrailzied locations

2-AWS Config:
>>first enable select the resource what you need the resource type and enable recording on 
>>AWS config is used to track the details of what you changed on your instances or modfied your instances.  if you chnaged something in your ec2-instances 
it will automatically shows what you updated in your system it will and it will save in until 7 years

>>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance.

195-How would you use AWS CloudTrail and CloudWatch to monitor and log AWS resource activity and events?

>>AWS CloudTrail and CloudWatch are two services that can be used to monitor and log AWS resource activity and events. CloudTrail records API calls made to AWS services and stores 
information about the caller, time of call, source IP address, request parameters, and response elements. This information can be used to track changes to AWS resources and identify 
security threats. CloudTrail trails can be created to specify the AWS resources for which API calls should be recorded and logs can be sent to CloudWatch Logs for central storage and analysis.

>>CloudWatch Alarms can be set up to trigger when specific conditions are met, such as when an API call is made to a sensitive AWS resource. Alarms can be configured to send notifications 
to specified Amazon SNS topics or to stop or terminate an Amazon EC2 instance. The logs collected by CloudTrail and stored in CloudWatch Logs can be analyzed to identify trends, 
track resource activity, and diagnose issues.

>>By using CloudTrail and CloudWatch, organizations can have a comprehensive view of AWS resource activity and events, enabling them to identify security threats, troubleshoot issues,
and meet compliance requirements. The centralized logging and analysis provided by these services helps organizations to ensure the security and availability of their AWS resources, 
giving them greater control and visibility into their cloud infrastructure.

196-What is CDN and how it works?

>>A CDN or cloud-front is same. content deliver network is decrese the latency in your website and it provide the cache server in all edge locations 
on your website.it is used to speedup your website over the internet

197-what is global acclerator

>>it is like cloud-front or CDN. CDN provides cache server but global-acclerator gives to allow ports and give you can configure fixed entry-points to ip address  
>> AWS Global accelerator is a networking service that helps you to improve the availability zones and performance of the applications that offer you to your 
golbal users. aws global acclerator easy to setup and manage and configure. it provides static ip address and fixed entry point your application. 
global acclerator is basically global load balancer which load balancing the regions as well .it is costly compared to dns load balancer

198-what is WAF in AWS

>>it can provide the additional security of your web application. it will hide your servers on the internet it will prevent cyber attacks like sql injection and xss and ddos attacks 

199-what is flow logs

>>New Flow Logs will appear in the Flow Logs tab of the VPC dashboard. The Flow Logs are saved into log groups in CloudWatch Logs. The log group will be created
approximately 15 minutes after you create a new Flow Log. You can access them via the CloudWatch Logs dashboard.

200-how to port forwarding in linux or ec2

>>public ec2 and private ec2 you can login one public ec2 instnace using ssh then go to the vi /etc/multifortforwarding/multiforwad.config 
then add the private ec2 instance private ip then reboot using sudo reboot the you can login using ssh -i -/ssh/anji.pem anji@12.24.45.25 -p 2222

201-what ssh key types

1-RSA
2-DSA 
3-ECDSA.

202-what is AWS KMS Policys

>>AWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control the cryptographic keys that are used to protect your data
two types of keys 
1-symatric key : same key is used encrypt and decrypt 
2-asymatric ky : differnt keys used to encrypt and decrypt
>>you can assign the key is and attach that key to one user and then create the s3 bucket then attach the kms policy to one user then the user will get read or write 
permission no can get the write the permission to that bucket

>>Use AWS KMS to encrypt data across your AWS workloads, digitally sign data, encrypt within your applications using AWS Encryption SDK, and generate and verify
message authentication codes (MACs).

203-what is ARN

>>Amazon Resource Names (ARNs) uniquely identify AWS resources. We require an ARN when you need to specify a resource unambiguously across all of AWS, 
such as in IAM policies, Amazon Relational Database Service (Amazon RDS) tags, and API calls.

204-what is aws migration

>>Cloud migration is the process of moving a company's digital assets, services, databases, IT resources, and applications either partially, 
or wholly, into the cloud. Cloud migration is also about moving from one cloud to another.

types of migration
1-rehosting.
2-re-platforming.
3-repurchasing.
4-refactoring.
5-retaining.
6-retiring.

205-what is session manager

>>Session Manager is a fully managed AWS Systems Manager capability that lets you manage your Amazon EC2 instances through an interactive 
one-click browser-based shell or through the AWS CLI. You can use Session Manager to start a session with an instance in your account.

206-what is virtual private gateway

>>A virtual private gateway is a tool establishig a safe connection or tonnel to connect over the Internet . it will create secure connection over the
Internet and it reliable. virtual network gateway is to connect your on-premises network with VPC
>>A virtual private gateway is a tool for establishing a safe tunnel to connect and transport encrypted data between devices, the cloud, and enterprise servers
across the internet. Use a virtual private gateway to create a VPN connection that is both secure and reliable.

207-What is site-to-site-vpn

>>A site-to-site virtual private network (VPN) is a connection between two or more networks, such as a corporate network and a branch office network. the site-to-site-vpn creates a tonnel to connect a
secure way over the ineternet .Many organizations use site-to-site VPNs to leverage an internet connection for private traffic as an alternative to using private MPLS circuits
>>it will give two ips you can create the two tonnles if first one fails then you can use or set second one 
>>tonnel is nothing but connecting the secure way in the ineternet

208-What is point-to-site VPN?

>>A Point-to-Site (P2S) VPN gateway connection lets you create a secure connection to your virtual network from an individual client computer.
A P2S connection is established by starting it from the client computer.
>>you launch public and private certificates in your system and connect to the aws VPCS

209- what is client vpn-endpoint

>>AWS Client VPN is a fully-managed remote access VPN solution used by your remote workforce to securely access resources within both AWS and your on-premises network. 
Fully elastic, it automatically scales up, or down, based on demand

210-what is direct connect in aws

>>direct connect is like vpn but it like leased line or dedicted line to internet . you can create your virtual private network then you can connect your aws to your on-premises. 
AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your internet and Amazon VPC. it is compared to vpn it is costly
>>AWS Direct Connect is a networking service that provides an alternative to using the internet to connect to AWS.

211-What is OpenVPN used for?

>>Open source OpenVPN uses VPN technologies to secure and encrypt data sent over the internet. Its custom VPN protocol uses SSL/TLS for key exchange. 
Since its creation in 2001 it has become the de facto standard in the open source networking space with over 60 million downloads 

212-what is IAM

>>AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. You use IAM to control who is authenticated 
(signed in) and authorized (has permissions) to use resources.

213-what are the types IAM policies    -->see in iam.txt read inline policy

>>AWS managed policies:- aws managed by this policies, you can reuse the policies
>>customer managed policies:- you can create and attach the policies to user and groups like giving permssiones and denying users or groups. you can reuse the polices
>>Inline policies:- its like same customer maneged policies but its one to one like one user or role you cant reuse the poliices it is not visible for everyone 
it is visible for only user and it is embed directly into a single user, group, or role.

214-what is IAM policy simulator?

>>Iam policy simulator is used to see the policy briefly what you attached to the users for permisiions what you given

215-what is the differnce between customer managed policy and inline policy

>>A customer managed policy is a standalone policy that you administer in your own AWS account. An inline policy is a policy that's embedded in an IAM identity (a user, group, or role).

216-where we can use roles

>>role is used manage the Accounts
>>AWS Identity and Access Management (IAM) roles are entities you create and assign specific permissions to that allow trusted identities such as workforce identities and applications
to perform actions in AWS. When your trusted identities assume IAM roles, they are granted only the permissions scoped by those IAM roles.role is not a user
>>roles is also use access-key and secret-key but will not show its automatically create that and also create the token in your system but it will be expiry in 6-8 hours then 
it will create the one more token. if you want use that credentials in without token it will not work

217-what is the difference between IAM and roles

>>The difference between IAM roles and policies in AWS is that a role is a type of IAM identity that can be authenticated and authorized to utilize an AWS resource, 
whereas a policy defines the permissions of the IAM identity.
>>A Role with no Policy attached to it won’t have to access any AWS resources.
>>A Policy that is not attached to an IAM role is effectively unused. Permissions listed in an IAM policy are only enforced when that policy is attached to an IAM identity.

218-what is Assume role

>>Assume role is nothing your giving temporary or prmenaent access another aws account user or group level access to another account.the user can access what permission your
giving that will be accessbile that user anytime briefly in below see

>>your in Anji account createing role in as anji1 role using another AWS account enter sampi AWS account number 11256 and add persmission like ec2 or s3 click on create.copy your ARN 
go to sampi account have vinod user go to policies create STS policy in that paste anji1 role ARN and create that policy. attach that policy into vinod user and login with vinod user
you can see option in support that see switch to role paste the Account number and role number you will be able to access anji1 role what the permission you have

219-what is role-switching

>>When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. 
When you exit the role, your user permissions are automatically restored

>>your createing one role giving this access to another accounts then that account can access the your given resources this called role-switching

220-what is Service control policys

>>Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available 
permissions for all accounts in your organization.

221-how you can manage the 20 accounts as acccounts

>>we can use orgnzation create the some group to add and divide specific groups to aws accounts
                              (or)
>>if we have 20 accounts we use master and sub-account architecture we use. master is there we use organzations, properly use organzation units we kept as internal
external and third party we are properly segrgated. then we can apply security control policy ,we can restrict users using permisiion boundary

222-how to secure your system or aws accounts how to see logs 

>>using cloud trail feature enable it send the logs into s3 bucket

223-an you explain the steps to secure an AWS environment using IAM, VPC security groups, and network ACLs

>>Certainly! Securing an AWS environment involves multiple layers of security controls, including Identity and Access Management (IAM), Virtual Private Cloud (VPC) security groups, 
and network ACLs. Here are the steps to secure an AWS environment using these components:

IAM (Identity and Access Management):

Create individual IAM user accounts for each person or system that requires access to your AWS resources.
Assign appropriate IAM policies to each IAM user, granting them the least privilege necessary to perform their tasks.
Implement multi-factor authentication (MFA) for IAM users to provide an extra layer of security.
Regularly review and audit IAM policies to ensure they align with the principle of least privilege and remove unnecessary access.
VPC (Virtual Private Cloud) Security Groups:

Define security groups within your VPC to control inbound and outbound traffic at the instance level.
Create separate security groups for different types of resources based on their security requirements.
Specify the inbound and outbound rules for each security group, allowing only necessary protocols, ports, and IP ranges.
Regularly review and update security group rules to ensure they align with your organization's security policies.
Avoid overly permissive rules and follow the principle of least privilege.
Network ACLs (Access Control Lists):

Network ACLs act as a firewall for controlling traffic at the subnet level within your VPC.
Define inbound and outbound rules in network ACLs to allow or deny traffic based on IP addresses, protocols, and ports.
Carefully evaluate and configure network ACLs to restrict access to your resources.
Regularly review and update network ACL rules as per your security requirements.

224-What is the difference between IAM users, groups, roles and policies ?

>>IAM User: An IAM user is an identity within AWS that represents an individual or application needing access to AWS resources. IAM users have permanent long-term credentials, such as
a username and password, or access keys (Access Key ID and Secret Access Key). IAM users can be assigned directly to IAM policies or added to IAM groups for easier management of permissions.
>>IAM Role: An IAM role is similar to an IAM user but is not associated with a specific individual. Instead, it is assumed by entities such as IAM users, applications, or services to 
obtain temporary security credentials. IAM roles are useful when you want to grant permissions to entities that are external to your AWS account or when you want to delegate access to 
AWS resources across accounts. IAM roles have policies attached to them that define the permissions granted when the role is assumed.
>>IAM Group: An IAM group is a collection of IAM users. By organizing IAM users into groups, you can manage permissions collectively. IAM groups make it easier to assign permissions 
to multiple users simultaneously. Users within an IAM group inherit the permissions assigned to that group. For example, you can create a "Developers" group and assign appropriate 
policies to grant permissions required for developers across your organization.
>>IAM Policy: An IAM policy is a document that defines permissions and access controls in AWS. IAM policies can be attached to IAM users, IAM roles, and IAM groups to define what actions 
can be performed on which AWS resources. IAM policies use JSON (JavaScript Object Notation) syntax to specify the permissions and can be created and managed independently of the users, 
roles, or groups. IAM policies consist of statements that include the actions allowed or denied, the resources on which the actions can be performed, and any additionalconditions.

225-What is the relation between AMI and Instance?

>>Instances can be launched by AMIs. One AMI can launch as many instances as required. An instance type defines the hardware of the host computer including information about computers and
its memory abilities. After launching an instance, it works as a traditional host and could be interacted with as with any other computer.

226-What are the tools used to send logs to the cloud environment?

1-AWS CloudWatch Logs
2-AWS Elastic Beanstalk
3-AWS Lambda
4-Fluentd
5-Logstash

227-what is the schedular and event pattern in events in Cloudwatch

>> Schedular is like you can schedule a certain time
>> Event pattern you can create a pattern and you do whenever the event triggers it will execute

228-what are the main reasons to server went down

>>ddos attacks
software issues
developer are human errors
check the memory and cpu utilization
check the network issues like pinging or not

>>prevention
>>implment firewalls and cybersecurity 
>>montior the requests 
>>check the cpu utilization and network and memoty 
>>add load balancer and auto-scaling groups on-demand

229-what is control tower

>>control tower is used to create setup Set up and govern your multi-account AWS environment and  accounts like create the sub accounts and organizations and creating who are going to 
create aws environment newly then this will help you
>>you can Enable security and compilance  using establishing guardrails which prevent resources from being deployed  that dont conform policies plus you can detect and remediate 
non-compliant accounts and resources as your team provisions them 
>>montior compilance you'll get visual summaries of your AWS environment through dashboard taht lets see your accounts guardrails and compliance status all in one place  

230-what is cloud-formation (it is infrastructure as a code)(IAAS) what is CFT template

>>A CFT (CloudFormation Template) is a JSON or YAML file used to describe and provision AWS resources in a repeatable and automated manner. It allows you to define your 
infrastructure as code, specifying the desired state of your AWS resources and their relationships.In a CloudFormation template, you define a collection of AWS resources, their properties,
and the relationships between them. This includes specifying dependencies between resources, which ensures that resources are created and configured in the correct order.

Resources:
  MyS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: my-bucket

  MyEC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: ami-12345678
      InstanceType: t2.micro
      KeyName: my-key-pair
      DependsOn: MyS3Bucket

---Advantages
>>CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, 
all the resources needed for your applications across all regions and accounts. This gives you a single source of truth for your AWS and third-party resources.

---Disadvantages
>>CloudFormation uses either JSON or YAML. This makes CloudFormation easy to read and manage. But there is a constraint that doesn't allow AWS developers to 
create CloudFormation templates greater than 51MB in size. In case the template exceeds this size limit, developers need to create a nested stack for the templates.

>we can use this template to create the services we can use this code to version control and if we can modify the code we can easily undesrstand
>>aws cloud-formation uses YAML and JSON lanugauge
>>Save the code in an S3 bucket, which serves as a repository for the code.
>>Use AWS CloudFormation to call the bucket and create a stack on your template

231-what are important component of CFT

1-Resources:-
2-parameters:-
3-Mappings:-
4-Outputs:-
5-Conditions:-
6-Metadata:-
7-Mappings:-
8-Trandforms:-

232-if the replacement is true in cloud-formation

>>in cloud formation when your are deployed one server and then changed key pair or something nything in your cloud-formation and importing in resource 
through updating when its come true means it will destroy the server or ec2-system and install fresh system or ec2 or resource. if it comes false then it update the system or resources

233-what is import-resource in cloud-formation

>>in cloud formation import-resource using. when you acccidentaly created ec2-instance manualy by mistake without deleeting that
you can import that resource using the Import-resource option but you must mention "deletionpolicy": "retain" is mandatory and if you want destory that system using
cloud-formation means first system will delete but next time add system not gone delete because it will using retain policy

234-what is roolback failure in cloud-formation

>>roll-back failure is must be enable. when your deploying code using cloud-formation if gets an error then automatically delete the everything what is deployed  in cloud-formation 

235-what is a designer in cloud-formation

>>AWS CloudFormation Designer (Designer) is a graphic tool for creating, viewing, and modifying AWS CloudFormation templates. With Designer, you can diagram your 
template  resources using a drag-and-drop interface, and then edit their details using the integrated JSON and YAML editor

236-what is stack policy in cloud-formation

>>stack-policy is used to restrict the updates of your cloud-formation templates of any user anyone

237-what is nested template in cloud-formation

>>nested stack using you can create multiple resource like 3 differnet vpc inside the 3 servers using centrailized url or your cloud formation stored or saved 
in somewhere s3 any where 

238-what is stackset in cloud-formation

>>stacksets using can deploy cloud-formation using the resources in different accounts . this accounts added in your orgnaztion , first need to create the stackset 
role and then create the stack-set execcution rule and give master account id and do same in sub-accounts and give master account id and then copy the ou id 
and then deploy your resources in other accounts if you added two more accounts that accounts also deployed that resources

239- what is detect drift in cloud-formation

>>detect drift is used to identify what is changes done when you deployed in cloud-formation configuration it detect and show you either you can add or delete further
                          (or)
>>Drift is the difference between the expected configuration values of stack resources defined in CloudFormation templates and the actual configuration values of these 
resources in the corresponding CloudFormation stacks.

240-what is lambda and what is use of lambda

>>AWS Lambda is a server-less computing platform that allows engineers to create a small function, configure the function in the AWS console and have the
code executed without the need to provision servers-paying only for the resources used during the execution
>>lambda runs your code high-availbilty compute infrastructure and performs all the administration of your compute resources
>>simply its like an editor(vim,pycharm,sublime text,atom)with some extra features
>>it supports to run differnet languages like python,go,nodejs,java,..etc
>>it is installed or running on amazon linux server and we can access /tmp using lambda function
>>aws lambda needs server it is installed and running amazon linux but your not maintaing thats it
>>lambda execution maximum time is 15mins. maximum file size in lambda is 75gb in a region. maximum thousnd instance in a region

Requirements for AWS Lambda function:-
1-the handler is the entry point for the lambda. the lambda function accepts json formated input and will usaully return the same
2-we usually correlate directly with the language selected to write your function
3-manual trigger or rnby us you can configure a lambda invocation to an event such as upload to s3,dynamodb,API Gateway,AWS Cloudwatch

>>you can stop and start ec2-instance using python boto3 and lambda, Alerting mails if ec2 stop or start and creating EBS snapshots,copy of EBS snapshots

241-differnce between ec2 and lambda

ec2:-
1-ec2 instance how much used you should pay the bill
2-on-demand price. for example your launch one ec2 not using but you should pay the bill
3-install required software
4-user can host the complete application
5-need to update and patches user should maintain

lambda:-What are the two types of AMI?

1-when we runing functions on that time you should pay the bill into lambda service
2-AWS Lambad you pay only what you used . your charges based on the number of requests on your functions and duration
3-selecting only required software or runtime
4-it is just peace of code to host the code
5-no need to update the updates and maintian by aws

242-what is concurrency in lambda

>>concurrency is the number of requests that your function is serving any given time when your function is invoked lambda allocates 
an instance of it to process the event when the function code finishes running it can handler another request

243-what is Boto3

>>Boto3 is python libray for aws to make simplefiy your work 
>>Boto3 makes it easy to integrate your Python application, library, or script with AWS services including Amazon S3, Amazon EC2, Amazon DynamoDB, and more.

244-what is authentication and authriozation

>>authentication your accessing or login into the applicatio or portal entry only
>>authrozation is nothing but you should be assign some roles are features

245-What is an Elastic Transcoder?

>>You can use Amazon Elastic Transcoder to convert video and audio files into supported output formats optimized for playback on desktops, mobile devices, tablets, and televisions

246-what is AWS Service Catalog

>>AWS Service Catalog is like menu what you want give the resources to your users you give that images or cloud-formation templates or anything specfiy the 
role to aws service catalog then it give the permisssions to that user. it will use in big environments like so many developers or there you can specif the role 
to access the particular only one cloud-formation template you can specify that using AWS service catalog

247-what is AWS SSM

>>service manager is  configuration management tool. we can use deployment and like ansible. we need ssm agnet then we can do deployment without ssm agent
we can't do anything. ssm agent is like role add to ec2-instance then we can patches and updates over the deployment and configure the resources

248-what is cognito

>>Amazon Cognito lets you easily add user sign-up and authentication to your mobile and web apps. Amazon Cognito also enables you to authenticate users through an 
external identity provider and provides temporary security credentials to access your app's backend resources in AWS or any service behind Amazon API Gateway

>>you can intregrate the third-party signin and login functionalitys

249-what is OpsWork

>>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use 
code to automate the configurations of your servers.

250-What is Trusted Advisior

>>you can check your helath and security issues and performance and fault tolerance if the account is secured or not all the snapshots taking perfectly 
load balancer and cost utilization and if the coudtrail enable or not you can see everything in trusted advisior

251-what is process management

>>when you start a program running an application in linux it actually run as a process

252-What is Elastic Beanstalk

>>AWS Elastic Beanstalk is an orchestration service offered by Amazon Web Servicesand it is used deploying and scaleup the AWS services, 
including EC2, S3, Simple Notification Service, CloudWatch, autoscaling, and Elastic Load Balancers, health monitoring

>>Elastic beanstaclk is used  if the developer is developing java application he is checking and deploying in 3 days in a week so we can use elastic beanstaclk
using create servers and required configuration easily

253-what is service 

this controls the starting and stopping of services

254-what is chkconfig

>>this controls which services are set to start on boot

255-what is crontab

>>In any operating system it is possible to create jobs that you want to recoccur. this process know as job-scheduling
for any linux this process is handled by the cron service or daemon called crond

* * * *  *
minutes hour day month week

256-what is ECS

>>Amazon Elastic Container Service (ECS) is a highly scalable, high performance container management service that supports Docker containers and allows 
you to easily run applications on a managed cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances.
>>when you creating an ecs cluster it will automatically install the ecs agent in your nodes 

1- tasks:-when any node is down the application will down it will not start another node or any ec2 machine
2-service:-if you using the service if service will down in any node it will move to any other node where is availble it will deploy

257-What is difference between ECS and EC2?

>>The largest difference for EC2 is that it deploys isolated VM instances with auto scaling support, 
and ECS deploys scalable clusters of managed Docker containers. Enterprises can use ECS to scale web applications, perform batch processing, 
and run services in a hybrid environment to deliver better services to users
two services is there

258-what is Fargate in aws

>>AWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers. 
AWS Fargate is compatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).

259-What is the difference between EC2 and Fargate?

>>Amazon EC2 manages or deploys your own EC2 instances to effectively run applications. Fargate AWS allows you to run containers without the need for EC2 instances.
Both are excellent methods for managing or scaling your containers in a dependable manner, but deciding which service to use is always a difficult task.

260-what is SLA(service-level agreement)

>>They provide information on all contracted services and on the agreed expected reliability and they clearly set out the parameters, responsibilities and expectations 
so that in the event of a problem with the service, no party can plead ignorance

261-what is AWS API-Gateway

>>Amazon API-Gateway is a fully-managed service that makes it easy for developers to create, publish, maintain, montiro, and secure APIs at any Scale
>>APIs acts as the "front-door" for applications to access data business logic or functionality from your backend services
>>API-Gateway helps us to create a RESTful APIs and Web-Socket APIs that enable real-time two-way communcication

262-where security comes in 

>>when we build the application then it go the qa team and test then it any changes happen then it will take the build then go the security testing and then deployment

what precautions would take the security of your application

>>we can build the feature development then security testing then sends to qa testing and then security testing then releasing the production
>>detecting issues early shortens development time

why security issues comes into production what bases it will come

1-time restrictions:- need to relase the relases within a week without checking the secuity issues deploy application into production
2-hard to get answers:- where the problems hard to find out the issues need to clarify with the developers to take understand the issues
3-unfamiliar with pipelines 

263-what is SAST

>>SAST stands for Static application security testing and its one of the easier types of security testing to implement in an existing or a new DevOps or DevSecOps pipeline
>>SAST tools run somewhere between the code and build stage, may be it'll be included in the test satge, but typically it's run pretty close to the code 
>>SAST we're thinking of statically testing the application. SAST is also known as Source Code Analysis 
>>it is white-box testing it cheks all the packages in your code it has 
>>it may false reports also some time
>>less expensive to fix the vulnerabilites 
>>it check differnet authentication patterns and scan config files potential security issues 
Tools like:-
1-sonarqube
2-Fortify.
3-Horusec
4-Snyk
5-Insider
6-HuskyCI

264-what is DAST

>>DAST stands for Dynamic application security testing. we check the application while running application for an issues . we are not loooking code directly
>>DAST is block box testing the tester dont know anything try to break the code or application logics 
>>more expensive to fix the vulnerabilites
>>we can discover run-time environment related issues
Tools:-
1-PortSwigger Burp-Suite
2-OWASP-ZAP
3-Veracode
4-Nuclei
5-HCL AppScan

265-what is Fortify security tool

>>Fortify offers the most comprehensive static and dynamic application security testing technologies, along with runtime application monitoring and protection, backed by indus try-leading\
security research. Solutions can be deployed in-house or as a managed service to build a scalable, nimble Software Security Assurance program that meets the evolving needs of today’s
IT organization.

266-what is SNS

>>Amazon Simple Notification Service (Amazon SNS) is a managed service that provides message delivery from publishers to subscribers (also known as producers and 
consumers). Publishers communicate asynchronously with subscribers by sending messages to a topic, which is a logical access point and communication channel

267-what is Simple Queue service

>>Amazon Simple Queue Service (SQS) lets you send, store, and receive messages between software components at any volume, 
without losing messages or requiring other services to be available.

268-what is first in first out

>>FIFO (First-In-First-Out) queues have all the capabilities of the standard queues, but are designed to enhance messaging between applications
when the order of operations and events is critical, or where duplicates can't be tolerated.
>>For FIFO queues, there can be a maximum of 20,000 in flight messages (received from a queue by a consumer, but not yet deleted from the queue). 
If you reach this quota, Amazon SQS returns no error messages. A queue name can have up to 80 characters

269-Your business prefers to use its email address and domain to send and receive compliance emails. What service do you recommend to implement it easily 
and budget-friendly?

>>This can be accomplished by using Amazon Simple Email Service (Amazon SES), a cloud-based   email-sending service.

270-Describe SES.

>>Amazon offers the Simple Email Service (SES) service, which allows you to send bulk emails to customers swiftly at a minimal cost.
>>it will everyday 50000 emails every day like 24 hours. 14 emails per second 

271-what is AWS secret Manager

>>AWS Secret manager is a service for easier management of secret. Secrets can be passwords, DB Credentials API keys or any text
>>You can store and control to these secrets centrally by using one of the following
1-secret manager console
2-secret manager command line interface(CLI)
3-secret manager Https query API
4-AWS SDKS
>>secrets are secured by audting at a central location
>>secret manager integrates with AWS logging and monitoring service provides centrailized auditing
>>secrets are encrypted using AWS KMS key manager service
>>using cloud watch and cloud trail all events relating to secrets manager can be monitored

272-what is docker-ECR

>>Amazon Elastic Container Registry (Amazon ECR) is an AWS managed container image registry service that is secure, scalable, and reliable. Amazon ECR supports 
private repositories with resource-based permissions using AWS IAM
>>aws ecr get-login-password --region <your-region> | docker login --username AWS --password-stdin <your-account-id>.dkr.ecr.<your-region>.amazonaws.com
>>docker pull <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/<repository-name>:<tag>
>>docker build -t <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/<repository-name>:<tag> .
>>docker push <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/<repository-name>:<tag>

273-what is code smell

>>Code smells indicate a deeper problem, but as the name suggests, they are sniffable or quick to spot. The best smell is something easy to find but 
will lead to an interesting problem, like classes with data and no behavior.

274-What is Inspector in AWS

>>Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure.

275-what JSON (JavaScript Object Notation)

>>JSON stands for JavaScript Object Notation . JSON is a lightweight format for storing and transporting data
(e.g., sending some data from the server to the client, so it can be displayed on a web page, or vice versa).

{
"Name": "Anji",
"Age": "26",
"DOB": "01-01-1996",
"Address": [
    {
   	"Type": "permenanent",
	"DoorNo": "1-80/1",
	"Address-line": "ganesh nagar"
     },
     {
	"Type": "current",
	"DoorNo": "1/90",
	"Address-line": "neknampur"
      }
   ] 
}

276-what is YAML

>> Yaml is yet another markup languge.YAML is a human-readable data-serialization language
>>YAML is a human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted
>>xml,json,yaml are DTO  -->data transfer object

first type(key: value or list)
Name: Anji
Age: 26
DOB: 01-07-1996
Address:
- present: VDk
  street: ganesh nagar
  city: peddapelli
- current: hyd
  street: neknampur
  city: hyd 

==second type(map type)
Name: Anji
Age: 26
DOB: 01-07-1996
Address:
  perm-address:
    present: VDk
    street: ganesh nagar
    city: peddapelli
  current-address:
    present: VDk
    street: ganesh nagar
    city: peddapelli

277-what is XML

>>XML stands for eXtensible Markup Language. XML is a markup language much like HTML
>>XML was designed to store and transport data. XML was designed to be self-descriptive

ex:-
<person>
	<Name>anji</Name>
	<Age>26</Age>
	<DOB>01-07-1996</DOB>
	<Address>
		<H-No>1-80/1</H-No>
		<City>VDK</VDK>
	</Address>
</person>

278-What are the two types of AMI?

>>AMI is divided into two categories:
1-EBS-backed Instances.
2-Instance Store-backed Instances.

279-What is Nginx?

>>Nginx is a type of open-source web server that is especially used for reverse proxy, load balancer, mail proxy, and for the HTTP cache. 
Nowadays NGINX software is most commonly used by various companies. NGINX can run on Linux, Mac OS X, Solaris, AIX, HP-UX, and the BSD variants.
>>Nginx is an open supply web server and an opposite proxy server for HTTP, SMTP, POP3, and IMAP protocols with robust attention on excessive concurrency,
performance, and low reminiscence utilization. It is stated as “engine x”.

280-what is webserver

>>A web server is a computer that runs websites. It's a computer program that distributes web pages as they are requisitioned. The basic objective of the web server is to store, process and deliver 
web pages to the users. This intercommunication is done using Hypertext Transfer Protocol (HTTP)
>>webservers serves the static content it acts as reverse proxy to the app-servers 
ex:-Nginx, Apache, caddy, jetty 

281-use case of nginx web-server 

>>it is webserver and it is used for reverese proxy for webserver. and load-balancing the request and email proxy and http cache service 
>>it serves the static content 
>>you can setup reverse proxy videos in cloudwithsiva video10 

282-what is website and webapplication 

>>the content is always same its like static pages is website 
1-news website 
2-blogging websites 
3-wikipedia websites 
>>the content always change its Dynamic pages is webapplication 
1-Facebook
2-Instagram
3-Twitter
4-Gmail 
5-Amazon 

283-What is the default root folder nginx?

>>The way nginx and its modules work is determined in the configuration file. By default, the configuration file is named nginx. conf and placed in the
directory /usr/local/nginx/conf, /etc/nginx, or /usr/local/etc/nginx 

284-Can we deploy the Node.js application on the Tomcat server?

>>You can run Nodejs such as react or angular on JavaPipe's Tomcat service. You will need to use the npm run build to package the Nodejs for production.
It creates a build folder where you can package it as a war file. cd into the build directory and then run jar cvf webui.

285--what is the node.js build command

>>npm install 

286-where I can see the logs of nginx server or Tomcat server

>>By default, the access log is located at /var/log/nginx/access. log
>>You can override the default settings and change the format of logged messages by editing the NGINX configuration file ( /etc/nginx/nginx.)
>>The main log is in /opt/tomcat/logs/catalina. <date>. log 

287-In what language was the Nginx software being written?

>>The language in which the Nginx software is written is ‘C’ Language.

288-What are the main tasks of the Nginx web server?

>>The main task of the Nginx web server is to deploy dynamic HTTP content on a network using SCGI, Fast CGI handlers for scripts, 
WSGI application servers or Phusion passenger module.
>>Nginx is also used to serve as a load balancer.

289-What are the difference between Nginx and Apache?

Firstly, Nginx is an event-based web server and Apache is a process-based server.
Nginx is best known for its speed and Apache is best known for its power.
Nginx is the best when it comes to memory consumption and connection whereas Apache is not best in this category.
In Nginx, a single thread is handling all of the requests whereas in Apache single thread handles a single request.
Nginx is best when you want the load balancing. But Apache will refuse the new connection when traffic reaches the limit of the process.
Apache provides lots of functionality as compared to Nginx.

>>A web server is a software application that delivers web content, such as web pages, to client computers that request it via the Internet. The server receives and processes HTTP requests
from clients and responds with HTML documents or other types of content that the client’s web browser can display.
>>Apache and Nginx are two popular web server software applications that are used to serve web content. Both are open-source and widely used in production environments. 
While both can serve static and dynamic content, there are some differences between them.
>>Apache is a very popular web server that has been around since the mid-1990s. It is known for its flexibility and ease of use, and it supports a wide range of modules and plugins
that can be used to add functionality. Apache is also widely used on shared hosting environments, as it can be easily configured to serve multiple websites from the same server.
>>Nginx, on the other hand, is a newer web server that was first released in 2004. It is known for its high performance and scalability, and it is often used to serve high-traffic websites.
Nginx uses an event-driven architecture that allows it to handle a large number of requests with low resource usage. It is also commonly used as a reverse proxy and load balancer, 
in addition to its role as a web server.
>>In terms of working mechanisms, both Apache and Nginx work by listening on a specific port for incoming requests. When a request comes in, the server processes it and generates a response,
which is sent back to the client. Apache uses a multi-process or multi-threaded model, where each incoming request is handled by a separate process or thread. Nginx, on the other hand, 
uses an asynchronous, event-driven model, where a single process can handle many connections simultaneously.

290-What are the types of versions of the Nginx web server?

Mainline - This version of Nginx contains the latest features and bug fixes that are up to date. It is also reliable but it may contain some experimental modules.
Stable- This version of Nginx doesn’t have the latest features but has critical bug fixes. A stable version is always recommended for the production servers.

291-What are the controls used in the Nginx web server?

Stop
Quit
Reopen
Reload

292-What is a directive in Nginx?

>>A directive in NGINX is used to route the server where to look for a resource by including files and folders while matching a location block against a URL.

293-What Are the Features Of Nginx?

Simultaneous Connections with low reminiscence
Auto Indexing
Load Balancing
Reverse Proxy with Caching
Fault Tolerance

294-How to start and stop the nginx server?

>>sudo systemctl start nginx

295-What do you mean by Apache Web Server?

>>Apache web server is the HTTP web server that is open source, and it is used for hosting the website

296-What is the port of HTTP and https of Apache?

The port of HTTP is 80, and HTTP is 443 in Apache.

297-Where are the configuration directories of the Apache webserver?

>>cd /etc/HTTP and type ls -l

298-What do you mean by the log files of the Apache webserver?

>> We can access the log files of the Apache server from the below location: /var/log/httpd/access_log and error log from /var/log/httpd/error_log

299-What do you mean by Apache Virtual Hosting?

>>Hosting multiple websites on a single web server is known as Apache Virtual Hosting. There are two types of virtual hosting:
Name-Based Virtual Hosting and 
IP-Based Virtual Hosting.

300-How to stop/start Apache Web Server?

>>You can restart by going to the Apache instance location >> bin folder and execute Apachectl script.

./apachectl stop
./apachectl start
You may also use a script located in /etc/init.d/. Mostly it will be named either “Apache” or “HTTPD”

/etc/init.d/apache stop
/etc/init.d/apache start

Another procedure would be to use services

httpd stop
service httpd start

301-What are the log files generated by Apache?

>>There are two popular log files created;

access.log – all request details with the status code
error.log – capture all the errors within Apache or connecting to the backend

302-How to secure a Website hosted on Apache Web Server?

>>There are multiple ways to secure the Apache webserver including the following.

Implementing SSL
Integrating with WAF (Web Application Firewall) like ModSecurity, etc.
Using a cloud-based security provider

303-What’s the difference between Apache Web Server and Apache Tomcat?

>>Apache Web is an HTTP server to serve static contents whereas Tomcat is a servlet container to deploy JSP files.

>>You can always integrate Apache HTTP with Tomcat, however, based on the requirement you need to choose either one. If you need a proper web server, 
then Apache HTTP else Tomcat for JSP-Servlet Container.

304-where is the Apache root directory

>> The Apache web server serves documents stored in the var/www/html directory by default. 

305-What is Apache Tomcat?

>>Apache Tomcat is basically a Web Server and Servlet system that is open-source (i.e. freely available on the Internet)
It is the server mostly used by Java Developers.

The server is nothing but a computer program that provides service to other computers.

There are basically two types of servers:

Application Server
Web Server
Apache Tomcat offers HTTP protocol, which means the user can connect with the server from anywhere by the URL provided and can access the Java application.

This is very easy and simple to install and is compatible with any Operating System.

306-What are the types of batch files with the help of which we can Start and Stop the Apache Tomcat Server?

>>There are basically two types of batch files with which we can Start and Stop the Server.

Startup.bat
Shutdown.bat

307-What are the vital benefits of Running Tomcat as a service?
The benefits of Running Tomcat as a service are:

>>Automatic Startup – If the Tomcat window service starts up automatically then it would be helpful when we want to start the system remotely.
>>Security – It allows you to execute under a special account which is protected from the other accounts.
>>Starting off the server without active user login: So even if there is no active user, the available server can be started.

308-you can configure the online commands like this to start and stop

>>ln -s /opt/tomcat/bin/startup.sh /usr/local/bin/tomcatup
>>ln -s /opt/tomcat/bin/shutdown.sh /usr/local/bin/tomcatdown

309-how to copy your war file into your tomcat 

>>>cd /usr/local/tomcat
>>cd webapps.dist
>>cp -R * ../webapps/ 

>> You can copy your build file into the web apps directory

310-how to add SSL certificate into Apache server

>>need to create the .crt file and private.key then add it into the conf folder and add it 
>> Then open .conf file 
add this details
<VirtualHost *:443>
    ServerName your_domain.com
    DocumentRoot /var/www/HTML
    
    # SSL Configuration
    SSLEngine on
    SSLCertificateFile /path/to/your_domain.crt
    SSLCertificateKeyFile /path/to/your_domain.key
    SSLCertificateChainFile /path/to/intermediate_certs.crt
</VirtualHost>
>>and replace your domain 

311-While we start the tomcat server its not starting

>>check first java installed and set the home path or not
>>chek any system or server running in same port 8080
>>check the you hav sufficent permissions is there are not user permissions
>>check file permissions
>>check the system resources like memory and cpu and disk-space availble
>>Review configuration files: Double-check the configuration files, such as server.xml, web.xml, and any application-specific configuration files, for any errors or misconfigurations

312-what is the full of form WAR file

>>Web Archive File 

313-while installing any package its not installing what will be the problem

>>you have privellges or not

314-what is CORS and SOP

--sop->same origin policy
>>The same-origin policy is a browser security mechanism that restricts how a
document or script loaded by one origin can interact with a resource from another
origin.
It helps isolate potentially malicious documents, reducing possible attack vectors
>> The browser will implement the SOP in all browsers beacuse of security
>> This implementation happiness we can't see the data of your own subdomain also
>>it checks the three objects
>>protocal >https:// (or) http://
>>site >google (or) facebook
>>port number >80 (or) 443
--cors->cros origin resource policy
>>Cross-Origin Resource Sharing (CORS) is an HTTP-header based mechanism that
allows a server to indicate any origins (domain, scheme, or port)
other than its own from which a browser should permit loading resources.
Acess-control-allow-origin==>true,get,post,put,delete
>>the sop implmented in browser policy we cant read the data in your subdomain of
the same domian.
if we can use the cors we can read write the data from main domain to subdomain
>http://www.google.com
>http://admin.google.com

315-what is soft and hard link in cmd or symbolic link

>>Symbolic links (also called "soft" links) are files that point to a file or directory in your system, but don't mirror the other file's data.
>>soft link is an actual link to the original file, whereas a hard link is a mirror copy of the original file. 
>>ln -s /opt/tomcat/bin/startup.sh /usr/local/bin/tomcatup    -->it will create short-cut to start and down the server
>>ln -s /opt/tomcat/bin/shutdown.sh /usr/local/bin/tomcatdown         -->this soft link or symbalic link
>>ln /opt/tomcat/bin/shutdown.sh /usr/local/bin/tomcatdown   -->this hard link
>>tomcatup    -->it will start the server
>> If you delete the original file, the soft link has no value, because it points to a non-existent file. But in the case of hard link, it is entirely opposite.

316-what is absolute path and relative path in linux

>>In simple words, an absolute path refers to the same location in a file system relative to the root directory, 
whereas a relative path points to a specific location in a file system relative to the current directory you are working on.

317-Is this possible to send the mail using cli in command using shell -script

>>yes you need to configure the mail in your system its posssible to send the mail using cli in 
mail -s "$(date): Report on $(hostname)" "anji159258@gmail.com"

318-how to configure your mail server in linux

>>>>sudo apt-get install mailutils >>select internet site >>give domain name >>click yes
>>sudo vi /etc/postfix/main.cf
realyhost = [smtp.gmail.com]:587

#Enable SASL authentication

>>sudo vi/etc/postfix/sasl/sasl_passwd
[smtp.gmail.com]:587 anji1592@gmail.com
>>sudo postmap /etc/postfix/sasl/sasl_passwd

echo "test alert" | mail -s "system down" anji1592@gmail.com

tail -f /var/log/mail.log

>>not complted see the video in liked videos

319-what is the shebang line

>> Shebang is nothing but which shell are using to run your script that is shebang line

320-what is daily activites in shell-script

>>delete old files using shell script
>>taking inventory files like version and ips and those
>>monitoring files 
>>installing packages
>>to see versions
>>to see backup
>>if server stop sending a mail 

321-how to stop all services in linux terminal

>>killall

322-how to clear history in linux

>>history -c  //it will delete all history
>>history -d //it will delete the one line particular you can mention

323-day to day activities

>>my self K Anjaneyulu Iam from Karimnagar iam done by b-tech in mother theressa college in the stream of ece comming to my family background
we are 4 members mother father and younger brother. young brother is an android developer mother and father formers
>>iam doing devops engineer in keeno technologies 
>>currently we using agile methodology we manage the configuration management multiple machines and infrstructure as an on-premises cloud using tools like terraform
>>we deploy the applications and deliver the containers on the platform like kubernetes and end of the day take care of montoring this applications on kubernetes that
deployed as containers or pods and and we also montior tools like grafana promethus and node exporter

324-What are the tools and techniques that you can use in AWS to identify if you are paying more than you should be, and how to correct it?

Check the Top Services Table
It is a dashboard in the cost management console that shows you the top five most used services. This will let you know how much money you are 
spending on the resources in question.Cost Explorer
There are cost explorer services available that will help you to view and analyze your usage costs for the last 13 months. You can also get a cost
forecast for the upcoming three months.AWS Budgets
This allows you to plan a budget for the services. Also, it will enable you to check if the current plan meets your budget and the details of how you 
use the services. Cost Allocation Tags
This helps in identifying the resource that has cost more in a particular month. It lets you organize your resources and cost allocation tags to keep 
track of your AWS costs.

325-Difference between AWS CloudFormation and AWS Elastic Beanstalk?

AWS-CloudFormation:-
1-Infrastructure as a code service that automates the deployment and management of AWS Services
2-It is used for Infrastructure Automation
3-can deploy both infrastructure and application code
4-support wide range of AWS resources
5-offers a high degree of customization and control over the deployment process
6-high and flexible but requires more time and effort to set up
7-more complex requires a good understanding of AWS infrastructure and services
8-free pay only for the resources created
9-best for managing and automating complex infrastructure
AWS-Elastic-Beanstalk
1-Platform as a service(Paas) that automates the deployment and management of applications
2-Application Deployment Automation
3-only deploys application code
4-supports only certain languages and application types
5-provides pre-defined opinionated deployment process
6-limited flexibility but easier and quick to set
7-less complex requires less knowledge of AWS infrastructure and services
8-free pay only for the resources created
9-best for deploying and managing web-applications

326-What are the meaning of different error codes such as 202, 404, 501

>> 100=continue (or) Informational
2XX-->>Success
>> 200=ok
>> 201=created
>> 202=Acepted
>> 203 Non-Authoritative Information
>> 204=Delete or no content
3XX-->>Redirects
>> 301=moved permanently
>> 302=found
>> 304=Not modified
4XX-->>Client side derror
>> 400=Bad request
>> 401=unauthorized
>> 402=payment required
>> 403=forbideen=>not allowed and is similar to 401
>> 404=not found
>> 409=conflict
5XX-Server side errors
>> 500=internal server error
>> 501=not implemented
>> 502=bad gateway
>> 503= service unavailable
>> 504=gateway timeout
>> 509=network timeout

327-what is the difference between reverse proxy and forward proxy

>> If you use Reverse proxy user doesn't know the proxy details and everything this request will be forwarded to another endpoint or any other servers also we use SSL termination in 
as reverse proxy
>>if you use a forward proxy user knows everything which will help you this things
1-client citrix
2-Anonymus
3-content filter
4-cacheing 

328-what  is ACM and SSL

1-ACM(AWS Certificate MAnager) :-ACM is a service provided by Amazon Web Services (AWS) that simplifies the process of provisioning, managing, and deploying SSL/TLS 
(Secure Socket Layer/Transport Layer Security) certificates for use with AWS services and your own applications.
Key Features:
>>Securing websites 
>>Securing communication between clients and AWS services
>>ACM provides free SSL/TLS certificates for use with AWS resources like Elastic Load Balancers (ELBs), Amazon CloudFront, and API Gateway.
>>Certificates issued by ACM are automatically renewed and can be easily integrated with supported AWS services.
>>ACM supports both RSA and Elliptic Curve Cryptography (ECC) algorithms for key pairs.
>>ACM also provides a management console for easy certificate management and monitoring.
2-SSL(Secure Socket Layer):-is a cryptographic protocol that provides secure communication over a computer network. It is the successor to TLS (Transport Layer Security), which is now the more commonly 
used protocol. SSL/TLS works by establishing a secure connection between a client (such as a web browser) and a server (such as a website). This connection is encrypted, which means that only the client
and server can read the data that is exchanged between them.
Key Features:
>>Encryption: SSL/TLS encrypts data to ensure that sensitive information, such as login credentials or credit card details, is transmitted securely.
>>Authentication: SSL/TLS provides a mechanism for authenticating the server to the client, assuring the client that it is communicating with the intended server.
>>Data Integrity: SSL/TLS ensures the integrity of transmitted data, preventing it from being tampered with during transit.
>>Widely Adopted: SSL has been succeeded by TLS, and both are widely adopted standards for securing online communication.
>>Securing websites and web applications to protect user data.
>>Encrypting data transmitted over email (SMTP/IMAP) or other network protocols.
>>Securing communication between servers and APIs.

329-what is the advantages of using API-Gateway 

>>using API-Gateway in micro-services or monolithic application the request will come api-gateway like authentication or authorization it will fetch the data using APIS 
>>it will also store or maintain the cache mechnisam the performance will be improved latency will reduced 
>>it will cpature the metricss like
1-it will capture the how many API-call will recieved 
2-how much latency will caused between the request and responce 
3-how many call is failed 
4-which backend is not responding using APIS

330-upload file into s3 bucket using aws cli

>>aws s3 cp local-file s3://bucket-name/destination-prefix/
>>aws s3 cp example.txt s3://my-bucket/
>>aws s3 cp example.txt s3://my-bucket/my-directory/
>>aws s3 ls s3://my-bucket/

331-how to delete a file in s3 bucket using aws cli 

>>aws s3 rm s3://bucket-name/path/to/file
>>aws s3 rm s3://my-bucket/example.txt
>>aws s3 ls s3://my-bucket/

332-how to create or delete the s3 bucket using aws cli 

>>aws s3api create-bucket --bucket BUCKET-NAME --region REGION
>>aws s3 rb s3://BUCKET-NAME --force
>>aws s3 mb s3://anji159258

333-how to send a nginx docker container logs to AWS cloud watch if you use ECS also  

1-Install the AWS CloudWatch Logs Agent: First, you need to install the AWS CloudWatch Logs agent on your EC2 instance. This agent will be responsible for streaming logs from your Docker containers to
CloudWatch.
2-Create an IAM Role: Ensure your EC2 instance has an IAM role that grants permissions to write logs to CloudWatch Logs. Attach the role to the EC2 instance where your Docker containers are running.
3-Configure Docker Logging Driver: To send Docker container logs to AWS CloudWatch, you need to configure Docker to use the awslogs logging driver. You can do this by adding the --log-driver and --log-opt
options 
when starting a container.

For example:
>>docker run -d --name nginx-container --log-driver=awslogs --log-opt awslogs-region=your-region --log-opt awslogs-group=your-log-group nginx
>>Replace your region with your AWS region and your log group with the name of the CloudWatch Log Group you want to send logs to.

4-Configure the AWS CloudWatch Agent: The AWS CloudWatch Logs agent should be configured to watch the log files generated by the Docker containers and send them to CloudWatch.

>>Edit the CloudWatch agent configuration file, typically located at /etc/awslogs/awslogs.conf, and add an entry for each log stream you want to send. For Nginx, the log path 
might be /var/log/nginx/access.log and /var/log/nginx/error.log. An example entry might look like:

[nginx-access-logs]
file = /var/log/nginx/access.log
log_group_name = your-log-group
log_stream_name = nginx-access
datetime_format = %d/%b/%Y:%H:%M:%S %z
Make sure to adjust the file, log_group_name, and log_stream_name to match your specific setup.

5-Restart the CloudWatch Agent: After editing the configuration file, restart the AWS CloudWatch Logs agent to apply the changes:

sudo service AWS-logs restart

6-View Logs in CloudWatch Console: Your Nginx container logs should now be streaming to the specified CloudWatch Log Group. You can view and analyze the logs from the AWS CloudWatch Console.

Please note that this is a high-level overview, and you may need to adjust the configuration based on your specific requirements and log file paths. Make sure to follow AWS security best practices 
when setting up log forwarding to CloudWatch.

ECS using:-
1-Update Task Definition:
>>In your ECS task definition, you need to specify that you want to use the awslogs logging driver for the Nginx container.
>>For example, in your task definition JSON or in the AWS Management Console, you can add a log configuration section for the container:

JSON
Copy code
"logConfiguration": {
  "logDriver": "awslogs",
  "options": {
    "awslogs-group": "your-log-group",
    "awslogs-region": "your-region",
    "awslogs-stream-prefix": "nginx-container"
  }
}
Replace your log group with the name of the CloudWatch Log Group you want to send logs to, and your region with your AWS region.
2-Create an IAM Role:-Ensure that your ECS task definition uses an IAM role with the necessary permissions to write logs to CloudWatch Logs. The IAM role should have a policy allowing logs: CreateLogStream,
logs: PutLogEvents, and logs: CreateLogGroup actions on the specified log group.
3-Configure the AWS CloudWatch Agent: You'll need to install and configure the AWS CloudWatch Logs agent on the EC2 instances that run your ECS tasks. The agent will be responsible for streaming the 
container logs to CloudWatch.
4-Run ECS Tasks:-Launch your ECS tasks as usual. Ensure that they use the updated task definition that specifies the AWS logs logging driver for the Nginx container.
5-View Logs in CloudWatch Console: Your Nginx container logs should now be streaming to the specified CloudWatch Log Group. You can view and analyze the logs from the AWS CloudWatch Console.
>>Please note that this approach is for ECS tasks running on EC2 instances. If you are using AWS Fargate, the setup may be slightly different, as Fargate abstracts the underlying infrastructure. 
Ensure that the AWS CloudWatch Logs agent is properly configured and running on the EC2 instances that host your ECS tasks.

334-Types of Load-balancing Techniques like Algorithms

1-Round-Robin(weighted-an-unweighted):- Distribute incoming requests sequentially across all servers in the list
2-Least-Connections(weighted-an-unweighted):- Direct traffic to the server with the fewest active connections 
3-Least-Response-Time:-Routes incoming requests to the server with the lowest response
4-Least-Bandwidth-Method:- Directs Traffic to the server that's currently handling the least amount of data
5-Least-Packages:- send incoming requests to the server that has received the fewest number of packets
6-IP-Hash:-Determines the server to handle a request by hashing the IP address of the Client
7-Sticky-Sessions:- useful for applications that store session information locally on the server
8-Layer7-Load-Balancing:-Distribute Incoming Traffic across multiple targets based on content type URL or Header
9-Geographical-Load-balancing:- Routes requests based on the geographic location of the client
10-DNS-Load-Balancing:- uses the DNS to direct users to the best server based on various strategies
11-Transport-layer-protocol-load-balancing:- Distribute incoming traffic based on transport-layer protocols

335-what is the parameter store

>>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, 
and license codes as parameter values.
>>this will not rotate your password but the Sceret manager will do
>> Some kind of the features are not available in comparison to the secret manager 

336-what is mounting in Linux

>>In Linux, "mounting" refers to the process of associating a filesystem with a specific location (mount point) in the directory tree of the file system hierarchy. When you mount a filesystem, 
you are essentially making the contents of that filesystem accessible at the specified mount point in the directory structure.
1-mountpoint mount point is a directory on the file system where an additional filesystem is attached. This is the location where the contents of the mounted filesystem become accessible.
Common mount points include directories like /mnt, /media, or custom directories created for specific purposes.
2-Filesystem:A filesystem is a hierarchy of directories and files that is used to organize and store data on a storage device. Examples of filesystems in Linux include ext4, XFS, and others.
3-Mount Command:The mount command is used to attach a filesystem to the system's directory tree. The basic 
mount [options] device|mount_point
4-Unmount Command:The umount command is used to detach a mounted filesystem. The syntax is:
umount mount_point

this is the process:-
>>fdisk -l or lsblk 
>>Identify the device name of your 1 TB hard disk. It will typically be something like /dev/xvdb or /dev/xvdc.
>>Type n to create a new partition.
>>Choose the primary partition type (usually the default, press Enter).
>>Specify the starting and ending sectors. For a 500 GB partition, you might specify +500G as the ending sector.
>>Type t to change the partition type (if needed) and set the file system type.
>>Type w to write the changes to the disk and exit.
>>After you've created the partition, you should have a new partition (e.g., /dev/xvdb1)
>>sudo mkfs.ext4 /dev/xvdb1   -->You need to create a file system on the new partition. The following example creates an ext4 file system:
>>sudo mkdir /mnt/mynewdrive   -->Create a directory where you want to mount the new partition.
>>sudo mount /dev/xvdb1 /mnt/mynewdrive  -->Mount the new partition to this directory
>>vi /etc/fstab
/dev/xvdb1 /mnt/mynewdrive ext4 defaults 0 0
:wq   -->To ensure the partition is mounted at boot, add an entry to the /etc/fstab file. Edit the file using a text editor (e.g., sudo nano /etc/fstab)
>>df -h   -->You can verify that the partition is correctly mounted by running

337-an user, only he can terminate the ec2 and start only creating iam policy and full access code

Start&Terminate:-
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ec2:TerminateInstances",
        "ec2:StartInstances"
      ],
      "Resource": "arn:aws:ec2:region:account-id:instance/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "iam:CreatePolicy"
      ],
      "Resource": "*"
    }
  ]
}
Full-Access:-
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "ec2:*",
      "Resource": "*"
    }
  ]
}

338-what is the parameter group in MySQL or RDS in AWS 

>>If you want to modify any setting or any changes then like if you want to change the max=connection for example your RDS max connection 20 it small instance but that pods connected 2 pods is okay 
you used hpa the pods increased to 20 it will not connect because of the RDS instance type max connection also you need to change 

339-what is Operating System 

>>What is the operating system it is a combination of software and hardware
>>Like os will take the instructions from to convert into 0/1 binrays into hardware
>>Like clicking something converts into 0/1 in software or OS in the background you will not see that but it will work that way only the computer understands binaries we changed that like 
human-readable for understanding for example you want to run binaries it big headache Or it's tds task so that why the operating system this when we run instructions it runs in the background in binaries 
so this operating system

340-what is client-server explain day to day also

>>Who is server the who is asking for some information for someone that means your asking information from your friend your friend is server your client
>>Who is client asking information from someone like your asking details from your friend here you is client your friend is server

341-what is best practice for download 

>>If you download pem directly you can create ppk if you lost pem you can't create the pem Or ppk if you download pem you can create ppk if you lost pem you can't create one more time using ppk
>>Always try to download to pem  not ppk if you download pem you can create the ppk you can store pem somewhere for backup

Inlinux 
>>puttygen ppkkey.ppk -O private-openssh -o pemkey.pem
>>puttygen pemKey.pem -o ppkKey.ppk -O private
>>Pem is for linux purpose. ppk is for putty or windows

342-what is called in AWS everything

>>in AWS Everything is called as Resource

343-what is shared storage and distributed storage

Shared-Stoarge:-Shared storage consolidates files and other information in a central resource that can be accessed or shared among multiple users and data systems simultaneously. Gathering and storing
data in a single, shared resource streamlines archive and backup processes, and simplifies information security. Shared storage solutions include network attached storage (NAS), 
storage area networks (SAN), and redundant arrays of independent disks (RAID), as well as dedicated storage servers and cloud storage.
Distributed Storage:-Distributed block storage is a computer data storage architecture which the data is stored in volumes across multiple physical servers, as opposed to other storage architectures
like file systems which manage data as a file hierarchy, and object storage which manages data as objects

344-can i add the one instance in one load-balancer using anji target group created another target group using same instance is it possible to create that instance in one more load-balancer

>>no its possible if you created one instance in using load-blancer using the target group.then its not possible use one more load-balancer 

345-why the load-balacner and nat-gateway in public subnets

>>so when we create the load-balacner we must select the public subnets only in load-balancer or nat-gateway then we add the route into private route table and select the target groups under private instance    

346-can i create the target group under like not instances in that place is IPs and another load-balancer or lambda function

>>yes you can create rather than use instance you can use IPS and load-balancer and lambda functions 

347-what is physical servers

hardware-->operating-systems-->application 
--Problams
1-Take time to boot 
2-physical server is down no backup until you create extra server 
3-need to configure everything 

1-vm is faster to boot 
2-if the server is down we can configure next one is easy 
3-

348-what is Caching 

>>Caching is nothing but storing some data in your cache server dont hit every request on the real webserver any db-server 

349-mostly frontend will work 80 port 

>>backend will work on 8080 either java or python or nodejs also 
>>if you use java most compile if you node or python no need to compile only java required to compile  
>>python for pip , nodejs for npm , java for maven , dotnet for msbuild 
>>APP/WEB apps are called stateless -->database will follow the CRUD operations  
>>DB apps are called stateful. it take the request and give back the responce to customers that why called stateful. request come and take the responce back is called stateful   
>>nginx used as reverse proxy for web applications. like any if the servers crash we can create immediatly but coming to DB its not possible if loose the data we can't recreate it until you have backup  

350-what is forward proxy and reverse proxy 

>>forward proxy:-
1-A Forward proxy sits infront of users then after the traffic will go the internet then hit the servers 
>>if you use forward proxy server dont understand real clients. hiding our identity for security purpose companies can impose restrictions not to use particular sites access not do naything from internet  
>>Clients configure their applications or systems to use a forward proxy to access external resources.
>>When a client makes a request, it first goes through the forward proxy, which then forwards the request to the target server.
>>The target server sees the request as coming from the forward proxy, not the original client.
Use-Cases:-
1-Client Anonymity
2-Caching
3-Traffic Control
4-Logging
5-Request/Response Transformation
6-Encryption
Examples:- 	Squid, Proxy, Tor 
>>Reverse proxy:-  
1-A reverse proxy sits in front of servers and acts on behalf of those servers to handle incoming client requests.
>>to hide the web appliation so organization puts the web server infront load-balancer then webserver then app-servers then db-servers when user hit the url it will connect the loadBalancer then 
web-server this act as reverse proxy  
>>Clients send requests to the reverse proxy, thinking it is the server.
>>The reverse proxy then forwards the request to the appropriate backend server.
>>The backend server processes the request, and the reverse proxy sends the response back to the client.
Use-Cases:-
1-Server Anonymity
2-Caching
3-Load Balancing
4-DDoS Protection
5-Canary Experimentation
6-URL/Content Rewriting
Examples:- Nginx, Apache, HAProxy

351-when npm install used build once build happens what you will get 

>>package-lock.json and node_modules this two files will get 
>>if you build using java application then you will get .Jar or .War files

352-if you compile java code where the jar file will go 

>>if you compile or build the java code by default it will go target/shipping.jar it will go target folder under anyfile name what you will give 

353-what is TTL in Route53 

>>The time for which a DNS resolver caches a response is set by a value called the time to live (TTL) associated with every record. Amazon Route 53 does not have a default TTL for any record type.
>> It is like cache your request on the server by default 300 seconds is there you reduce how much you want 
>>Best practice 
1-Short TTLs (e.g., 300 seconds or 5 minutes):
2-Medium TTLs (e.g., 3600 seconds or 1 hour):
3-Long TTLs (e.g., 86400 seconds or 1 day, or even longer):

354-how to attch a role to eks cluster 

>>by using OIDC when you creating role you can select the web-identity and select the eks arn then next then attach your policy give role name thats it 

355-what is CRUD

>>In computer programming, create, read, update, and delete are the four basic operations of persistent storage. CRUD is also sometimes used to describe user interface conventions that
facilitate viewing, searching, and changing information using computer-based forms and reports.

C    -->Create
R    -->Read
U    -->Update
D    -->Delete

356-what is HTTP Methods in Web-Application

GET:-GET Request is used to request information from a resource such as a website or server 
POST:-It creates a new resource on the backend(server)we can send data to the server in the request body 
PUT:-IT will update the name or any details in the server or appliation
PATCH:-it will work same put,patch the updates of resource it updates data in server or application
DELETE:-it will delete the data in server or application
HEAD:-the HEAD method is similar to GET method but its doesn't have any response body 
OPTIONS:-this method is used to get information about the possible communcation option for the give URL or asterisk 
TRACE:-the trace method is for diagnosis purposes it createa loop-back test with the same request body that the client sent to the server before 
CONNECT:-the connection method is for making end to end connections between client to server     

357-what are code standards and code best practice

code standards:-
1-code should Never be kept in local(Laptop/Desktop)
2-always try to push code to central like (github)
3-code should developed in local only 
4-always choose editors as per your comfort
5-editors always improve productivity 
best practice:-
1-code should never hardcode the username and passwords 
we can passthat password along with the server system only like export root_mysql_password 
sudo -E anji.sh  ##this anji.sh has written code if [ -z "${root_mysql_password}" ]; then
  echo "Variable root_mysql_password is missing"
  exit 1
fi  ##this root password has given some other command has variable
mysql_secure_installation --set-root-pass ${root_mysql_password}  ##like this this password will be taken direcctly from the server it self because we written environment variables if not will fail   
2-code DRY vs WET, Always try to make code DRY 
3-rerun of automation should never fail 

358-how to configure one domain www.anji.com to another sales.anji.com its not a subdomain sub-sub-domain

>>you have two aws accounts for example 1st account anji.com 2nd account sales that domain need add this 1st why because takenover by my company how we achive this 
>>first create the www.anji.com in 1st account then name server you are pasting godday or hostinger right then go the second create sales.anji.com this will give you name servers then paste that 
servers into 1st account like this create subdomain sales.anji.com select the NS record paste that 2nd account NS record into 1st account then automatically name resloution will happen then you can create
the subdomain 2nd account like 
sampi.sales.anji.com in 2nd account only . like that you can you can AZURE also    

359-how to build angular application

>>npm install -g @angular/cli    -->Make sure you have the Angular CLI installed globally on your system. If you haven't already installed it, you can do so using the following command
>>ng build   -->command Run the ng build command to build your Angular application. By default, this command will create a dist/ directory with the compiled and optimized application code. 
You can specify various options and flags with this command, such as the --prod flag for a production build

360-what is the difference between SQL or NO-SQL 

SQL:-sequl database it means store the data rows and columns data only 
NO-SQL:-non-relastional database it will store rows and columns and more than that like images or videos 

361-what is rabbitmq 

>>RabbitMQ is an open-source message-broker software that originally implemented the Advanced Message Queuing Protocol and has since been extended with a plug-in architecture to support Streaming 
Text Oriented Messaging Protocol, MQ Telemetry Transport, and other protocols
>>RabbitMQ is a messaging Queue which is used by some components of the applications.
>>port number of rabbitmq 5672 
>>for example take Whatsapp you send message to your friend how the message will go your friend in the middle their is message streaming platform like rabbitmq or kafka like your friend number always
pull the request from rabbitmq your pushing rabbitmq then it will send to friend and their is Queue option if your not online it will be on Queuewhenever it will be online then that messages will be pulled
you from your rabbitmq  
>>your publisher your-friend subscriber to buy something the for ecomerce for example only 

362-use case of rabbitmq

1-Messaging System
2-Activity Tracking
3-Gather metrics from many differnet locations
4-Application logs gathering
5-Stream processing(with the kafka Streams API fro example)
6-De-coupling of system dependencies
7-Integration with spark, Flink, Storm, Hadoop, and many other Big Data technologies
8-it uses microservice public

363-how to install specific version of nginx or httpd in linux 

>>sudo apt install nginx=1.16.* 
see video of cloudwithsiva video no 10

364-what is the difference between public and private nat-gateway in aws

public-nat-gateway:-

>>Use Case: A public NAT gateway is typically used in a public subnet. Instances in public subnets can have public IP addresses and are intended for resources that need to be directly accessible from 
the internet. Public NAT gateways allow these instances to initiate outbound traffic to the internet while also providing a level of security.
>>Configuration: To set up a public NAT gateway, it's placed in a public subnet and associated with an Elastic IP address. The route tables for private subnets are configured to route outbound traffic
to the public NAT gateway. Instances in private subnets send their internet-bound traffic to the public NAT gateway, which then performs Network Address Translation (NAT) to make the traffic appear as
if it's coming from the public NAT gateway's Elastic IP address.
>>Security: A public NAT gateway is typically used when you want your private instances to have limited inbound traffic from the internet. In this configuration, the security groups for the private 
instances should be configured to allow only the necessary outbound traffic.

private-nat-gateway:-

>>Use Case: A private NAT gateway is used in a private subnet and is designed for resources that should not have direct public internet access. Instead, private NAT gateways are used for outbound traffic
from private instances that need to access the internet, such as for software updates or retrieving external data. This approach provides enhanced security for the private instances.
>>Configuration: Similar to a public NAT gateway, a private NAT gateway is placed in a public subnet, but it doesn't have a direct public IP address. Instead, it routes traffic to the internet through a 
Network Address Translation (NAT) gateway service managed by AWS. The route tables for private subnets are configured to route outbound traffic to the private NAT gateway. The instances in private subnets
send their internet-bound traffic to the private NAT gateway, which then routes it to the AWS NAT gateway.
>>Security: Private NAT gateways provide a higher level of security for private instances. Since they don't have a public IP address, they can't receive direct inbound traffic from the internet. 
This configuration helps to protect sensitive or critical resources.

365-what are the load-balancing algorithms in load-balancers

1-Round Robin: In Round Robin load balancing, each new connection or request is directed to the next server in a circular order. It evenly distributes traffic among all available servers. 
It is simple and can be effective, but it doesn't take into account server load or response times.
2-Least Connections: The Least Connections load balancing method routes traffic to the server with the fewest active connections. This helps in distributing traffic based on server load and ensures
that servers are not overwhelmed.
3-Least Response Time: This method directs traffic to the server with the lowest response time. It is based on the idea that faster-responding servers should receive more traffic. 
This method can be effective in minimizing response time for end users. 
4-IP Hash:IP Hash load balancing uses a hash function to map the client's IP address to a specific server. This ensures that a specific client always connects to the same server, which can be useful
for session persistence or stateful applications. 
5-Weighted Round Robin: Weighted Round Robin allows you to assign different weights to servers. Servers with higher weights receive more traffic. This is useful when servers have 
different capacities or performance levels. 
6-Weighted Least Connections: Similar to Weighted Round Robin, Weighted Least Connections allows you to assign weights to servers but routes traffic to the server with the fewest connections,
taking server load into account. 
7-Session Persistence (Sticky Sessions): Sticky sessions, also known as session persistence, ensure that a user's requests are always directed to the same server for the duration of their session.
This is crucial for stateful applications that rely on a specific server for session data
8-Content-Based Load Balancing: Content-based load balancing examines the content or characteristics of incoming requests to make routing decisions. For example, it can route traffic based on 
URL paths or specific HTTP headers.
9-Geographic Load Balancing: Geographic load balancing directs traffic based on the geographical location of the client, which can be useful for serving content from servers closer to the user,
reducing latency, and complying with data sovereignty regulations.
10-Dynamic Load Balancing: Dynamic load balancing adapts to changing server conditions by continually monitoring server health and traffic conditions. It can automatically adjust the 
routing decisions to ensure optimal performance and high availability.

366-what is instance 

>>instance is nothing but server. "instance" typically refers to a virtual machine (VM) that you can run in the cloud. These instances are a fundamental part of AWS's Elastic Compute Cloud (EC2) service,
which provides scalable computing capacity in the cloud.
>>is is virtual machine 

367-how to login username and password to linux ec2 instance 

>>create ec2 instance connect to the instance using ssh keys then create root password
>>ssh -i /path/to/your/key.pem ec2-user@your-instance-public-ip
>>sudo passwd root
>>sudo nano /etc/ssh/sshd_config
>>PermitRootLogin yes
>>sudo service ssh restart
>>ssh root@your-instance-public-ip

368-what is bare images

>>bare images is nothing but it will not come with any software its base image 

369-what is the difference between reload and restart

Reload:-Reloading typically means reloading a specific component or configuration within a system without stopping the entire system.
Example: In the context of web servers, a reload might involve reloading the configuration files or restarting a specific module or application without stopping the entire server.
Restart:-Restarting usually involves stopping the entire system or application and then starting it again. This action results in a fresh initialization of all components.
Example: Restarting a computer, server, or service involves shutting it down completely and then starting it anew. This ensures that all processes and configurations are reloaded from scratch.

370-what is mutable and immutable

>>if use mutable always take longtime to deploy change the version or OS Anything 
>>if use immutable always take lesstime to deploy change the version or OS or Anything 

371-what is instance refresh in auto-scaling 

>>Instance refresh in auto-scaling refers to the process of replacing instances within an auto-scaling group with new instances running the latest version of the application or operating system.
This process helps ensure that the auto-scaling group maintains a healthy and up-to-date fleet of instances.
>>video 71 26mins 

372-how to control java memory in java application or package 

>>uWSGI is a fast, open-source application server container that is widely used to deploy and run web applications and frameworks written in various programming languages. It acts as a bridge between
web servers and web applications, providing a communication interface for handling requests and responses.
>>if using package Running. The most basic way to run uWSGI is to tell it to start an HTTP server and import your application. If you're using the app factory pattern, you'll need to create a small Python
file to create the app, then point uWSGI at that.
ExecStart=/bin/java -XX:MaxRAMPercentage=90 -XX:MinRAMPercentage=50 -XX:InitailRAMPercentage=50 -jar /app/shipping.jar 

373-what is uwsgi configuration

>>vi /app/payment.ini 
[uwsgi]
wsgi-file = payment.py
callable  = app
master    = true 
processes = 4
threads   = 2
lazy-apps = true 
enable-threads = true 

socket  = 0.0.0.0:8080
protocal  = https 
uid  = 1
gid  = 1

374-heap space or out of memory

>>video 67 1:00:00

375-is this possible to change the log format in nginx logs 

>>yes you can see video 65 40 mins how to change
>>vi /etc/nginx/nginx.conf 

log_format main 'remote_addr $time_local $request $status $body_bytes_sent $request_time'  
>>this format used to collect required information ex: you have send the data elk you can search easily what you want 

376-what is mean by database sharding 

>>video 54 01-03-00 time 
>>videos 58 32 mins 

377-What is the maximum number of instances allowed in an Amazon DocumentDB cluster?

>>Instances	40
>>Manual cluster snapshots	100
>>Read replicas per cluster	15
>>Subnet groups	50


378-what is placement group in ec2 

>>

379-what is desire-capacity in Auto-scaling group 

>>how many system do you want to launch instances in AWS 

380-how role and policy works 

>>first create the role which service like ec2 then attach that poilcy to ec2 then create the policy permission like ssm parameter permission then attach this policy to role then that role attached 
to ec2 will bring the data from aws what permission you given 

381-how to replace any server ip in route53 using shell script 

IP=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=jenkins"  --query 'Reservations[*].Instances[*].PublicIpAddress' --output text)

echo '
{
  "Comment": "CREATE/DELETE/UPSERT a record ",
  "Changes": [{
    "Action": "UPSERT",
    "ResourceRecordSet": {
      "Name": "jenkins.devopsb70.online",
      "Type": "A",
      "TTL": 15,
      "ResourceRecords": [{ "Value": "IPADDRESS"}]
    }}]
}' | sed -e "s/IPADDRESS/${IP}/" >/tmp/jenkins.json

ZONE_ID="Z0366464237Z7LZLZPKFA"
aws route53 change-resource-record-sets --hosted-zone-id ${ZONE_ID} --change-batch file:///tmp/jenkins.json | jq .

382-what is Amazon landling zone how we can configure 


383-is document db and mongodb is same 

>>documnet db is mongodb alternative in aws 

384-how to track or see the login into server using bastion server 

>>using bastillion tool 

385-what is bastillion tool what is the use of it 

>>

386-what is monorepo & multirepo what is the difference between this

>>all the repos is one place or all the state files in one place is called monorepo 
>>one place to bother
>>runing multiple modules in order is needed means multiple executions
>>sharing this someone easy 
>>terraform apply -target=module.mysql  -->>you can apply single module if you want in terraform 

>>Making repos like state files any files in differnet repos is called multi-repos  
>>

387-how to check whether the server is running or not connecting 

1-check the server is running or not (ps -ef)
2-check the port is open or not (netstat -lntp)
3-check the ports open security group or not 
4-In case that service is accessed  by another server then you can check the reachability using telnet 14.98.164.230
5-restart the services or server 
6-repeat step 4
7-finally you can check the server 

388-how to do MongoDB bindIp address to listen to another service while connected to MongoDB when we give MongoDB URL to any other service 

>>vim /etc/mongod.conf 

net:
  port: 27017
  bindIp: 127.0.0.1 ##replace 0.0.0.0 or your web server IP like any public IP or private of your service 
:wq

>>systemctl restart mongod 

389-what is systemd service file

>>A systemd service file is a configuration file used by the system init system in many modern Linux distributions to define and manage services, daemons, and other system tasks. Systemd is a system 
and service manager that plays a central role in the boot process and service management on Linux systems. The service file specifies how a service should be started, stopped, and managed by systemd.

>>vi /etc/systemd/system/catalogue.service 
[Unit]
Description = Catalogue Service

[Service]
User=roboshop
Environment=MONGO=true
Environment=MONGO_URL="mongodb://<MONGODB-SERVER-IPADDRESS>:27017/catalogue"
ExecStart=/bin/node /app/server.js
SyslogIdentifier=catalogue

[Install]
WantedBy=multi-user.target
:wq

>>For example, your installed fronted and Mongodb service then mongod must talk this catalog service right you need to give the endpoint of Mongod 

390-create ec2 only view and delete the policy for users in AWS

>>To create an AWS Identity and Access Management (IAM) policy that allows a user to view and delete Amazon Elastic Compute Cloud (EC2) instances, you can use the following JSON policy document as a
starting point:

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:Describe*",
                "ec2:List*",
                "ec2:TerminateInstances"
            ],
            "Resource": "*"
        }
    ]
}

391-delete unused EBS volume using lambda

1-Create an IAM Role for Lambda:
2-Create a Lambda Function:
3-
import boto3

def lambda_handler(event, context):
    ec2 = boto3.client('ec2')
    response = ec2.describe_volumes()

    for volume in response['Volumes']:
        if 'Attachments' are not in volume:
            # This volume is not attached to any instance
            volume_id = volume['VolumeId']
            print(f"Deleting unattached volume: {volume_id}")
            ec2.delete_volume(VolumeId=volume_id)
4-Create a CloudWatch Events Trigger:
5-Create a CloudWatch Events Trigger:
6-Test Your Lambda Function:
7-Schedule the Lambda Function:
8-Monitor and Logging:
9-Alerts:
10-Cleanup Strategy:

392-how to raise a ticket in AWS 

>>

393-what is the difference between SRE and DevOps

SRE:-
>>A site reliability engineer's job is to ensure the high availability, reliability, and resilience of production systems and services. SRE responsibilities can encompass on-premises, hybrid cloud, and
public cloud environments in any given system.
>>Performance tuning and optimization fall on the SRE team, even in complex hybrid and multi-cloud environments. This requires automation and centralized tooling to ensure maximum team productivity.
The SRE team automates deployment, scaling, monitoring, and related tasks across these environments.
>>SRE teams also define and maintain customer SLAs within their area of responsibility. In addition, they provide technical and operations support to remediate cases of SLA system violations.
>>Designing, testing, and implementing disaster recovery plans is also an SRE responsibility. This requires proactivity and ownership by SREs to ensure their team's response to a disaster situation
is well-rehearsed and on point. Disaster recovery plans aren't meant to be shelfware; SRE teams should constantly test and improve their plans and practices.
SRE use cases:-
Proactive monitoring of system health to identify problems before they become significant issues that might impact operations and customer experience.
Automation of routine site monitoring and related tasks to improve SRE productivity, reduce human error, and free up SREs to work on more strategic tasks to improve site reliability and operations.
Incident management, which includes both resolving the incident swiftly and putting the tools and playbooks in place to ensure that the incident won't happen again.
Problems SREs solve:-
>>SRE teams focus on large-scale problems that could cost organizations money due to system outages. Here are some of the problems they solve:
>>Service disruptions. SREs have the monitoring, alerting, and incident response tools and playbooks to mitigate problems that disruptions bring.
>>Scalability challenges. These threaten the operations of commercial and public sector systems. Planning for increased workloads and traffic requires the specialized architectural expertise that
SREs bring to large-scale enterprise operations.
>>Slow response time. SREs address bottlenecks, optimize code, and implement caching strategies to improve response times and meet customer expectations.
SRE-Tools:-
1-Grafana
2-Prometheus
3-Kubernetes
DevOps:-
>>DevOps teams implement CI/CD pipelines to manage and maintain their organization's development infrastructure, including public cloud environments.
>>DevOps is responsible for automating the build, test, and deployment processes to increase the speed and efficiency of application delivery. This isn't a one-and-done task; DevOps teams must approach 
this task with an eye to continuous improvement.
>>DevOps teams should aim to continuously improve the deployment process by making it faster, more reliable, and more scalable. This requires the team to document and communicate improvements to the 
SRE team and other technical stakeholders.
>>Other DevOps responsibilities include ensuring the high availability and scalability of the systems they develop. DevOps also monitors and troubleshoots technical and security issues in development
and testing environments.
DevOps team use cases:-
>>Use cases for DevOps teams bridge the gap between development and operations, enabling faster software deployment, continuous improvement, and the continuous delivery of services. DevOps teams automate
repetitive tasks to remove human error and improve the overall efficiency of software development, delivery, and operations. Automation accelerates software delivery velocity, giving organizations a 
time-to-market advantage.
>>Another DevOps team use case is software testing -- DevOps teams are responsible for testing software before they deploy it to internal and external customers.
>>Monitoring and logging are also DevOps responsibilities. DevOps teams must continuously monitor the systems to detect any anomalies or potential issues. As more DevOps teams move to the cloud,
AI could begin to play a bigger part in data consumption and logging.
Problems DevOps teams solve:-
>>DevOps teams are responsible for alleviating and mitigating numerous problems that could crop up across the delivery pipeline. Lack of visibility into the delivery pipeline is one problem that
DevOps teams rectify. By using observability tools, DevOps teams capture and interpret actionable data on the current state of their development software.
>>DevOps teams also face problems with the upheaval of waterfall software development. DevOps teams are responsible for pivoting their organization from legacy processes to the agility of DevOps. 
This pivot improves software delivery velocity, software quality and security.
>>Lastly, not all the problems that DevOps teams solve deal with technology. DevOps teams must also tackle cultural barriers such as communication silos that stymy collaboration amongst developers
and stakeholders across their organization.
DevOps-Tools:-
1-Jenkins
2-Git
3-Docker
4-Kubernetes
5-AWS
6-Terraform

Collaboration points and similarities between SRE and DevOps:-

>>To deliver secure and quality software, SRE and DevOps teams must collaborate on a few essential points.
>>When the organization launches a new feature or service, SRE teams should collaborate with their DevOps counterparts to ensure the scalability and reliability of the new offerings. 
This responsibility ties back to site reliability engineers' SLA and performance-tuning work.
>>SRE and DevOps work together to monitor their areas of responsibility and collaborate on responses when incidents occur. They must also collaborate on incident postmortems and root cause analysis,
aiming to identify and resolve the underlying causes of the incident so that it won't happen again.
>>Security throughout the development lifecycle is becoming increasingly critical as teams try to do more with less while facing an ever-evolving cyber threat landscape. Both DevOps and SRE teams 
can automate and secure toolchains to ensure the organization can deliver new features and bug fixes to its customers continuously and securely.
>>Configuration management and capacity planning are other areas that require DevOps-SRE collaboration. Each group can suffer if configuration issues arise in an application across their environments.
Likewise, expertise and data from both groups are necessary to scale software to meet business needs while staying within budget.
>>Finally, SRE and DevOps can come together to communicate about technical projects outside the IT department. Using shared project management reporting and collaboration tools, DevOps and SRE teams 
can give executive stakeholders the end-to-end picture of a project's status or an incident in the organization's IT environment.

394-what is EBS-CSI-Driver or plugin

>>The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver manages the lifecycle of Amazon EBS volumes as storage for the Kubernetes Volumes that you create.
The Amazon EBS CSI driver makes Amazon EBS volumes for these types of Kubernetes volumes: generic ephemeral volumes and persistent volumes
this is mostly for EBS volumes only. if you use EFS no need to use 
>>CSI drivers in Kubernetes are typically deployed with controller and per-node components. The controller plugin is deployed as either a Deployment or a StatefulSet and can be mounted on any node within
the cluster.

395-what key you need to send the servers to connect 

>>Create public and private keys pass the public key into other servers then connect using the private key 
>>You need to add public key to any server and then connect by using a private key only 

396-What is a meaning of archive files?

>>An archive is a collection of data moved to a repository for long-term retention, to be kept separate for compliance reasons or for moving off primary storage media. 
It can include a simple list of files or files organized under a directory or catalog structure, depending on how a particular program supports archiving.

397-Is there any other alternative tool to log into the cloud environment other than console?

Putty
AWS CLI for Linux
AWS CLI for Windows
AWS CLI for Windows CMD
AWS SDK
Eclipse

398-what is amazon light-sail

>>Amazon Lightsail is a virtual private server (VPS) provider and is the easiest way to get started with AWS for developers, small businesses, students, and other users 
who need a solution to build and host their applications on cloud.Lightsail provides developers compute, storage, and networking capacity and capabilities to deploy 
and manage websites and web applications in the cloud. Lightsail includes everything you need to launch your project quickly – virtual machines, containers, databases,
CDN, load balancers, DNS management etc. – for a low, predictable monthly price.

399-how to check the bucket size in aws

>>you can select the bucket and click on actions and see the bucket size

400-how to restrict specific user in specific region or specific ip range

>>you can create the policy and attach the policy to the user then he can access the only that ip range or that region only user IAM roles

401-it is poosible to chnage dns name in linux

>>nano /etc/resolv.conf   --->change DNS name server in linux

402-what are the strategies to follow cost optimization in cloud

>>Cost optimization in cloud strategies for databases and instances is a crucial aspect of managing your cloud expenditures. 
There are several strategies you can implement to achieve significant cost savings without compromising performance or availability. Here are some key areas to focus on:

>>Right-sizing instances: Analyze your database workload and choose the appropriate instance type and size. Don't over-provision resources you won't utilize, as you'll be paying for unused capacity.
>>Utilize reserved instances: If your workload has predictable usage patterns, consider buying reserved instances for a significant discount. You can commit to a specific instance type and duration
for a pre-defined price.
>>Leverage Spot Instances: For non-critical workloads, look into using Spot instances, which are unused cloud resources offered at a significantly lower price. However, be prepared for potential 
interruptions if demand increases.
>>Automate scaling: Implement autoscaling mechanisms to automatically adjust resources based on real-time demand. This will prevent resource underutilization and reduce costs during low-traffic periods.
>>Stop idle resources: Shut down database instances or applications when they're not in use, especially for development or testing environments.
>>Utilize cloud cost management tools: Most cloud providers offer built-in or third-party tools that provide detailed cost insights, identify optimization opportunities, and set budgets for cost control.
>>Monitor and analyze usage: Regularly monitor your cloud resource usage and analyze cost trends to identify areas for improvement. Set up alerts for anomalies or unexpected spikes in spending.

403-how to do cost optimization database

1-Analyze Your Database Workload:
>>Identify usage patterns: Examine database usage metrics to understand peak hours, idle times, and overall resource consumption.
>>Categorize data: Classify data based on access frequency (hot, warm, cold) to determine appropriate storage classes.
>>Measure query performance: Analyze query performance to identify bottlenecks and optimization opportunities.
2-Right-Size Resources:
>>Choose appropriate instance types: Select instance types that align with your workload's CPU, memory, I/O, and network requirements.
>>Adjust instance size: Scale instances up or down as needed to match actual resource needs.
>>Utilize reserved instances: For predictable workloads, commit to reserved instances for significant discounts.
3. Optimize Storage:
>>Select suitable storage classes: Choose cost-effective storage classes based on data access patterns (e.g., SSD for hot data, archive storage for cold data).
>>Implement data compression: Reduce storage footprint and costs by compressing data effectively.
>>Tier data based on access: Automatically move less frequently accessed data to cheaper storage tiers over time.
4. Implement Cost Management Tools:
>>Utilize cloud provider tools: Leverage built-in cost management tools from your cloud provider to monitor usage, set budgets, and identify optimization opportunities.
>>Consider third-party tools: Explore third-party cost optimization tools for advanced features and analysis.
5. Optimize Database Queries:
>>Identify inefficient queries: Use query optimization tools to pinpoint slow-running queries and improve their performance.
>>Tune database configuration: Adjust database settings and indexes to enhance query efficiency.
6. Consider Serverless Options:
>>Evaluate serverless databases: For workloads with variable usage patterns, explore serverless database options that offer automatic scaling and pay-per-use pricing.
7. Monitor and Adjust:
>>Review usage patterns regularly: Continuously monitor database usage and costs to identify areas for further optimization.
>>Adjust strategies as needed: Adapt your cost optimization approaches based on changing workloads and usage trends.

404-what are serverless computing and how does it differnet from traditinal server architecture 

>>>>imageine that im ceo of startup and i have 10 clients and idont know when the 10 clients will access or loggedin  my application. if i use fro this clinets in traditional server is runing 24/7 the 
cost will be high . what i can do whenever my client access my application the server will up and it will reduce the cost how much you used then you pay . if you used the scaling the headche will taken by 
AWS provider 
>>lambda , SNS, SQS it is for application more robust 

Serverless computing is a cloud computing execution model that enables developers to build and run applications without provisioning or managing servers. It's essentially a way to offload the operational burden of managing infrastructure to the cloud provider, allowing you to focus on writing code and building your application logic.

Here are some key aspects of serverless computing:

Focus on code: You write code in the form of "functions" that are triggered by events, like user interactions, API calls, or scheduled tasks. You don't worry about servers, operating systems, or patching software.

Pay-per-use: You only pay for the execution time of your code, not for idle servers. This makes serverless ideal for applications with variable workloads and can lead to significant cost savings compared to traditional server architectures.

Automatic scaling: The cloud provider automatically scales your functions up and down based on demand, ensuring optimal resource utilization and avoiding performance bottlenecks.

Faster development: By eliminating server management tasks, serverless frees you to focus on writing and deploying code, potentially leading to faster development cycles.

High availability: Cloud providers typically offer high availability guarantees for serverless functions, ensuring your application is always accessible.

Here are some examples of use cases for serverless computing:

Backend logic for web and mobile applications
Processing data streams from IoT devices
Triggering automated workflows based on events
Building serverless API gateways
Running scheduled tasks
However, serverless computing also has some limitations:

Vendor lock-in: You can become dependent on the specific cloud provider's offering, making it difficult to switch providers later.
Cold start latency: The first time a function is invoked, there can be a slight delay while the cloud provider spins up resources.
Limited debugging and logging: Debugging and logging can be more challenging in serverless environments due to the ephemeral nature of functions.
