/1	128.0.0.0	2,147,483,646
/2	192.0.0.0	1,073,741,822
/3	224.0.0.0	536,870,910
/4	240.0.0.0	268,435,454
/5	248.0.0.0	134,217,726
/6	252.0.0.0	67,108,862
/7	254.0.0.0	33,554,430

/8	255.0.0.0	16,777,214
/9	255.128.0.0	8,388,606
/10	255.192.0.0	4,194,302
/11	255.224.0.0	2,097,150
/12	255.240.0.0	1,048,574
/13	255.248.0.0	524,286
/14	255.252.0.0	262,142
/15	255.254.0.0	131,070

/16	255.255.0.0	  -->65,534
/17	255.255.128.0 --->32,766
/18	255.255.192.0	16,382
/19	255.255.224.0	8,190
/20	255.255.240.0	4,094
/21	255.255.248.0	2,046
/22	255.255.252.0	1,022
/23	255.255.254.0	510

/24	255.255.255.0	254
/25	255.255.255.128	126
/26	255.255.255.192	62
/27	255.255.255.224	30
/28	255.255.255.240	14
/29	255.255.255.248	6
/30	255.255.255.252	2
/31	255.255.255.254	0
/32	255.255.255.255	0

vpc
ec2
vpc perring
tarnsit gateway
ebs volumes
efs 
s3
nat gateway
iam 
route53
vpc endpoint
cloudfront(CDN)
load-balancer
auto-scaling
elastic beanstalk
subnets
routetable
nacl
secuity groups
sns
elasticIP
AWS-firewall
rds
elasticCache
cloudwatch
lambda
cloudtrail
awsconfig
global acclerator
session manager
site-to-site-vpn
point-to-site-vpn
direct vpn
openvpn
control lower
cloud-formation
coginto
SSM->service manager
service catalogue
ECS
EKS
ECR
AWS FARGATE
SQS
SES
INSpector
parameterstore
secret-manager

Byte
Kilo Byte
MegaByte
Giga Byte
Tera Byte
Peta Byte
Exa Byte
Zetta Byte
Yotta Byte

APPLICATION-----------------------------------------------| These two is SAAS
DATA------------------------------------------------------|
RUNTIME                              |
MIDDLE-WARE                          |
OPERATING SYSTEM ----                |------>This all is PAAS like MySQL, Azure app service
VIRTUALIZATION      |                |
SERVER              |----------------|------------->This all are IAAS ex:-AWS
STORAGE             |                |
NETWORK-------------------------------

0-day to day activities

>>my self K Anjaneyulu Iam from Karimnagar iam done by b-tech in mother theressa college in the stream of ece comming to my family background
we are 4 members mother father and younger brother. young brother is an android developer mother and father formers
>>iam doing devops engineer in keeno technologies 
>>currently we using agile methodology we manage the configuration management multiple machines and infrstructure as an on-premises cloud using tools like terraform
>>we deploy the applications and deliver the containers on the platform like kubernetes and end of the day take care of montoring this applications on kubernetes that
deployed as containers or pods and and we also montior tools like grafana promethus and node exporter

>>first we check the pipleines then we can check our infrastructure then we can check the servers any errors or logs 
>>then after we do our client requirments activity deployment we used to manage the source in github like github repositores 
>>Interaction with other teams
>>then we do some anisble playbooks to write manage the Applications
>>in order to maintaine the server we used to build the deploy that containzed into docker or Kubernetes
>>and we used to maintaine highly and scalable and appliances using elb and auto-scaling 
>>coming to aws i worked on ec2. auto-scaling, s3, ebs, elb , route53, cloudwatch,sns iam roles vpc, efs  

>>montoring the pipelines 
>>check the service is properly working or not deployment or infrastructure 
>>we follow the agile methdology so then after standing the scrum or metting then after working on prority of the tickets then we learn new
things  

1-Make sure that the pipeline is running smoothly
2-Interaction with other teams 
3-Work on Automation Backlog – Automation is soul of DevOps so DevOps engineering need to plan it out and I can see DevOps engineer spending
4-Infrastructure Management – DevOps engineer are also responsible for maintaining and managing the infrastructure required for CI/CD pipeline
 and making sure that its up #devops and running and being used optimally is also part of their daily schedule. Working on Backup, 
 High Availability, New Platform setup etc.
5-Dealing with Legacy stuff – Not everyone is lucky to work on latest and newest things and DevOps engineers are no exception hence 
they also need to spend time on legacy i.e. in terms of supporting it or migrating to the latest.
6-Exploration – DevOps leverage a lot from the various tools which are available, there are many options as open source so team need 
to regularly check on this to make sure the adoptions as required, this is something which also require some effort not on daily but
 regular basis. What are open source options available to keep the cost at minimum?
7-Removing bottleneck – DevOps primary purpose is identify the bottlenecks / Manual handshakes and work with everyone involved (Dev / QA and 
all other stakeholder) to remove them so team spend good amount of time in finding such things and build the Automation Backlog using this.
8-Documentation – Though Agile / DevOps stresses less on the documentation, it is still the important one which DevOps engineer does on daily 
basis, Be it Server Information, Daily Week charted, Scrum / Kanban board or Simple steps to configure / backup or modify the infrastructure,
 you need to spent good amount of time in coming up these artifacts.
9-Training and Self Development – Self leaning and Training is very useful in getting better understanding and many organisations encourage
 their employee to take the time out and do some of these and same holds true for DevOps folks as well, So learn something new everyday.
10-Continuous Improvement as Practice – Last but not least, It’s up to the DevOps folks to build awareness on the potential of CI/CD and
 DevOps practices and building a culture of leveraging it for doing things better, reducing re-work, increasing the productivity and 
 optimising the use of existing resources.


1-what is cloud-computing

>>Cloud computing is the on-demand delivery of IT resources over the Internet with pay-as-you-go pricing. Instead of buying, owning, and maintaining physical data centers and servers,
you can access technology services, such as computing power, storage, and databases, on an as-needed basis from a cloud provider like Amazon Web Services (AWS).

2-what is Cloud-Native

>>Cloud native is the software approach of building, deploying, and managing modern applications in cloud computing environments. Modern companies want to build highly scalable, flexible,
and resilient applications that they can update quickly to meet customer demands
>>Cloud-Native means using cloud resources all the services entirely managed by AWS no downtime no manual intervention 

Benefits
>>Increase efficiency
>>Reduce Cost
>>Ensure Availability
>>Faster development
>>Platform independence
>>Cost-effective operations

EX:-AWS:-
1-lambda
2-AWS Fargate
3-AWS ECS
4-AWS EKS
5-AWS S3
6-EBS 
7-AWS Auoura
8-DynmoDB
9-VPC
10-CloudFront
11-API-Gateway
12-AWS Cloudwatch
13-AWS WAF
14-AWS IAM
15-AWS RDS
16-SQS
17-SNS
18-Docker
19-Kubernetes

3-what is IAAS

>>Infrastructure as a service (IaaS) is also known as Hardware as a service. the cloud provider provides the Infrastructure to the developers
like network, storage, virtualization, and servers can managed by the cloud provider
>> This is not a physical thing. it will provide only infrastructure. like VMS, compute, memory, OS, network, storage, firewall. things you have full control over virtual machines
ex:- Virtual machines 

>> Users are managed by the operating system and middleware, and runtime data applications all are managed by developers
>>by using IAAS to develop the application is very hard compared to PAAS
Advantages:-
1-controlling is high because you controlled
2-cost saving
3-scalibility. 
4-Faster time to market
5-expand as you grow
6-tight security
7-anytime any where access
Disadvantages:-
1-no internet no work
2-privacy is limited
3-virtualization depend

4-what PAAS
 
>>This is not a physical thing. the PAAS will give a platform for your application. for example you have an appliation or java or dot net application
there is no database person in your team you can ask the cloud service provide he will give the database and mantainance. things you dont have full control on virtual machines
>>Platform as a Service (PaaS) is a complete cloud environment that includes everything developers need to build, run, and manage applications—from servers and 
operating systems to all the networking, storage, middleware, tools, and more.

>>the disadvantage you can't customize the things in you can't troubleshoot and changes you can trade-off or compromise. you can customize but limited things only
ex:- Azure sql and App-service it is managed by the Azure you can't control anything
  
5-what is SAAS

>>This is not a physical thing. it is a readymade product. like Office 365 or google apps or salesforce .it will give full software service. its controling on you it is very limited 
>>SAAS is a pre-existed application that applications are accessible through internet connection and web-browsers like google chrome, gmail, mozila browser, salesforce

6-what is Reserved instance in Total upfront,Partial upfront

>>Upfront:-your paying bill first then he will give some discount 
>>Partial upfront:- you can pay 50% of bill and then you can use remaing amount will pay after the month end bill generation time
>>No Upfront:- this is on-demand systems

7-what is scheduleed reserve instance 

>>like wise you can chooseing the reserved instance like this scheduled reserved instance you can choose weekly or monthly like this 
for reserved will accept only 1 year or 3 years only 
for scheduled reserved will accept weekly or monthly or daily 

8-what is capacity reservations 

>>like you can create the reserve for instance some kind of family t2.micro it will reserve that kind capacity to you . either choose or use he dont care but you need to pay either use or not 

9-what is shared intency and dedicated instance 

>>if you use shared intency in cloud if you use AWS . AWS use single rack that racK under shared VMs to availble tata or suzuki or yamaha like this this will be shared tenacy .
if you want you can choose the dedicated tenacy that rack under they will give dedicated rack no one will comes to that rack like server cpus and memory processers like this 

10-what is 1-tier application

>>In 1-tier architecture, the data is directly provided to the user and that user can directly use the database through the computer. Any changes or updates that are done will reflect 
directly to the database

11-what is 2-tier application

>>Two-tier architecture. In a two-tier architecture, the client is on the first tier. The database server and web application server reside on the same server machine, which is the second tier.
>>This second tier serves the data and executes the business logic for the web application.

12-can you explain 3tier architecture or elaborate them

>>First we have external load balancer and an internal load balancer then the external load balancer is open to the internet
through route53 and the internal load balancer is between the app and the web and the app is connected to the database
>>Using public and private subnets

13-what is Front-end 

>>A front-end developer builds the front-end portion of websites and web applications—the part users see and interact with. A front-end developer creates websites and applications using 
web languages such as HTML, CSS, and JavaScript that allow users to access and interact with the site or app
HTML, CSS, javascript, angular, react-js, Boot-strap, vue-js, Django, Swift, nodejs
port number 80 or 443
>> You will see the visual of the application

14-what is backend

>>Back-end development means working on server-side software, which focuses on everything you can't see on a website. Back-end developers ensure the website performs correctly, focusing 
on databases, back-end logic, application programming interface (APIs), architecture, and servers.
java, python, .net, PHP, Nodejs, Go, C#
the port number is 8080 or anything
>>java .net python  working on server-side software, which focuses on everything you can't see on a website

15-what is database

>>Databases often store information about people, such as customers or users. For example, social media platforms use databases to store user information, such as names, email addresses 
and user behavior. The data is used to recommend content to users and improve the user experience
mysql, RDS, Nosql, mongoDB, Oracle, PostgresSQl 
port number is 3306 or any other port 
>>NO-Sql will store the images and data using mongodb

16-how would you know which library is using in code

>>we run before packing the application we can run unit testing and compile the code using maven tool
>>we dont know which librarys using so that why we combine source code + librarys create into war or jar file
>>we can check the pom.xml all the librays and it contains information about the project and configuration details plugins goals and versions we can check only pom.xml
the build toolis 
java: maven  -->pom.xml
Dotnet: MSBuild  -->project.json
node js: NPM   -->package.json
python: pip    -->reqirments.txt           -->this files contains library and dependies list
python: no need to use build tool in python it is not inter-preted language. it will automaticaly install required packages using pip module

17-what is library

>>without library application will not work. library is nothig but its 3rd party application we will use the librarys to make up or development the application is more easy

18-what is this

>>if my project is java i will take maven as build tool
>>if my project is Android i will take graddle as build tool
>> Weservers are nginx, apache2,Caddy,monkey http server which will directly take the traffic which direcly rout to app servers
apache2 -->in ubuntu 80 port
httpd    -->in RHEL 80 port
nginx  -->80 port
>>app servers is an tomcat,JBoss,WildFly,Oracle weblogic
tomcat  -->8080 port 

jenkins   -->8080
sonarqube   -->9000
jfrog    -->8081
Nexus    -->8081

19-what is web server 

>>Web-Servers are software applications or program that handle incoming network requests over the Hypertext Transfer Protocal(HTTP)
and server files in responce those requests. that play a crucial role in hosting and deivering web content to users browsers 
>>A web server is a computer that runs websites. It's a computer program that distributes web pages as they are requisitioned. The basic objective of the web server is to store, process and deliver 
web pages to the users. This intercommunication is done using Hypertext Transfer Protocol (HTTP)
>>webservers serves the static content it acts as reverse proxy to the app-servers 
ex:-Nginx, Apache, caddy, jetty 

1-Request and responce
2-Http Protocal
3-web servers host delivers static and dynamic content 
4-resources= html, pages, css , js files , 
5-Reverse proxy 

Types of webserver:
1-Nginx 
>>Higher concurrence and low memory usage  -->at any point of time 1000 or 1000 request come nginx easily handle  
>>its famous when three architecture comes 
>>ts mostly support for statics content (htmlfiles, images, javascript, gifs  )
>>its high speed and efficient for serving staticc content 
>>its used for reverse proxy also  

2-Apache HTTP Server  -->
>>its mostly support for dynamic content (generated or customized or fly based user input )
>>its high resource  and 
>>its oldets
>>its known flexibility and extencity 
>>its uses higher resource 

3-Microsoft Internet Information Services(IIS)

20-what is website and webapplication 

>>the content is always same its like static pages is website 
1-news website 
2-blogging websites 
3-wikipedia websites 
>>the content always change its Dynamic pages is webapplication 
1-Facebook
2-Instagram
3-Twitter
4-Gmail 
5-Amazon 

21-How many ways to Access the AWS

>>By using the management console
>>access_kay and secret_key
>>AWS SDKs(software development Kits): using python boto3 this is also use access and secret key (API-also came in to this type using access and secret_key ex: packer or terraform third-party applications)
>>AWS SSO(single sign-on) using anji@wipro.com
>>AWS Federation like SAML or Okta
>>AWS STS(Security token Service)

22-how many users can login at a time in winodws in aws 

>>you can login by default two users but if you want more users to access at a time you need to purchase the license from Microsoft 

23-why aws not recomended access_key and secret_key in local laptop 

>>if your using aws cli in local laptop you missed or forgaoted to lock or anything any other person can stole compromise your account instead aws recomended to use aws cloud shell 

if you have two profiles in local laptop or keys like 

>>1st 
aws configure 

>>2nd 
aws configure --profile anji 

>>aws s3 ls --profile anji    -->it will take anji account if your not using profile then automatically take default credentailas

24-your unable to SSH into a Linux server what steps would you take to troubleshoot this

>>Check if the server is running and network accessible 
>>You can use the ping command to see if the server is reachable
>>Check if the SSH port is open or not
>>Check the firewall rules
>>Check ssh configuration

25-how much time would take to launch and install

>>Linux: minimum 3 minutes maximum 4 minutes (it requires a minimum of 8 GB storage)
>>Windows: minimum 4 minutes maximum 5 minutes (it requires minimum 30 GB storage)

26-what is the secret_key and access_key

>>Access keys consist of an access key ID and secret access key, which are used to sign programmatic requests that you make to AWS. 
If you don't have access keys, you can create them by using the IAM console

>>AWS access key ID is a form of unique user/account identifier. AWS secret key is like a private key. When AWS CLI sends an API request, 
the payload is signed by generating an HMAC with the secret key as the key

27-what is virtualization

>>Virtualization uses software to create an abstraction layer over computer hardware that allows the hardware elements of a single computer—processors, memory, 
storage and more—to be divided into multiple virtual computers, commonly called virtual machines (VMs)

28-What are the different types of virtualization in AWS, and what are the differences between them?

>>Hardware Virtual Machine (HVM)
It is fully virtualized hardware, where all the virtual machines act separately from each other. These virtual machines boot by executing a master boot record 
in the root block device of your image.
>>Paravirtualization (PV)
Paravirtualization-GRUB is the bootloader that boots the PV AMIs. The PV-GRUB chain loads the kernel specified in the menu.
>>Paravirtualization on HVM
PV on HVM helps operating systems take advantage of storage and network I/O available through the host.

29-Name some of the AWS services that are not region-specific

>>AWS services that are not region-specific are:

IAM
Route 53
Web Application Firewall 
CloudFront

30-Define and explain the three basic types of cloud services and the AWS products that are built based on them.

--The three basic types of cloud services are:

>>Computing: - These include EC2, Elastic Beanstalk, Lambda, Auto-Scaling, and Lightsail.
>>Storage - These include S3-simple service storage, Glacier, Elastic Block Storage, and Elastic File System.
>>Networking - These include VPC, Amazon CloudFront, Route53

31-what is CIDR 

>>CIDR stands for Classless Inter-Domain Routing. It is a method used to allocate and specify IP addresses and their routing on the Internet. CIDR replaces the older system that used classes A, B, and C to 
define network addresses, providing a more flexible and efficient way to allocate IP addresses.
>>CIDR is used to create a subnet that will reduce the wastage of the IPS 

32-what is VPC

>> VPC is a virtual private cloud it is nothing but it will provide network-level isolation and it will provide network security you can work with other organizations also. VPC has so many components like 
subnets under public and private subnets and internet gateway and route tables and nat-gateway   
>>Amazon VPC lets you provision a logically isolated section of the Amazon Web Services (AWS) cloud where you can launch AWS resources in a 
virtual network that you define.
>>Amazon Virtual Private Cloud (Amazon VPC) gives you full control over your virtual networking environment, including resource placement, 
connectivity, and security. it is logical entity. in one network under he will create multiple network. and isolation in every network  
by default vpc will not communicate each other until you peering or transit until it will  
>>if you want communicate one more vpc its possible but IPs will not overlap if overlap it will not work 
>>your created vpc cidr 10.1.0.0/16 if you want add onemore cidr you can select the vpc click on actions 10.2.0.0/16 is possible 
>>vpc under vpc name is anji no-one cares there is value only vpc-id if you use vpc-id when peering also name there is no value in vpc   
>>Can i attach multiple internet gateways to single vpc it's not possible just google it
>>If you want automatically public ip to the vpc you need to go the subnet click on actions click on edit subnet settings then allow auto-assign public ip

>>It is a region-dependent
>>Vpc is a logical boundary
>>vpc id is globally unique
>>you can stop the particular ip one ec2 to another ip through routing table in peering
>> You must enable DNS hostname. otherwise you wont get the hostname you will get the public ip but not the hostname 

Use-Cases:-
1-Isolation of Resources
2-Multi-Tier Applications: You can create multi-tier applications by deploying different components of your application in separate subnets within a VPC. For example, web servers in one subnet, 
application servers in another, and databases in yet another, each with its own security and routing rules.
3-Connectivity to On-Premises Data Centers:
4-Hybrid Cloud Deployments
5-IP Address Management:
6-Highly Available Architectures:
7-Security and Network Access Control:
8-Elastic Load Balancing: When deploying applications with varying loads, VPCs can be used with Elastic Load Balancing (ELB) to distribute incoming traffic across multiple instances, ensuring high availability
and fault tolerance
9-Managed Virtual Private Networks:
10-VPC Peering

33-when I am creating my vpc under subnet why 251 ip come 

>>If you create subnet 10.1.1.0/24 means actually the will get 256 but you will get 251 only 5 ips will reserved any subnet like this
>>First one is network 
>>Secondone is router
>>Thirdone dns
>>Fourthone is feature purpose
>>last one brodcast
>>See similar to azure also ashiq bastion host video

34-how many vpc can i create in a region 

>>in vpc you must create 5 vpc only . you can ask to enable the extra quota to create more vpcs 

35-what is Subnet

>>A subnet is a range of IP addresses in your VPC. You launch AWS resources, such as Amazon EC2 instances, into your subnets. You can connect a subnet to the internet, other VPCs,
and your data centers, and route traffic to and from your subnets using route tables.
>>once you created subnet you cant decrease or you can't edit the subnet  

Use Cases:-
1-Multi-Tier Applications: Subnets allow you to organize components of multi-tier applications. For example, you can have separate subnets for web servers, application servers, and databases. 
This helps in enforcing security measures and controlling the flow of traffic between different tiers.
2-High Availability:
3-Public and Private Subnets:
4-Network Isolation:
5-AWS Lambda Functions:
6-Amazon RDS Instances:
7-Elastic Load Balancer (ELB) Placement:
8-VPN and Direct Connect Connections:
9-Resource Grouping:
10-VPC Peering:
11-Internet Gateway and NAT Gateway Placement:

36-how many subnets can be created in a VPC 

>>200 subnets per vpc 

37-what are Route tables in AWS and Use-Cases 

>>A Route table defines how the communication should happen within the subnet and outside of the subnet
>>We can create custom route tables per each subnet design for the network traffic
>>The route table plays a key role when you want to route traffic to 
>>It is also subnet level
-other Vnets
-virtual network gateways
-virtual appliances
-Internet
Use-Cases:-
1-Routing Decisions: The primary purpose of a route table is to make routing decisions for traffic within a VPC. It determines where the traffic within a subnet should be directed. 
This includes local traffic within the VPC and outbound traffic to the internet or other VPCs.
2-Default Routes: Route tables can include default routes, also known as "0.0.0.0/0" routes. These routes define the default path for traffic that doesn't match any specific route in the table.
For example, the default route might point to an Internet Gateway for outbound internet-bound traffic.
3-Internet Gateway Association: When you want your instances in a subnet to communicate with the Internet, you associate the subnet's route table with an Internet Gateway. 
This enables the routing of traffic to and from the internet.
4-Virtual Private Gateway Association: If you have a Virtual Private Network (VPN) or Direct Connect connection to your on-premises network, you can associate a subnet's route table with a
Virtual Private Gateway. This allows traffic to flow between your VPC and your on-premises network.
5-Peering Connection Association: When you have VPC peering connections between multiple VPCs, you can associate a subnet's route table with a peering connection. This allows traffic to flow between 
the peered VPCs.
6-Egress-Only Internet Gateway Association (for IPv6): When using IPv6, you can associate a subnet's route table with an egress-only Internet Gateway. This allows IPv6-enabled instances in the subnet 
to communicate with the internet over IPv6.
7-Custom Routes: Route tables can include custom routes to define specific paths for traffic. This is useful for scenarios where you want to route traffic to specific destinations through particular 
network devices, such as a transit gateway or specific instances acting as routers.
8-Multiple Route Tables per VPC: A VPC can have multiple route tables, and subnets can be associated with different route tables. This allows for flexibility in defining routing policies for different
subsets of resources within a VPC.

38-What is IGW.

>>Internet Gateway (IGW) is a service that allows communication between resources in your Virtual Private Cloud (VPC) and the Internet. 
It serves as a horizontally scaled, redundant, and highly available component that enables outbound and inbound traffic to and from the 
Internet.
1-Connectivity to the Internet:
2-Outbound Internet Access:
3-Inbound Internet Access:
4-Routing:
5-Elastic IP Address Association:
6-Security: Access control for traffic to and from the internet is primarily managed through Network Access Control Lists (NACLs) and Security
 Groups associated with your VPC and its resources.

To use an Internet Gateway:

1-You attach the Internet Gateway to your VPC.
2-Configure route tables to direct traffic to and from the Internet Gateway.
3-Associate Elastic IP addresses with resources as needed.

39-What is the relation between the Availability Zone and Region?

>>AWS regions are separate geographical areas, like the US-West 1 (North California) and Asia South (Mumbai). On the other hand, availability zones
are the areas that are present inside the regions. These are generally isolated zones that can replicate themselves whenever required.
>>The AWS Cloud spans 99 Availability Zones within 31 geographic regions
>>20 per Region you can request more
>> You can create up to 100 buckets in each of your AWS accounts. If you need more buckets, you can increase your account bucket limit to a 
maximum of 1,000 buckets by submitting a service limit increase.

40-what is nat-gateway

>>A NAT gateway is a Network Address Translation (NAT) service. it is used in a private system to connect over the internet to take patches or updates nat gateway is an intelligent device using 
source destination packets. it will send only traffic through the public instance but not allow outside the traffic to your private instances. when you enable nat gateway you must select the 
public subnet, not the private subnet
>>Nat gateway intelligent device  using source destination packets. if any request goes private to the internet then traffic comes to private it will allow because of source and destination packets stored 
in nat-gateway . if the directly come request through internet it will not allow because the source and packet is not stored in nat-gateway 
>>Nat gateway used to private ip to public devices
>>if you use nat-gateway you must select the public subnet only if you select the private subnet it will not works. so select the public subnet only  and your allocateing one elastic-ip to nat-gateway 

>>if you need highly available enviorment like you anji-vpc public-subnet-1 and piblic-subnet-2 and then app-private-1 and app-private-2 then db-private-1 and db-private-2 then create route tables anji-public
public-subnet-1, public-subnet-2  app-private-route app-private1, app-private-2 db-private-route db-private-1, db-private-2  then create nat-gateway anji-public-1 allocate elastic-ip click on ok then go the 
app-private-route table add routes 0.0.0.0/0 and select the anji-public-1 nat-gateway. like this create nat-gateway anji-public-2 allocate elastic-ip click on ok then go the db-private-route table 
add routes 0.0.0.0/0 and select the anji-public-2 nat-gateway       
       
1 - nat gateway managed by AWS 
2 - nat gateway can't assign security groups
3 - nat gateway highly available
4 - nat gateway bandwidth is up to 45 Mbps
5 - nat gateway performance is good 

41-what is NAT-Instance

>>NAT-A NAT (Network Address Translation) instance is, like the same nat-instance gateway but the difference is the amount is big in the nat gateway when compared to the nat instance

42-What are the differences between NAT Gateways and NAT Instances?
 
NAT-Gateway
1 - nat gateway managed by AWS 
2 - nat gateway can't assign security groups
3 - nat gateway highly available
4 - nat gateway bandwidth is up to 45 Mbps
5 - nat gateway performance is good

NAT-Instance
1 - NAT instance managed by you
2 - NAT instance can assign security groups
3 - NAt instance is also highly available
4 - NAT instance bandwidth depends on instance bandwidth
5 - NAT instance performance compared to NAT gateway is average
 
43-what is security group stateful or stateless in aws

>>Security groups are stateful—if you send a request from your instance, the response traffic for that request is allowed to flow in regardless of 
inbound security group rules. For VPC security groups, this also means that responses to allowed inbound traffic are allowed to flow out, regardless of 
outbound rules.

44-what is NACL

>>A network access control list (NACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.
You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.

>> When you create vpc automatically create NaCl 
>> If anyone want say to change something nacl first ask to send a mail 
>>don't do changes in nacl in production environment
>>need to add every rule in inbound and outbound  -->if you add ssh in inbound it will not allow so you need to add outbound also

45-What is the difference between security group and NACL?

>>Security Group is applied to an instance only when you specify a security group while launching an instance.NACL has applied automatically to all
the instances which are associated with an instance. is the second layer of defense.
>>by default in security group there no deny option it will allow the traffic how you will deny the ip . in that place you can allow specific ip only
>>security-group is stateful
>>if you use security group it will be only server level only. security group only have allow the ips only

>>NACL It is the first layer of defense.
>>NACL only for subnets by default automatically creates a VPC when we create a VPC and NCL is Stateless you need to enable inbound and outbound also if you are enable inbound not 
enabled outboind it will not work and it will sit infornt of security group 
>>if you add one port in the inbound rule you need to add an ephemeral port in the outbound otherwise it won't work
>>it will applied by subnet any server is there in subnet this will apply to that server or subnet level  using NACL.NACL have allow and deny option

46-How many security groups can be attached to an EC2 instance?

>>five

47-where we can attach security groups

>>EC2
>>Load-Balancer  (all)
>>Not for subnets

48-What is the difference between NACL and Subnet ? Explain with a use case ?

>>For example, I want to design a security architecture, I would use a combination of NACLs and security groups. At the subnet level, I would configure NACLs to enforce inbound and
outbound traffic restrictions based on source and destination IP addresses, ports, and protocols. NACLs are stateless and can provide an additional layer of defense by filtering traffic 
at the subnet boundary.At the instance level, I would leverage security groups to control inbound and outbound traffic. Security groups are stateful and operate at the instance level. 
By carefully defining security group rules, I can allow or deny specific traffic to and from the instances based on the application's security requirements. By combining NACLs and 
security groups, I can achieve granular security controls at both the network and instance levels, providing defense-in-depth for sensitive applications.

49-what is describe menas vpc

>>it will show the entire vpc details of vpcs

50-what is flow logs

>>VPC Flow Logs capture information about the IP traffic going to and from network interfaces in your VPC. 
>>VPC Flow Logs provide information at the network flow level, including details like source and destination IP addresses, ports, protocols, and the amount of data transferred.
>>you can capture entire vpc or subnet level flow logs and pass to log-group in Cloudwatch then you can use elastic search to visualize them  

>>New Flow Logs will appear in the Flow Logs tab of the VPC dashboard. The Flow Logs are saved into log groups in CloudWatch Logs. The log group will be created
approximately 15 minutes after you create a new Flow Log. You can access them via the CloudWatch Logs dashboard.
>>Amazon VPC Flow Logs is a feature in AWS that allows to capture information about the IP traffic going to and from network interfaces in vpc
>>VPC Flow Logs provide detailed information about the traffic, including source and destination IP addresses, ports, protocol, and more.  This information is valuable for network monitoring, troubleshooting,
security analysis, and compliance checks. select the vpc enable the fow logs and select the logs where to send either any s3 bucket or cloudwatch choose 1 minute 10minutes like this 

51-how you can montior the traffic in vpc

>>we can use flow logs that logs will be stored s3 bucket or cloud watch 

52-How you can secure communication between your vpc and on-premises infrastructure?

>>communication between VPC in the cloud and  on-premises infrastructure is crucial for maintaining a secure and reliable network environment.
There are several methods are there to achieve secure communication
a.Virtual Private Network (VPN)
b.AWS Direct Connect
c.Internet Protocol Security

53-can we change the IP range of an existing VPC?

>>* It’s not possible if we need to change ip address range we need to create new vpc then we need to migrate all resources from old vpc to new vpc

54-can I want increase buckets and vpc how to do that 

>>yes its possible. go the resource quota and select which servic you want then request increase the quota AWS will increase that quota 

55-What is reachbility analyzer in nacl in aws

>>using this you can see the network how going to your network or vpc instance how its connecting to other resource

56-what is the VPC end-point

>>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services. instance in your VPC does not require a public IP address to communicate the AWS resources
this is gateway endpoint two types of endpoints 1-gateway endpoint 2-interface endpoint 
>>S3
>>DynmoDB
>>Also create Interfaces like EkS, Auto-Scaling, Cloud-Trail, Cloud-formation
use cases of endpoints 
1-Replacing NAT gateway with interface endpoints
2-Adding gateways endpoint for s3 intensive Workload
3-private access to saas based applications 

===VPC END-Point

>>vpc nat gateway is expensive that why some of the companies use vpc end-points

>>Nat gateway used to connect private ip to public devices

>>create vpc name Green ip 10.1.1.0/16

>>create subnet name Green-public(webapp) az (us-east-1a) select vpc Green and ip 10.1.1.0/24

>>craete subnet name Green-private(app-server) az (us-east-1a) select vpc Green and ip 10.1.10.0/24

>>create subnet name Green-private(db-server) az (us-east-1b) select vpc Green and ip 10.1.20.0/24

>>create internet gateway name Green-igw and attach vpc Green

>>create route table name Green-public and select vpc Green

>>create route table name Green-private and select vpc Green

>>click on Green-public edit route add 0.0.0.0/0 and select internet gateway Green-igw and click on subnet assosication select Green-public click  on ok

>>click on Green-private edit subnet assocation select Green-private(appserver)and Green-private(dbserver) click on ok don't add internet gateway

>> click on nat gateway 

>>create nat name Green-nat and select subnet Green-public and allocate elastic ip, don't select Green-private any case

>>its take sometime

>>create one security group name Green-sec and select vpc Green allow all ports 

>>launch one ec2 using vpc Green and subnet Green-public enable public ip and select security group Green-sec and launch

>>launch one ec2 using vpc Green and subnet Green-private(appserver) disable public ip and select security group Green-sec and launch

>>connect one system using Green-public login using putty

>>sudo su -

>>if you want to connect private(appserver or dbserver) system do this things in public(web) system

>>vi anji.pem

paste the private key

:wq

>>chmod 400 anji.pem

>>ssh -i anji.pem ec2-user@10.1.10.200    -->your login as private system lets connect google or ping the servers

>>ping google.com

>>your not get ping resopnnce because your not add route table

>>go route table select Green-private add 0.0.0.0/0 and select nat gateway and done

>> then try to ping google.com you will get

>>create a S3 bucket name Green-s3 and upload one file and make public (create .txt)

>>wget http://s3.url   -->in private system

>>create one user with admin access and create secret and access keys

>>login private system and configure

>>sudo su -

>>aws configure
secret-key = xxxxxxxxxx
access-key = xxxxxxxxxx

>aws s3 ls

>>touch onefile.txt

hai hello 

:wq

>>aws s3 cp onefile.txt s3://Green-s3     -->copy that file in s3 bucket

>>make public that file    -->copy that file

>>wget https://xxxxxxx    -->it will copy into that file in your system

>>remove nat gateway in route table

>>what is gateway end-point
gateway end-point is a gateway that you specific as a target for a route in your table for traffic destined to a supported AWS service
Amazon S3
DynmoDB

--if you created s3 gateway automatically updated route table
>>remove nat gatewau in private route table 
>>create end-point select s3 gateway select vpc Green select routing table in Green-priv and creating endpoint
>>it will add the route table in 2 minutes wait until
>>next try the to download the file using s3
>>wget https://s3.xxxxxxx            -->only s3 files will download

57-what is gateway endpoint

>>Gateway end-point is a gateway that you specify as a target for a route in your table for traffic destined to a supported AWS service. It serves as a bridge between different networks or 
systems and enables communication and data transfer between them. Gateways can be used for various purposes, such as security, monitoring, load balancing, and routing. Examples of 
cloud gateways include API gateways, database gateways, and messaging gateways.
Amazon S3
DynmoDB
>>Must be inside the VPC to be used 
>>Use s3 public IP address
>>Use same S3 DNS names
>>No access from on-permise 
>> Cross-region access not allowed
>>Not billed 
>>Assocaited on a VPC level 
>>if you create DynmoDB or S3 endpoint automatically updated route table 

>> Gateway is a hardware device that gives you entry and exit points in your network
>>gateway end-point is a gateway that you specific as a target for a route in your table for traffic destined to a supported AWS service

58-what is the interface endpoint 

>>It will create an interface to connecting privately through the AWS Interface endpoint securely   
>>An interface VPC endpoint allows you to privately connect your Amazon VPC to supported AWS services. 
Interface VPC endpoints also connect to endpoint services hosted by other AWS customers and partners and AWS Marketplace partner services
>>An Interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destinated to a supported service 
services:-
Amazon API Gateway 
Amazon AppStream
Amazon APP Mesh 
>>Support most of the AWS Services 
>>It is ENI(Elastic Network Interface) which is attached with a security group 
>> Use a private IP address from your VPC to s3 
>>Requires endpoint specific S3 DNS names
>>Allow access from on-premise 
>>Allow cross-region access through VPC peering or Tranist gateway 
>>billed
>>Associated on subnet level 
>> It will create a separate endpoint for service then you can connect the Interface resource or AWS Resources  

59-what is VPC Peering

>>A VPC peering  is a networking connection between two VPCs or two differnet networks. that enables you to route traffic between two different networks or two Vpcs. 
once perring is completed they can communicate with each other as if they are within the same network. There are no charges in VPC peering
>> You can also create the VPC peering one region Mumbai to another region Ohio like this
>> You can also VPC peering one account to another account
>> VPC peering is one to one it will not support transit like example
you one system us-east-1 and onemore system in  us-west-1 this regions in peered it will ping each other for example you created one server in ap-south-1 you peerd us-east-1 to ap-south-1 it will ping 
each other but not us-west-1 ping with ap-south-1 why beacuse your not created peering this two regions thats why iam saying one to one peeering in vpc peering 
>> Vpc peering is one to one if you want red vpc to green vpc then if you want green to yellow and green to blue but inorder to ping or share only green-to red and yellow and blue but not red to blue or 
yellow because these peered each other so its not support transit  
>> If you want to give only one subnet peering yes its possible when peering  ex:-
you created green us-east-1 subnet-1 10.1.1.0/24 and subnet-2 10.1.2.0/24 and in ohio us-east-1 red subnet 172.16.1.0/24 you want green subnet-2 only ping to red vpc yes you can create vpc peering green 
requester red accepter add routes in green 172.16.0.0/16 then go to red vpc add route 10.1.2.0/24 and select the vpc-peering then thats it 
if you try to ping red to green subnet-1 you will not get ping because you addes only one subnet-2 only . if you try to ping green subnet-2 ping to red . if you want full vpc you can add 10.1.0.0/16 
>> Vpc peering is two way you need to accept one side if you request to green and you need go red vpc and accept the vpc peering 

Use-cases:-
1-Cross-Account Connectivity: When different AWS accounts need to communicate with each other securely.
2-Isolation of Environments: Organizations often use separate VPCs for different environments, such as development, testing, and production, to isolate resources and environments from each other.
3-Shared Resources: Multiple VPCs within the same AWS account may need to share resources, such as databases, application servers, or other services.
4-Hub-and-Spoke Architecture: Implementing a hub-and-spoke architecture where a central VPC (hub) connects to multiple spoke VPCs.
5-Data Sharing: Sharing data securely between VPCs.
6-Reduced costs: Peering can help to reduce costs by eliminating the need to pay transit fees to an ISP.
7-Improved performance: Peering can improve performance by reducing latency and jitter.
8-Increased reliability: Peering can increase reliability by providing an alternative path for traffic to flow in the event of an outage.

===disadvantages of peering

>> An increase in the VPC will increase in the VPC peerings
>> Each VPC needs a VPG for VPN connections

===VPC Peering

creating connection between two vpc

>>ip address must not be overlap

>>vpc peering don't transit. transit means mediator or green is connected to red and red is connected to Blue but Blue is not interconnected or
ping to Green you need to every vpc interconnection

>>it is one to one connection

>>real time your not giving public ips your connecting your vpn through the server

Green

>>create vpc name Green ip 10.1.1.0/24

>>create subnet name Green-subnet select vpc Green and ip 10.1.1.0/24 select aZ us-east-1a

>>create internet gateway Green-igw and attach vpc Green

>>create route name Green-route and select vpc Green and edit route add 0.0.0.0/0 and select interet gateway Green-igw

>>craete one instance using VPC Green and subnet green-sub and public ip enable and private ip 10.1.1.100 security group all key 

Red

>>create vpc name Red and ip 192.168.1.0/24

>>create subnet name Red-subnet select vpc Red and ip 192.168.1.0/24 select aZ us-east-1a

>>create internet gateway Red-igw and attach vpc Red

>>create route name Red-route and select vpc Red and edit route add 0.0.0.0/0 and select interet gateway Red-igw

>>craete one instance using VPC Red subnet Red-sub and public ip enable and private ip 192.168.1.100 security group all protocal

Blue

>>create vpc name Blue and ip 172.16.1.0/24

>>create subnet name Blue-subnet select vpc Blue and ip 172.16.1.0/24 select aZ us-east-1a

>>create internet gateway Blue-igw and attach vpc Green

>>create route name Blue-route and select vpc Blue and edit route add 0.0.0.0/0 and select interet gateway Blue-igw

>>craete one instance using VPC Blue and subnet Blue-sub and public ip enable and private ip 172.16.1.100 and security group all protocals

>>go to peering connection

>>create peering name Green-to-Red and requester Green and accepter Red and click on Create connection and accept the connection

>>create peering name Green-to-Blue and Requster Green and accepter Blue and click on create connection and accept the connection

>>craete peering name Blue-to-Red and requester Blue and accepter Red and click on create connection and accept the connection

>>go to route table in Green-Route add edit route add 192.168.1.0/16, 172.16.1.0/16 and select peering select

>>go to route table in Red-route and edit route add 10.1.1.0/16, 172.16.1.0/16 and select the peering connection

>>go to route table in Blue-route and edit route add 10.1.1.0/16, 192.168.1.0/16 and select the peering connection

>>ping 192 to 10 you will get

>>pinh 192 to 172 you will get 

>>ping 172 to 10 you will get

===peering one region mumbai to another region ohio

mumbai region

Green

>>create vpc name Green ip 10.1.1.0/24

>>create subnet name Green-subnet select vpc Green and ip 10.1.1.0/24 select aZ us-east-1a

>>create internet gateway Green-igw and attach vpc Green

>>create route name Green-route and select vpc Green and edit route add 0.0.0.0/0 and select interet gateway Green-igw

>>craete one instance using VPC Green and subnet green-sub and public ip enable and private ip 10.1.1.100 security group all key 

ohio region

Red

>>create vpc name Red and ip 192.168.1.0/24

>>create subnet name Red-subnet select vpc Red and ip 192.168.1.0/24 select aZ us-east-1a

>>create internet gateway Red-igw and attach vpc Red

>>create route name Red-route and select vpc Red and edit route add 0.0.0.0/0 and select interet gateway Red-igw

>>craete one instance using VPC Red subnet Red-sub and public ip enable and private ip 192.168.1.100 security group all protocal

create peering connection

>>craete peering name mumbai to ohio and accepter mumbai vpc(green) and ohio vpc(Red) and go to ohio region and accept the peering 

>>now go to route table add route edit route 10.1.1.0/16 and select the peering connection

>>now go to route table and edit route 192.168.1.0/16 and select the peering connection

>>now you will get ping

>>ping 10 to 192 you will get

>>ping 192 to 10 you will get

===peering one account to another account

first account

mumbai region

Green

>>create vpc name Green ip 10.1.1.0/24

>>create subnet name Green-subnet select vpc Green and ip 10.1.1.0/24 select aZ us-east-1a

>>create internet gateway Green-igw and attach vpc Green

>>create route name Green-route and select vpc Green and edit route add 0.0.0.0/0 and select interet gateway Green-igw

>>craete one instance using VPC Green and subnet green-sub and public ip enable and private ip 10.1.1.100 security group all key 

second account

ohio region

Red

>>create vpc name Red and ip 192.168.1.0/24

>>create subnet name Red-subnet select vpc Red and ip 192.168.1.0/24 select aZ us-east-1a

>>create internet gateway Red-igw and attach vpc Red

>>create route name Red-route and select vpc Red and edit route add 0.0.0.0/0 and select interet gateway Red-igw

>>craete one instance using VPC Red subnet Red-sub and public ip enable and private ip 192.168.1.100 security group all protocal

>>create peering connection in ohio to mumbai 

>>create peering name second account to first account requester name Red and select another account account id and accepter vpc id then create a coonection

>>then go to first account and accept

first account
>>then go to route tables add edit route add 192.168.1.0/16 and select peering connection

second account
>>go to route table and edit route add 10.1.1.0/16 and select peering connection

>>now go to  ping 10 to 192 you will get

>>ping 192 to 10 you will get

60-What is a transit gateway?

>>Transit gateway advance layer of VPC peering, in VPC peering if we create 3 or more VPCs we need to interconnect using VPC peering if we increase VPC peering it is
difficult to identify the traffic and get confused in VPC peering. so that we use a transit gateway we create one transit gateway so we can interconnect the
VPC in a region is easy to understand and easy to use. we can interconnect different regions in also easy way. but it is chargeable

>>Transit gateway advance layer of VPC peering. the transit gateway simplifies your network connection and puts an end to complex peering relationships.
Transit Gateway acts as a highly scalable cloud router—each new connection is made only once

===Transit gateways

>>transit gateway is chargeble

>>vpc need to connect using peering

>>if in case using mutiple vpc using environment that need multiple peerings.so that why using transit gateway

>>create 3vpc of transit gateway

>>create vpc name Green ip 10.1.0.0/16

>>create subnet Green-sub and vpc Green and ip 10.1.1.0/24

>>create internet gateway Green-igw and attach vpc Green

>>create route table Green-route and add 0.0.0.0/0 and select internet gateway

>>create vpc name blue ip 192.168.0.0/16

>>create subnet blue-sub select vpc blue and ip 192.168.1.0/24

>>create internet gateway blue-igw and select vpc blue. and attach to blue vpc 

>>create route blue-route and and add route 0.0.0.0/0 and internet gateway

>>create vpc name red ip 172.16.0.0/16

>>craete subnet red-sub select vpc red and ip 172.16.1.0/24

>>create internet gateway red-igw and select vpc red. and attach to red vpc 

>>create route red-route and and add route 0.0.0.0/0 and internet gateway

>>go to transit gateway

>>name TGW1 then ok -->it will take time mean time launch ec2-instances

>>launch instance using vpc green and subnet green-sub region us-east-1a ip 10.1.1.100 and allow all protocals name green-server

>>launch instance using vpc blue and subnet blue-sub region us-east-1a ip 192.168.1.100 and allow all protocals name blue-server

>>launch instance using vpc red and subnet red-sub region us-east-1a ip 172.16.1.100 and allow all protocals name red-server

>>try to login and ping each server you won't get ping access

>>go to transit gateway attachments and select transit gateway TGW1 and select vpc and vpc of Green and subnet green-sub1 region us-east-1a  and attach

>>go to transit gateway attachments and select transit gateway TGW1 and select vpc and vpc of blue and subnet blue-sub1 region us-east-1a  and attach

>>go to transit gateway attachments and select transit gateway TGW1 and select vpc and vpc of red and subnet red-sub1 region us-east-1a  and attach

>>go to green route table edit route add 192.168.0.0/16 and select transit gateway and edit route 172.16.0.0/16 and select transit gateway

>>go to blue route table edit route add 172.16.0.0/16 and select transit gateway and edit route 10.1.0.0/16 and select transit gateway

>>go to red route table edit route add 192.168.0.0/16 and select transit gateway and edit route 10.1.0.0/16 and select transit gateway

>>if you try ping all the three devies you will get

61-is it possible to do one region to another region tranist gateway perring 

>>yes you need to create the ap-south-1 region one transit and another us-east-1 region also one transit then add route table 

===you can do one region mumbai to another region ohio also

mumbai

>>create 3vpc of transit gateway

>>create vpc name Green ip 10.1.0.0/16

>>create subnet Green-sub and vpc Green and ip 10.1.1.0/24

>>create internet gateway Green-igw and attach vpc Green

>>create route table Green-route and add 0.0.0.0/0 and select internet gateway

>>create vpc name blue ip 192.168.0.0/16

>>craete subnet blue-sub select vpc blue and ip 192.168.1.0/24

>>create internet gateway blue-igw and select vpc blue. and attach to blue vpc 

>>create route blue-route and and add route 0.0.0.0/0 and internet gateway

>>create vpc name red ip 172.16.0.0/16

>>craete subnet red-sub select vpc red and ip 172.16.1.0/24

>>create internet gateway red-igw and select vpc red. and attach to red vpc 

>>create route red-route and and add route 0.0.0.0/0 and internet gateway

>>go to transit gateway

>>name TGW1 then ok -->it will take time mean time launch ec2-instances

>>launch instance using vpc green and subnet green-sub region us-east-1a ip 10.1.1.100 and allow all protocals name green-server

>>launch instance using vpc blue and subnet blue-sub region us-east-1a ip 192.168.1.100 and allow all protocals name blue-server

>>launch instance using vpc red and subnet red-sub region us-east-1a ip 172.16.1.100 and allow all protocals name red-server

>>try to login and ping each server you wont get ping access

>>go to transit gateway attachments and select transit gateway TGW1 and select vpc and vpc of Green and subnet green-sub1 region us-east-1a  and attach

>>go to transit gateway attachments and select transit gateway TGW1 and select vpc and vpc of blue and subnet blue-sub1 region us-east-1a  and attach

>>go to transit gateway attachments and select transit gateway TGW1 and select vpc and vpc of red and subnet red-sub1 region us-east-1a  and attach

>>go to green route table edit route add 192.168.0.0/16 and select transit gateway and edit route 172.16.0.0/16 and select transit gateway

>>go to blue route table edit route add 172.16.0.0/16 and select transit gateway and edit route 10.1.0.0/16 and select transit gateway

>>go to red route table edit route add 192.168.0.0/16 and select transit gateway and edit route 10.1.0.0/16 and select transit gateway

>>if you try ping all the three devies you will get

==============================
>>do same in ohio region

>>create vpc in yellow ip 172.18.0.0/16

>>create subnet yellow-sub 172.18.1.0/24 select vpc yellow

>>create internet gateway yellow-igw and attach the yellow

>>create route table yellow-route edit route add 0.0.0.0/0 select internet gateway yellow-igw

>>launch ec2 instance using vpc yellow and subnet yellow-sub region us-east-1a ip 172.18.1.100 and allow all ports and name yellow-server

>>go to transit gateway create name yellow-transit

>>transit gateway attachment select transit gateway yellow-transit and select vpc yellow and region us-eats-1a

>>go to route tables edit route add 172.16.0.0/16 and select the transit gateway yellow-transit

>>go to route tables edit route add 192.168.0.0/16 and select the transit gateway yellow-transit

>>go to route tables edit route add 10.1.0.0/16 and select the transit gateway yellow-transit

>>go to route in red route table add 172.18.0.0/16 and select the transit gateway TGW1

>>go to route in blue route table add 172.18.0.0/16 and select the transit gateway TGW1

>>go to route in green route table add 172.18.0.0/16 and select the transit gateway TGW1

>>in transit gateway create transit gateway attachment in mumbai region 

>>select transit gateway TGW1 and select vpc peering attachment tag mumbai to ohio and region ohio and id accepter copy id of yellow-transit id and click on ok

>>then go to ohio region and accept the request in transit gateway attachmet section

>>in mumbai region

>>go to transit gateway route table add route of ohio region

>>edit route 172.18.0.0/16 and select transit gateway TGW1

>>in ohio region

>>go to transit gateway route table add route of mumbai region

>>edit route 10.1.0.0/16 and select transit gateway yellow-transit

>>edit route 172.16.0.0/16 and select transit gateway yellow-transit

>>edit route 192.168.0.0/16 and select transit gateway yellow-transit

now you will get the ping

63-what is the use case of transit gateway 

1-Multi-VPC Connectivity
2-Hybrid Cloud Connectivity
3-Shared Services
4-Security and Compliance
5-Multi-Region Architectures
6-Application Migration
7-Third-Party SaaS Integration
8-Disaster Recovery
9-Multi-Tenancy
62-is it possible to integrate jenkins to aws code pipeline 

>>yes you can integrate you need to give appreate role for jenkins server then install the plugin then you create pipeline in code pipeline you will see the build in jenkins also 
https://www.youtube.com/watch?v=fN6vA80JbwI


61-What is Amazon EC2?

>>Ec2 Means elastic cloud computing where you are requesting AWS the cloud provider to give a virtual machine or virtual server that is elastic in nature this term is called 
compute that we called EC2
>> Elastic is nothing but scaled up and scaled-down any time is called elastic
ex:-Elastic cloud compute, elastic beanstalk, EKS, ECR , ECS
                                                     (or)
>>Amazon Elastic Compute Cloud (Amazon EC2) is a web-based service that allows businesses to run application programs in the Amazon Web Services (AWS) public cloud.
>>Amazon Ec2 is scalable computing capacity and you can use the images to you can launch the servers as you need. and its configured security networking and storage
Amazon ec2 helps you to scale up and down handling the requirements

62-what is AMI In AWS

>>An Amazon Machine Image (AMI) is a pre-configured image using this image to launch the virtual servers
>>AMI ID is different for every region and every image
>> You can use in production mostly use already created AMIs

63-you are trying to provide a service in a particular region, but you do not see the service in that region. Why is this happening, and how do you fix it?

>>Not all Amazon AWS services are available in all regions. When Amazon initially launches a new service, it doesn’t get immediately published in all the regions. 
They start small and then slowly expand to other regions. So, if you don’t see a specific service in your region, chances are the service hasn’t been published in 
your region yet. However, if you want to get the service that is not available, you can switch to the nearest region that provides the services

64-Mention the different types of instances in  Amazon EC2 and explain its features.

>>General Purpose Instances: They are used to compute a range of workloads and aid in the allocation of processing, memory, and networking resources.
>>Compute Optimized Instances: These are ideal for compute-intensive applications. They can handle  batch processing workloads, high-performance web servers, 
machine learning inference, and various other tasks
>>Memory Optimized: They process workloads that handle massive datasets in memory and deliver them quickly.
>>Accelerated Computing: It aids in the execution of floating-point number calculations, data pattern matching, and graphics processing. These functions are 
carried out using hardware accelerators.
>>Storage Optimised: They handle tasks that require sequential read and write access to big data sets on local storage

65-What is the difference between a Spot Instance, an On-demand Instance, and a Reserved Instance?

>>Spot instances are unused EC2 instances that users can use at a reduced cost.
>>When you use on-demand instances, you must pay for computing resources without making long-term obligations.
>>Reserved instances, on the other hand, allow you to specify attributes such as instance type, platform, tenancy, region, and availability zone. 
Reserved instances offer significant reductions and capacity reservations when instances in certain availability zones are used.

On-demand Instance
It is cheap for a short time but not when taken for the long term
Spot Instance
It is less expensive than the on-demand instance and can be bought through bidding. 
Reserved Instance
If you are planning to use an instance for a year or more, then this is the right one for you

66-With specified private IP addresses, can an Amazon Elastic Compute Cloud (EC2) instance be launched? If so, which Amazon service makes it possible?

>>Yes. Utilizing VPC makes it possible (Virtual Private Cloud).

67-What Are Some of the Security Best Practices for Amazon EC2?

>>Security best practices for Amazon EC2 include using Identity and Access Management (IAM) to control access to AWS resources; restricting access by only allowing 
trusted hosts or networks to access ports on an instance; only opening up those permissions you require, and disabling password-based logins for instances  launched from your AMI.

68-What are the common types of AMI designs?

Fully Baked AMI
Just Enough Baked AMI (JeOS AMI)
Hybrid AMI

69-what is t2/t3 unlimted in ec2 

>> If you're not using auto-scaling to your web servers sudden spikes come to the webserver if you enable t2/t3 unlimited then that time will increase the RAM and CPU according to that it will be chargeable

70-what is ENI in ec2 

>>elastic network interface you can create multiple network interfaces in. For example, you have a DB instance you can choose one network interface or multiple like 3 or 4 network interfaces you
can give that endpoint to one is ssh 2 second app-server connection 3 is backup like this. but here you can choose only one availability zone you cant use multiple availability zones 

71-what is bootstraping in ec2 or userdata 

>>you can add your script while launching an ec2 instance it will execute that scripyt while createing instance 

72-how to stop the spot instance 

>> While launching the spot instance you can choose the spot instance and click on customize then select the persistent and select the interpution bheaviour stop and click on launch    

73-how to secure the instance 

>>without enabling port number 22 we can secure out Instance
>>we can connect by using aws session-manager create policy for AmazonSSMmanageinstance store attach a policy to role and attach role to 
ec2 instance you you can stop and start then you can access using the session manager 

>>pom full form project object model 
>>you can move your data local env to aws using a storage gateway. instead of a storage gateway, you can mount s3 bucket but its not recommended 
>>for exampel you created rds read with replica in secondary also you will see the first read server details only if you want to secondary where the deployed go in the ec2 netwrok interface it will show 
the private ips you can see where it is deployed   
>>using secondary in rds or any database when you take the backup the disk will freeze that time secondary will help you 
>>if your disaster reccovery time you thinked right how the us-east-1 and ap-south-1 dbs will synk. you can use read replica so it will
 help you to synk if us-east-1 then you route53 failover will trigger to ap-south-1 then read replica will help you 
>>if you want rds reserved instance you can choose   
https://docs.aws.amazon.com/secretsmanager/latest/userguide/integrating_csi_driver.html
https://waswani.medium.com/integrating-secrets-manager-with-aws-eks-79c93e70c74e
https://thenewstack.io/managing-kubernetes-secrets-with-aws-secrets-manager/
https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/
https://xebia.com/blog/how-to-access-your-aws-secret-manager-secrets-in-an-elastic-kubernetes-service-cluster/
https://aws.amazon.com/blogs/containers/aws-secrets-controller-poc/
https://www.arthurkoziel.com/sync-aws-secrets-manager-to-k8s-secrets/
connect aws secret manager to kubernetes deployment
https://docs.aws.amazon.com/systems-manager/latest/userguide/integrating_csi_driver.html
https://www.google.com/search?q=aws+ssm+parameter+store+kubernetes&rlz=1C1VDKB_enIN1053IN1053&oq=aws+ssm+parameter+store+kubernetes+&gs_lcrp=EgZjaHJvbWUyCggAEEUYFhgeGDkyDQgBEAAYhgMYgAQYi
gUyDQgCEAAYhgMYgAQYigUyDQgDEAAYhgMYgAQYigUyDQgEEAAYhgMYgAQYigXSAQkyMTEwMmowajeoAgCwAgA&sourceid=chrome&ie=UTF-8

dynamodb connection to web server 
https://yuta-san.medium.com/a-simple-web-service-by-dynamodb-lambda-cloudfront-and-s3-a94b308669c
https://pravinmishra.in/how-to-add-aws-dynamodb-database-to-a-static-website-hosted-in-amazon-s3-a-step-by-step-guide/
https://www.cdata.com/kb/tech/dynamodb-cloud-linked-server.rst
you can see how to attach permission boundry to user video dymnodb sreeharsha 28 if the user is created then lambda will triger your created rule in cloudwatch selected the iam gievn code slecting 
lambda function code you written. if the user is created lambda will trigger user permission will add permission boundary

74-what is ec2 image builder 

>>its is like packer to create the image in aws 

75-What are Key-Pairs in AWS?.can you explain keypair how works 

>>>>A key pair is a combination of a public key that is used to encrypt data and a private key that is used to decrypt data. The key pairs are password-protected login credentials for the Virtual Machines
that are used to prove our identity while connecting the Amazon EC2 instances. The Key-Pairs are made up of a Private Key and a Public Key which lets us connect to the instances.
>>when you launch an instance in aws you are creating or selecting previous key pair they will store the public key into ec2-instance you will get the private .pem key you can download or else if already 
have you can connect 
>>if you use .pem you can easily connect the Linux 

ssh -i anji.pem ec2@55.67.95.76 -->like this 
>>sudo su -
>>vi anji.pem 
paste the private key 
>>:wq
>>chmod 400 anji.pem 
>>ssh -i anji.pem ec2@55.67.96.77 -->like this one linux system to another linux system 

>>if you use windows you can create anji.pem to anji.ppk using you can connect the putty to connect the linux server 
>>if you have anji.pem then you can able to create public keys also using puttygen see video sreeharsha vpc peering 45mins   

76-how to restore the login into ec2 if the pem file lost

>>Use a backup or a copy of the original .pem file:

If you have a backup or a copy of the original .pem file stored securely, you can use it to regain access to the EC2 instance.
Locate the backup or copy of the .pem file and follow the standard procedure to connect to the EC2 instance using the private key.

>>Launch a new EC2 instance and attach the original volume:

Launch a new EC2 instance and make sure to select the same availability zone and the same or compatible Amazon Machine Image (AMI) as the original instance.
During the instance launch, you can create a new key pair or choose an existing key pair to associate with the new instance.
Once the new instance is running, stop it and detach its root volume.
Detach the root volume from the original EC2 instance (the one for which you lost the .pem file).
Attach the original volume as a data volume to the new instance.
Start the new instance and access it using the new key pair. You can then mount the original volume and retrieve any necessary files.

>>Use EC2 Instance Connect or Session Manager:

If you have EC2 Instance Connect or AWS Systems Manager Session Manager set up on the EC2 instance, you may be able to regain access without the need for the .pem file.
EC2 Instance Connect allows you to connect to your EC2 instance using the AWS Management Console or the AWS CLI without a key pair. Check if EC2 Instance Connect is enabled for your
instance and follow the instructions provided by AWS. AWS Systems Manager Session Manager allows you to establish a secure shell (SSH) session to your instance through the AWS Management 
Console or the AWS CLI. If Session Manager is enabled for your instance, you can connect to it without needing the .pem file.

77-how many ways to access if the ssh key or pem key loose 

1-recovery server 
>>you have anji server you loose the key   
>>then create sampi server create sampi key pair  this server must same availability zoen like anji server 
>>then go the anji server stop the server and detach the volume
>>now attach this volume to sampi server 
>>now connect the sampi server now see lsblk disk details 
>>df -h 
>>file -s /dev/xvdf1 
>>mkdir /storage 
>>mount /dev/vdf1 /storage  -->it will through an error  
>>blikd  -->see same uid number that why your getting 
>>mount -o nouuid /dev/xvdf1 /storage 
>>df -h 
>>cp -rF /home/ec2-user/.ssh/authorized_keys /storage/home/ec2-user/..sh/
>>umount /storage 
>>now go the sampi server stop the server and detach the snji volume 
>>now go the anji volume attach to anji volume 
>>now go the anji server take ssh using sampi key 
2-using session manager
>>go the iam role create policy for AmazonSSMmanageinstance store attach policy to role and attahc role to ec2 instance you you cna stop and start then you can access using session manager 
3-using aws cli 
>>used should have ec2-instance permission
>>local system should be configured with cli 
>>managing through cli 
>>once you push your code you can access only 60 seconds 
>>create ec2 instance without key pair  
>>local system 
>>ssh-keygen -f anji 
>>aws ec2-instance-connect send-ssh-public-key --instance-id i-0679bi --instance-os-user ec2-user --availability-zone us-east-1b --ssh-public-key /home/anji/anji.pub --region us-east-1 
>> Then you can connect the server within 60 seconds when you push the key  
>>ssh -i anji.key ec2-user@56.46.37.89
4-YOu can push the public key into the ec2-instance using the systems manager full access attached to that ec2-instance then go to the systems manager select the shell script run the 
>>echo "public-key paste here" > /home/e2-user/.ssh/authorized_keys

78-Can we recover the EC2 instance when we have lost the key?

>>Yes, it is possible to recover an EC2 instance when you have lost the key. You can create a new key pair and then use it to connect to the instance.
Alternatively, you can also use other authentication methods such as user name and password, or even IAM roles to access the instance.
However, if you have encrypted the root volume of the instance using the lost key pair, you will not be able to recover the data on that volume.
>> If you lost the private key no longer need to take that form anywhere instead you can create one more keypair if you have the SSM or IAM option then log in to that new instance copy that public key 
pass that key to the old instance using SSM in this place ~/.ssh/authorized_keys 

79-how to encrypt the unencrypted AMI

>>A Launch Configuration is a configuration template that specifies the parameters for launching instances in an ASG. It includes details such as the EC2 instance type, AMI ID, 
security groups, key pairs, block device mappings, and user data scripts. When creating an ASG, you associate it with a Launch Configuration. If the desired capacity of the ASG changes, 
the ASG automatically creates new instances based on the Launch Configuration settings

80-how NAT-Gateway will work

>> When we launch the nat-gateway in the public subnet then we can add it to private route tables

81-Can S3 Be Used with EC2 Instances, and If Yes, How?

Amazon S3 can be used for instances with root devices backed by local instance storage. That way, developers have access to the same highly scalable, reliable, fast,
inexpensive data storage infrastructure that Amazon uses to run its own global network of websites. To execute systems in the Amazon EC2 environment, 
developers load Amazon Machine Images (AMIs) into Amazon S3 and then move them between Amazon S3 and Amazon EC2.
>>Amazon EC2 and Amazon S3 are two of the best-known web services that make up AWS

82-What is the maximum limit of elastic IPs anyone can produce?

>>A maximum of five elastic IP addresses can be generated per location and AWS account.

83-What is EIP.

>>Elastic IP (EIP) is a static IPv4 address designed for dynamic cloud computing. It is associated with your AWS account and can be allocated
 to and released from instances as needed. Here are key characteristics and use cases for Elastic IPs:
Static Public IP Address: An Elastic IP is a static public IP address that you can assign to an EC2 instance or a Network Load Balancer (NLB).
 Unlike the default public IP addresses assigned to instances, Elastic IPs remain constant unless explicitly released.
Failover and High Availability: Elastic IPs can be used in conjunction with Amazon EC2 instances in a Virtual Private Cloud (VPC) to implement 
high availability and failover configurations. If an instance fails, you can quickly remap the Elastic IP to another instance.

>> If you use a private ip any instance it will not change until you change. you use any public ip by default if the server restarts or reboot or stop then IP will change. instead you can use elastic-ip if
the server will restart or stop or reboot anything ip will not changed this elastic-ip your paying bill so the AWS will allocate until you released that IP so use of elastic-ip 
>>you can assign single elastic-ip multiple servers like you can assign one system then work is complete then remove elastic-ip to system then attach to another system like this you can attach one system 
after one more system 

use-Cases:-
1-nat-gateway
2-transit gateway
3-load Balancer
4-private-servers 

84-how many ENI can attach to one instance 

>> It's dependent on the instance of two ENIs only t2.micro 

85-What do you understand by 'changing' in Amazon EC2?

>>To make limit administration easier for customers, Amazon EC2 now offers the option to switch from the current 'instance count-based limitations' to the new 
'vCPU Based restrictions.' As a result, when launching a combination of instance types based on demand, utilization is measured in terms of the number of vCPUs.

86-what EBS Volumes

>>Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices. You can mount these volumes as devices
on your instances
>>It is designed to be used with Amazon Elastic Compute Cloud (EC2) instances to provide scalable and high-performance block-level storage volumes. EBS volumes are essentially virtual hard drives that 
can be attached to EC2 instances to provide additional storage space.it is permanent storage/persistence. it can be increased to a max of 16TB you can't decrease the ebs
>> It is available in SSD and HDD

Use-Cases:-
1-Operating System and Application Storage: EBS volumes are commonly used to store the operating system, applications, and other software required by EC2 instances.
2-Databases: EBS is frequently used to store database files, providing the required storage for relational databases like MySQL, PostgreSQL, Oracle, and Microsoft SQL Server.
3-File Systems: EBS volumes can be used as the storage backend for file systems. For example, you can attach EBS volumes to EC2 instances to create a scalable and reliable file system.
4-Data Backup and Disaster Recovery: EBS snapshots are often used for backup and disaster recovery purposes. Snapshots can be used to create copies of volumes, and these copies can be used to restore data
in case of data loss or system failures.
5-Development and Testing Environments: EBS volumes are valuable in development and testing scenarios where you need scalable and flexible storage for software development, testing, and debugging.
6-Media Storage: EBS volumes can be used to store and retrieve media files, such as images, videos, and audio files, providing a scalable and high-performance storage solution.
Dis-Advantages:-
1-Performance Variability: EBS performance can be variable, especially for the General Purpose (gp2) and Throughput Optimized (st1) volumes. While Provisioned IOPS (io1) volumes provide predictable performance,
they can be more expensive.
2-Cold Start Latency: When launching a new EBS volume or restoring a volume from a snapshot, there can be an initial latency (cold start) as the data is loaded or provisioned.
This might impact the performance for a short period.
3-Pricing Model: EBS costs can accumulate, particularly with Provisioned IOPS volumes or snapshots. Users should carefully consider the pricing model, especially for high-performance workloads,
to avoid unexpected costs.
4-Region Dependency: EBS volumes are tied to a specific AWS region. If you need to move data between regions or have a multi-region architecture, you may need to use mechanisms like AWS DataSync 
or third-party tools.
5-Snapshots Are Region-Specific: EBS snapshots are region-specific. If you want to use a snapshot in another region, you need to copy it to the desired region, which might incur additional costs.
6-Volume Size and Type Changes: While you can modify the size of an EBS volume, changing the volume type (e.g., from gp2 to io1) requires creating a new volume and migrating data, which can involve downtime.
7-Backup and Restore Complexity: Although EBS snapshots are a powerful backup tool, managing and restoring multiple snapshots or dealing with complex backup strategies might require additional scripting
or management tools.
8-Limited Granularity for Volume Modifications: Certain modifications to EBS volumes, such as changing the volume type or encryption status, may require creating a new volume and copying data.
There are limitations on the granularity of modifications.
9-Dependency on EC2 Instances: EBS volumes are associated with specific EC2 instances. If an instance is terminated, the EBS volume must be manually detached and reattached to another instance.
10-ebs volume you can apply only one ec2 system
11-ebs volume you can apply only one ec2 system and it should be the same availability zone

advantages:-
>> You can directly mount to the system 
>> It can shared between multiple servers and multiple availability zones

87-How to increase EBS volume size.

>>Identify the EBS Volume:
>>Select the EBS Volume:
>>Modify Volume:
>>specify New Size 
>>Confirm

88-How to decrease EBS volume.

>>Unfortunately, it is not possible to directly decrease the size of an existing EBS volume in Amazon Elastic Block Store (EBS). However, you can effectively reduce the amount of storage space used
by an EBS volume by creating a new smaller volume and migrating the data from the old volume to the new one. Here's a step-by-step guide on how to perform this process:
1-Create a Snapshot of the Old Volume: Before making any changes to the existing EBS volume, it's crucial to create a snapshot. This snapshot serves as a backup copy of the volume's data, allowing
you to revert to the original state if needed. To create a snapshot, navigate to the EBS console in the AWS Management Console and locate the volume you want to reduce. Click on the "Actions" button and
select "Create Snapshot." Assign a descriptive name to the snapshot and click "Create" to proceed.
2-Create a New Smaller Volume: Once the snapshot is complete, proceed to create a new EBS volume with the desired smaller size. Ensure the new volume is created in the same Availability Zone as the
original volume. To create a new volume, go to the EBS console, select "Volumes" from the left-hand menu, and click on the "Create Volume" button. Specify the desired volume size, select the same
volume type as the original volume, and choose the appropriate Availability Zone. Click "Create Volume" to finalize the process.
3-Attach Both Volumes to the Instance: To transfer the data from the old volume to the new one, you'll need to attach both volumes to the same EC2 instance. Identify the EC2 instance that is 
currently running the old volume. Locate the "Instances" section in the AWS Management Console and select the particular instance. Click on the "Actions" button and choose "Attach Volume." 
In the pop-up window, select the old volume and click on "Attach." Repeat the process to attach the newly created smaller volume to the same instance.
4-Transfer Data between Volumes: Once both volumes are attached to the instance, you can initiate the data transfer process. Log in to the EC2 instance and use a file system cloning tool to copy
the data from the old volume to the new one. Popular cloning tools include rsync, dd, and Clonezilla.
5-Detach and Delete the Old Volume: After successfully transferring the data, you can detach the old volume from the instance. This will make the old volume unmountable and free up the associated 
storage space. To detach the old volume, navigate to the EC2 console, select the instance, and click on the "Actions" button. Choose "Detach Volume" and select the old volume to detach it.
Once detached, you can safely delete the old volume from the EBS console.

89-Types of EBS-Volumes

===general purpose ssd gp2
>>it is cheaper compared to provisioned iops
>>it is high speed gp2  performance
>>minimum 1GB to you can extend volume upto 16TB
>>Minimum IOPS 100
>>Maximum IOPS 16000
>>if you taken 1GB you will get 3IOPS          -->input/output operations per second
>>you can't change iops you should use aws given iops

===gp3
>>it is also costly to gp2
>>minimum 1GB to you can extend volume upto 16TB
>>Minimum IOPS 3000
>>Maximum IOPS 16000

====IO1 Provisioned IOPS
>>it is costly compared to gp2 
>>it is also high spped performance
>>minimum 4GB to you can extend volume upto 16TB
>>Minimum IOPS 100
>>Maximum IOPS 64000
>>if you taken 1GB you will get 50IOPS
>>you change iops what you want you can modify the iops details

====IO2 Provisioned IOPS
>>it is costly compared to gp2 and IO1
>>it is also high spped performance
>>minimum 4GB to you can extend volume upto 64TB
>>Minimum IOPS 100
>>Maximum IOPS 256000
>>if you take 1GB you will get 1000IOPS
>>you change iops what you want you can modify the iops details

90--===example for iops gp2&IO1

DB requirement given by client 10TB he needs 100000 IOPs using do storage

gp2:
2 x 5120 = 10TB  30720 iops you will get
2 x 5120 = 10TB  30720 iops you will get
2 x 5120 = 10TB  30720 iops you will get
1 x 2500 = 2.5TB 7000 iops you will get 
>>estimate cost of your monthly bill ($3650.90)
>>con: max 250mb/sec

===if you need 100000iops you need to increase gp2 instance but IO1 you can get easily

IO1
2 x 5120 = 100000 iops with each volume 50000iops
requirement is fullfield  but it is costly compared to gp2

eatimate cost of monthly bill ($8558)
con:max throughput 781mb/sec

===cold HDD and optimized HDD and Magnetic (standard)

>>remaining all drives is slow and it is cheaper 
>>it is used for storage purpose or testing servers purpose
>>when you cant use data regularly then you can use this

where we can sql databese we can use ebs volumes explaining beloow

>>windows or linux we can gp2 volume
>>system databases we can gp2 volume
>>user database we can use IO1 volume
>>database transaction logs we can use IO1 volume 
>>tempDB we can use we can use instance store or standard
>>page file i virtual memory we can use instance store or standard
>>backups  we can use magnetic or standard

91-can attach ebs volume to multiple instance 

>>no its possible in gp2 and gp3. its available in IO1 and Io2 will available to attach multiple c5 large instance 

92-how to reduce my snapshot prices 

>> If you are not using your snapshots frequently you can create an archive snapshot it will reduce the prices of your snapshot
types of snapshot storage
standard snapshot  -->frequently accessed 
archive snapshot   -->in frequently accessed

93-Is it possible to retrieve the snapshot iam Unfortunately deleted my snapshot or ami 

>>yes its possible to retrieve if you configure recycle bin your aws account and retention period 

94-what is recycle bin in aws 

>> You can create this for backup or Unfortunately, delete your AMIs or Snapshots you can retrieve that 
minimum 1 day to max 365 days 

95-how to protect or encrypet the ebs volumes or snapshot 

>>you can enable to encrypt the volume to secure the volume
>>while create the volume you can select the encrypt and 
>>if you forgated to encrypet the volume you can enable ecncrypt you need to follow this first take snapshot of your volume then using that sanpshot take volume form snapshot and encryption will
available you create that 

96-is it possible to share your snapshot to another region or another account

>>yes its possible to share your snapshot to another region
>>yes another account is also available select the snapshot click on permission click on add account id paste the which account you want to send that 

97-if you want take snapshot everyday morning 6 clock how you will achive this 

>>in ec2 there is option lifecycle-manager you can create that in the lifecycle-manager you can choose volume or instance you can take snapshot possible by using tags name = anji this need to time and 
daily and retention period 
>>you can create are also then if the snapshot exists last two months back then delete that snapshot also 
>>you can choose or do using lambda also 
>>click on create a function -->craete a function -->lambda_function under you can paste your code and give proper permission

# Backup all in-use volumes in all regions
import boto3

def lambda_handler(event, context):
    # Create an EC2 client
    ec2 = boto3.client('ec2')

    # Get all in-use volumes in all regions
    result = ec2.describe_volumes(Filters=[{'Name': 'status', 'Values': ['in-use']}])

    # Loop through each volume
    for volume in result['Volumes']:
        print(f"Backing up {volume['VolumeId']} in {volume['AvailabilityZone']}")

        # Create snapshot
        result = ec2.create_snapshot(VolumeId=volume['VolumeId'], Description='Created by Lambda backup function ebs-snapshots')

        # Get snapshot resource
        ec2resource = boto3.resource('ec2')
        snapshot = ec2resource.Snapshot(result['SnapshotId'])

        # Find name tag for volume
        volumename = 'N/A'
        if 'Tags' in volume:
            for tags in volume['Tags']:
                if tags["Key"] == 'Name':
                    volumename = tags["Value"]

        # Add volume name to snapshot for easier identification
        snapshot.create_tags(Tags=[{'Key': 'Name', 'Value': volumename}])

98-can I store my EBS volume in an s3 bucket

>>When you create an EBS snapshot, it's automatically stored in an Amazon S3 bucket that's managed by Amazon Web Services (AWS). You can copy snapshots
within the same AWS Region, or from one Region to another. However, you can't copy snapshots to the S3 buckets that you manage.

99-Define Snapshots in Amazon Lightsail?

>>A snapshot is a base feature for creating backups of your EBS volumes. A snapshot takes a copy of the EBS volume and places it in Amazon S3, where it is 
stored redundantly in multiple Availability Zones. The initial snapshot is a full copy of the volume; ongoing snapshots store incremental block-level changes only
>>and can be used to enable disaster recovery, migrate data across regions and accounts, and improve backup compliance

100-delete unused EBS volume using lambda

1-Create an IAM Role for Lambda:
2-Create a Lambda Function:
3-
import boto3

def lambda_handler(event, context):
    ec2 = boto3.client('ec2')
    response = ec2.describe_volumes()

    for volume in response['Volumes']:
        if 'Attachments' are not in volume:
            # This volume is not attached to any instance
            volume_id = volume['VolumeId']
            print(f"Deleting unattached volume: {volume_id}")
            ec2.delete_volume(VolumeId=volume_id)
4-Create a CloudWatch Events Trigger:
5-Create a CloudWatch Events Trigger:
6-Test Your Lambda Function:
7-Schedule the Lambda Function:
8-Monitor and Logging:
9-Alerts:
10-Cleanup Strategy:

101-I have one scenario my aws account anyone try to create the ebs volume that volume must be encrypeted either he can select encryption or not how we can achive this 

>>go the ec2 dashboard click console experiments and click data protection and security click ebs secuirty and select the enable

102-i have volume in us-east-1a one instance i need to move that us-east-1b onemore instance  is it possible in aws

>>you can take snapshot of the the volume then select a snapshot and create a volume then you can attach the server 
>>if you have an Amazon Elastic Block Store (EBS) volume attached to an EC2 instance in a specific Availability Zone (e.g., us-east-1a) 
and you want to move it to another Availability Zone (e.g., us-east-1b), you generally cannot directly change the Availability Zone of an 
existing EBS volume. However, there are a few approaches you can take to achieve a similar outcome:

1-Create a Snapshot and Create a New Volume:

Create a snapshot of the existing EBS volume in us-east-1a.
Create a new EBS volume using the snapshot in us-east-1b.
Attach the new volume to your EC2 instance in us-east-1b

2-Use AWS DataSync:

AWS DataSync can be used to transfer data between EBS volumes in different Availability Zones.
Set up an EFS (Elastic File System) in us-east-1a and sync the data from the EBS volume to the EFS.
Create a new EBS volume in us-east-1b and sync the data from the EFS to the new EBS volume.
Attach the new volume to your EC2 instance in us-east-1b.

3-Create an Amazon Machine Image (AMI):

Create an Amazon Machine Image (AMI) of your EC2 instance in us-east-1a.
Launch a new EC2 instance in US-east-1 b using the AMI.
Ensure that the new instance is using the desired EBS volume configuration.

>>you created ec2 instance in us-east-1a and then created volume 60gb in us-east-1b  trying to attach its not possible .
 volume must be same availability_zone either wise wont accept 

>>you need CSR and private key then you can create the cname Authentication then click on ok it will give you certificate download you need crt certificate and ca.bundle 

103-what is DLM (data life cycle manager) in AWS 

>>With Amazon Data Lifecycle Manager, you can manage the lifecycle of your AWS resources. You create lifecycle policies, which are used to automate operations on the specified resources.
Amazon Data Lifecycle Manager supports Amazon EBS volumes and snapshots.

104-for example you have taken snapshot in ebs volume that stored in 10 gb name snapshot-A , then you stored deleted somedata or added somedata 4 gb taken snapshot-B , like this 
you added 6 gb size snapshot-C what will you restore in this scenrio

>>In the scenrio we will take snapshot-C because this all the data for last we taken snapshot      

105-what is the difference between AMI image taken and snapshot taken AWS 

>>if you taken Image in AMI like this will be like your operating system and tools and software and data . this operating restores and data only 
>>if you taken snapshot your taken care by your select the volume and taking snapshot of your volume not the operating system. data restore only 
>>your using that snapshot create a new volume and attaching to ec2 instance your recover your data . recomended last month snapshot is prefered Organizations
>>you can share the ami into another region and another account also 
>>you can copy the snapshot into another region. and you created the volume in us-east-1a and you need to use us-east-1b then  take snapshot while createing volume you can choose or change the 
availability zone us-east-1b or us-east-1c also    
>>if your executing or increasing the volume in a day you need to wait until the 6 hours 

You are partially correct. Here's a breakdown of waiting times when modifying an EBS volume in AWS:

Increasing volume size:

Minimum wait: 6 hours after initiating the resize. This applies to all EBS volume types and is necessary for the operation to complete and the new size to become available.
Additional wait time: The actual time for the resize to complete can vary depending on several factors, including:
Original and new volume size: Smaller volumes might take less time, while larger ones could take up to 24 hours or even longer.
EBS volume type: SSD volumes generally resize faster than HDD volumes.
Current I/O activity on the volume: High usage can prolong the resize process.
Other modifications:

Other operations: Modifying snapshot retention period, changing the volume type, attaching/detaching from an instance, etc., also have minimum wait times depending on the specific operation.
Same volume, subsequent modification: After any modification, you must wait at least six hours and ensure the volume is in the "in-use" or "available" state before modifying it again.
Overall:

Always anticipate a minimum of 6 hours after initiating any EBS volume modification.
Account for additional wait time based on the volume size, type, and current activity.
>>you can share your snapshot another region and another AWS Account also 

106-What is an Instance store

>>instance store is temporary storage data will be deleted if server reset
>> It cannot be increased or decreased
>>only ssd type is available
>> When you use instance storage while launching ec2 if the server restart or reset the data will be loose

107-what is lifecycle manager in ec2 

>>you can choose the life cycle manager in ec2 or volume you can take the snapshot automatically you give instruction and time and retain period it will automatically 
take the snapshot or volumes it will store that time .you can use tags also take snapshots 
>>you can take snapshots on particular time retention period and also if you want exclude root volume is possible and cross region snapshots and cross acount also possible 

108-what is EFS and what is the use case of it

>>Amazon Elastic File System is a cloud storage service provided by Amazon Web Services designed to provide scalable, elastic, concurrent with some restrictions, and encrypted file storage for 
use with both AWS cloud services and on-premises resources.
>>After you create an existing file system (EFS), you can't change its encryption setting. This means you can't modify an unencrypted file system to make it encrypted. Instead, you must: 
Create a new Amazon EFS file system with encryption turned on
>>Amazon Elastic File System (EFS) is a scalable and fully managed file storage service provided by Amazon Web Services (AWS). It is designed to provide scalable and shared file storage that 
can be accessed concurrently by multiple Amazon EC2 instances. EFS uses the Network File System version 4 (NFSv4) protocol, making it compatible with a wide range of Linux-based applications and workloads.
>> The storage limit 8PB
>>it can be shared between multiple servers and multiple regions
>>and replication is available you can create one more region also for replication you can replcate your data 
>>you can attach this or mount via Ip also but not recomended if ip down you get issues so not recommend 
>>if your mount the ec2 instance but your not added that fstab if the system reboot the mount point will loose so add fstab always 
vi /etc/fstab
fs-02097af1ee5a927d6.efs.us-east-1.amazonaws.com:/ /var/lib/jenkins/ nfs defaults 0 0
fs-02097af1ee5a927d6.efs.us-east-1.amazonaws.com:/ /usr/share/nginx/html/ nfs defaults 0 0

:wq
>>if you reboot the data will not go   
>>if your not select the all availability_zone like 6 is there but you selected us-east-1a and us-east-1b only if you want mount us-east-1c means it will not work or not mount you need to select all 
availability_zone for fault tolerance or latency

Uses cases:-
>>you can directly mount the system to ec2 instances
>>it is a share folder for network in a region
>>it is network file system  
>>it is unix or linux supported
>>it can shared between multiple servers and multiple regions
>>the storage limit 8PB
>>its a serverless service this is Managed by AWS 

109-what is the use cases of EFS

1-Content Management Systems (CMS):EFS can be used to store and share assets, templates, and other files across multiple instances in a CMS environment. 
This is useful for websites and applications that rely on a centralized repository for managing content.
2-Web Serving:EFS is well-suited for web serving architectures where multiple web servers need access to the same set of files. It provides a scalable and shared storage solution for hosting web content.
3-Development and Build Environments:In development environments, EFS can be used to store source code, libraries, and build artifacts. Multiple development instances can access and modify these 
shared files, facilitating collaboration and consistency.
4-Big Data Analytics:EFS is suitable for big data workloads where multiple EC2 instances in a cluster need access to shared data sets. It provides a common storage platform for distributed processing
frameworks like Apache Spark or Apache Hadoop.
5-Containers and Microservices:EFS can serve as a shared storage backend for containers running on Amazon Elastic Container Service (ECS) or Amazon Elastic Kubernetes Service (EKS). 
This enables multiple containers to access shared data, supporting microservices architectures.
6-Home Directories and User Files:EFS is suitable for scenarios where multiple users or instances need access to common home directories or user-specific files. This can be beneficial in environments
where users require consistent access to their files across different instances.
7-Media and Entertainment Workflows:EFS is well-suited for media processing and entertainment workflows where large files, such as videos or graphics, need to be shared and processed by multiple instances 
simultaneously.
8-Log and Data Storage for Applications:Applications that generate logs or require persistent data storage can leverage EFS for shared file storage. This is beneficial for scenarios where logs need to be
centrally stored and analyzed.

110-what is Advantage of EFS.

Advantages:-
1-Elasticity and Scalability:EFS automatically scales storage capacity up or down as you add or remove files, providing a high level of elasticity.
It can grow to petabyte scale, accommodating changing storage requirements without manual intervention.
2-Shared File System:EFS allows multiple Amazon EC2 instances to access the same file system concurrently. This makes it suitable for applications that require shared access to data.
3-Compatibility:EFS supports the Network File System version 4 (NFSv4) protocol, making it compatible with a wide range of Linux-based applications and workloads.
4-Managed Service:AWS takes care of the operational aspects of EFS, such as hardware provisioning, software configuration, patching, and maintenance. This reduces the administrative overhead for users.
5-Performance:EFS provides low-latency performance for a broad spectrum of workloads. It can handle a large number of concurrent connections and is suitable for various use cases, 
including big data analytics, media processing, and content management.
6-Security:EFS allows you to control access to your file systems using AWS Identity and Access Management (IAM) roles and policies. You can also use POSIX permissions to control access at 
the file and directory levels.
7-Integration with Other AWS Services:EFS can be easily integrated with other AWS services. For example, it can be used as shared storage for containers running on Amazon Elastic Container Service (ECS)
or Amazon Elastic Kubernetes Service (EKS).
8-Backup and Restore:EFS supports AWS Backup, allowing you to create and manage backups of your file systems. This makes it easier to implement backup and restore strategies for your data.
9-Regional Redundancy:EFS file systems can be configured for regional redundancy, ensuring high availability. This means that your data is stored across multiple Availability Zones within a region.
10-Cost-Effective:EFS offers a pay-as-you-go pricing model, where you pay only for the storage you use. It eliminates the need for upfront investments in hardware and allows you to scale your storage
infrastructure based on your actual needs.

111-what is EFS life cycle manager.

>>Amazon EFS lifecycle management uses an internal timer to track when a file was last accessed, and not the POSIX file system attributes that are publicly viewable. Whenever a file in Standard or
One Zone storage is accessed, the lifecycle management timer is reset.
>>Amazon EFS lifecycle management automatically manages cost-effective file storage for your file systems. When enabled, lifecycle management migrates files that have not been accessed for a set 
period of time to the EFS Standard–Infrequent Access (Standard-IA) or One Zone–Infrequent Access (One Zone-IA) storage class, depending on your file system. You define that period of time by using 
the Transition into IA lifecycle policy.
types of storage classes available in EFS:-
1-standard
2-standard-IA
3-one-zone-IA 

112-what is the difference between EFS and EBS 

EBS:-
1-The file storage type is block storage
2-It is attached to a single instance only 
3-limited Scalability to attach or increase the volume 
4-it works Boot volumes, data storage
5-it more expensive compared to EFS 
6-It can be increased to a max of 16TB you can't decrease the ebs
7-so many types of storage classes availble  
8- It is commonly used as block storage and is attached to EC2 instances using block-level protocols. EBS volumes are accessed within a single Availability Zone and can only be attached to 
one EC2 instance at a time.
EFS:-
1-The file storage type is a file storage
2-it is attached to multiple instances only 
3-Highly scalable
4-It works with Shared file systems, large data storage
5-less expensive compared to EBS 
6-It supports 8PB 
7-limited storage classes available only 
8-It uses the Network File System version 4 (NFSv4) protocol, allowing multiple EC2 instances to access the file system concurrently. EFS supports shared access across multiple Availability Zones 
within a region.

113-what is the advantages of Storage classes

1-Cost Optimization: Storage classes allow you to choose the right balance between performance and cost. If you feel your not frequently accessing then you can create the life cycle move to another
cheap stoarge 
standard 
standard-IA
One-Zone-IA 
Glacier
Glacier Deep Archive
Intelligent-Tiering
2-Performance Tiers: Speed based on selecting the storage class if you select the standard then it will fast if you select the one-Zone-IA it is low and glacier is to much slow 
3-Durability: Storage classes often have different durability characteristics. For example, Standard storage in Amazon S3 has 99.999999999% (11 9's) durability,
4-Lifecycle Management: there lifecycle management available you can move your data whenever you wnat 
5-Built-In Versioning Support: in this storage class there option called versioning it will help you keep data size low and if any case you deleted some files the version will help you 
6-Data Archiving and Compliance:Storage classes like Amazon Glacier and Amazon Glacier Deep Archive are designed for long-term data archiving and compliance. These storage classes offer extremely
low-cost storage for data that is rarely accessed but needs to be retained for compliance reasons.
7-Multi-Tiered Storage Strategies:By using multiple storage classes and lifecycle policies, you can implement multi-tiered storage strategies. This enables you to optimize storage costs based on the
changing access patterns of your data over time.
8-Integrated Services:Storage classes often integrate with other services and features provided by the cloud storage platform. For example, Amazon S3 Intelligent Tiering integrates with the S3 Inventory
and S3 Analytics features to provide insights into data access patterns.

114-What is Amazon S3? 

S3 is short for Simple Storage Service, and Amazon S3 is the most supported storage platform available. S3 is object storage that can store and retrieve any amount
of data from anywhere. Despite that versatility, it is practically unlimited as well as cost-effective because it is storage available on demand. In addition to 
these benefits, it offers unprecedented levels of durability and availability. Amazon S3 helps to manage data for cost optimization, access control, and compliance. 

>> We can store log files 
>> Maximum uploading of one file size up to 5TB
>>it is objeted level stoarge
>> It is a globally unique name
>> You can upload files 3 types 1 - cli and 2 - aws console 3 - application through API or Jenkins
>> Mostly upload through API or application through
>> The s3 bucket is not global why means when you create a bucket it asks for the region but it will show all the regions
>>s3 availability is 99.9 uptime
>>s3 durability is 99.999999 it will not go delete 
>> When you upload a file it will save in 3 az but it will not show that saving
>> It will charge when you upload and storage and when you access through the internet will cost 
>> You can enable version control

---storage types
>> If frequently using data means use this (standard)
>> If we can't access frequently that old data we can use this (standard, glacier)
>>temporary log files you can use this (One Zone-IA)

115-what is the S3 life cycle manager.

>>You can create the bucket using create the lifecycle then which will save in the first 30 days standard if not frequently accessed that you can move that data into
standard-IA then this data also not frequently used then you can move into One-Zone-IA then move to S3-glacier which is permanent data storage for long-term
if you want to delete you can specify after 1 or 2 or 10 years after also

1-standard
2-standard I-A
3-onezone I-A
4-S3 Intelligent-Tiering,
5-S3 glacier

116-what is S3 Replication

>> Transfer the data from one bucket to another bucket
>>craete bucket anji256 on that time you should enable the replication
>> It is used for mostly the log purpose of saving

=== How to add replication
>>click on add rule >>enable version
>>select bucket anji159
>> Select the storage type glacier or anything
>> Click on next done
>>if you upload data in the bucket of anji256 the data will save in the bucket of anji159

117-How many S3 buckets can be created?

Up to 100 buckets can be created by default

118-S3 bucket storage types

1-standard
2-standard I-A
3-onezone I-A
4-S3 Intelligent-Tiering,
5-S3 glacier

119-What is Glacier

>> S3 Glacier cheap storage
>> For long-term backup, we can use this
>> If you stored data in Glacier if retrieving it  will charge it's not for regular purposes it's for long-term
>> You can take glacier backup using third-party tools Fast Glacier, altro, veritas, Netbackup, networker, Aavamar, Tivoli
>> Glacier is called the vault

120-what is the difference between s3 storage ebs or ec2 volume storage  

>>Here are the key differences between S3 storage, EBS, and EC2 instance storage:

Data Type and Use Case:

S3 (Simple Storage Service): It is an object storage service designed for storing and retrieving any amount of data from anywhere on the web. It is suitable for static files, backups, and data archiving.
EBS (Elastic Block Store): It provides block-level storage volumes that can be attached to EC2 instances. EBS volumes are used as persistent storage for databases, file systems, and other data that requires 
frequent updates.
EC2 Instance Storage: EC2 instances come with ephemeral storage that is directly attached to the physical host machine. It is suitable for temporary data and is lost if the instance is stopped or terminated.
Durability and Availability:

S3: S3 is designed for 99.999999999% (11 nines) durability and 99.99% availability over a given year. It automatically replicates data across multiple servers and facilities.
EBS: EBS volumes are designed for 99.8% availability. While they are replicated within a specific Availability Zone (AZ), they are not automatically replicated across multiple AZs.
EC2 Instance Storage: The data on instance storage is tied to the life of the EC2 instance. It is not automatically replicated, and if the instance is stopped or terminated, the data is lost.
Access and Interface:

S3: Accessed via a simple web interface using HTTP/HTTPS, or through AWS SDKs and APIs.
EBS: Appears as a block device to the attached EC2 instance and can be formatted with a file system like any other block storage.
EC2 Instance Storage: Appears as block devices directly attached to the EC2 instance, and it's typically used as raw storage devices.
Pricing Model:

S3: Pay-as-you-go pricing based on the amount of data stored, number of requests, and data transfer.
EBS: Pay-as-you-go pricing based on the provisioned capacity of the EBS volumes.
EC2 Instance Storage: Included in the cost of the EC2 instance, but it is not persistent, and data loss can occur if the instance is stopped or terminated.
Snapshot and Backup:

S3: Versioning and lifecycle policies are used for managing object versions. S3 doesn't have a native snapshot feature.
EBS: Supports snapshots, allowing you to create point-in-time backups of your volumes, which can be used to create new volumes or migrate data to different regions.
EC2 Instance Storage: No snapshot feature for instance storage. Data needs to be backed up using other methods.

121-iam upload a file in s3 standard can i change to onezone-ia is it possible

>>yes its possibe. select the object or file then click on storage select which storage you want you can select 

122-if your delete and permanently delete what is the difference in aws 

>>if you use delete means it will retrieve or retention some days it will get 
>>if you permanently delete it will not retrieve delete permanent

123-in my bucket under i have 10 folders i need to anji folder data only move to Lifecycle management is it possibe to create 

>>yes you can select the bucket click lifecycle configuration and select the bucket and prefix add your folder then select the life cycle policy it will go accordingly
>>you have object tags to your that also you can set or object size like you 100MB files and 110 mb file you need send only 100 mb is also possibe
>>you can delete the data also you can select like standard 30 days then standard-ia 60 days then one-zone-ia 90 days then you can delete   
https://www.youtube.com/watch?v=2CvTsAITNnY&list=PLneBjIzDLECkYQ7dYpvWFSgEwTMeZkykF&index=48  14hours 13 mins 
>>you can copy or move the data in bucket

124-if you use replication means both buckets must be version-enabled

>>you can do replication in another aws also possible 
>>when you createing replication in source bucket to destination bucket you can choose storage class also like 
anji is source the data is standard so smapi bucket is another your anji bucket to sampi bucket sending data while sending you cna choose to stoarge onezone or Standard–Infrequent then that data will 
store the storage class only in the sampi bucket  

>>if you enbale server access logging in s3 enable you can choose any other bucket anyone access this bucket will store that logs in another bucket 
>>if you enable request netrics you can see how many request come and put a delete error every request will show 

125-you can create bucket policy to read all the content from public you need to create policy without using ACL or public 

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::YOUR_BUCKET_NAME/*"
        },
          "Principal": "*"             
    ]
}

>>its not recomended in public 

126-How can I grant a user access to a specific folder in my Amazon S3 bucket?

>>If your IAM user and s3 bucket are present in the AWS account then you can grant the user access to a specific bucket folder using the IAM policy

You can add the IAM policy to individual IAM users or you can attach the IAM policy to an IAM role that multiple users can switch to.
If the IAM identity (user or role) and the s3 bucket belong to different AWS accounts, then you  must grant access to both the IAM policy and the bucket policy

127-What does bucket policy mean?

>>With Amazon S3 bucket policies, you can secure access to objects in your buckets, so that only users with the appropriate permissions can access them. 
You can even prevent authenticated users without the appropriate permissions from accessing your Amazon S3 resources.

128-can I copy my AMI into the s3 bucket?

>>You can store an Amazon Machine Image (AMI) in an Amazon S3 bucket, copy the AMI to another S3 bucket, and then restore it from the S3 bucket.

129-what are the types of encryption in s3

>> Using AWS KMS policy using create the key assign to users then it will use encrypt the data 
two types of encryption there AWS KMS policies

1-symmetric key: the same key is used to encrypt and decrypt 
2-asymmetric key: different keys used to encrypt and decrypt

Server-side encryption – Amazon S3 encrypts your objects before saving them on disks in AWS data centers and then decrypts the objects when you download them. ...
Client-side encryption – You encrypt your data client-side and upload the encrypted data to Amazon S3.

130-Do you prefer to host a website on S3? What’s the reason if your answer is either yes or no?

>>Yes, it can be a good option to host a website on S3. Hosting a website on S3 is a cost-effective solution for static websites that don’t require server-side processing.
S3 is a reliable and scalable storage service that offers high availability and durability.
>>However, hosting a dynamic website with server-side processing on S3 is not recommended as it lacks the necessary computing resources and scalability to handle such websites.
In such cases, it’s better to use other AWS services like EC2 or Elastic Beanstalk.

131-which is faster storage EBS or S3

EBS: EBS provides block-level storage volumes that can be attached to EC2 instances. It offers low-latency, high-performance storage optimized for transactional workloads and applications
that require frequent read/write access to data. EBS volumes are typically used as primary storage for applications running on EC2 instances.

S3: S3, on the other hand, is an object storage service designed for storing and retrieving large amounts of unstructured data. It is highly scalable and durable, offering high availability
and durability for data. S3 is accessed over the network and is well-suited for use cases such as data backup, data archiving, static website hosting, and content distribution.
>>The performance of EBS and S3 can vary based on several factors, including the storage class, instance type, network connectivity, workload characteristics, and access patterns. 
However, in general, EBS is designed for low-latency access and is optimized for performance-sensitive workloads, while S3 is optimized for scalability, durability, and cost-effectiveness
at the expense of slightly higher latency.
>>It's important to consider the specific requirements of your application or use case when choosing between EBS and S3. If your application requires low-latency access to storage and
frequent read/write operations, EBS is usually the more suitable choice. On the other hand, if you have large volumes of unstructured data, require high scalability, or need to 
store data for long-term archival purposes, S3 is a better fit.

In summary, the "faster" storage option between EBS and S3 depends on your specific workload and requirements. It is recommended to analyze your application's needs and characteristics
of the storage services to make an informed decision.

132-Your application needs to access AWS services, such as S3 securely within your VPC. How would you achieve this?

>>To securely access AWS services within the VPC, we can use VPC endpoints. VPC endpoints allow instances in the VPC to communicate with AWS services privately, without requiring
Internet gateways or NAT gateways. 
We can create VPC endpoints for specific AWS services, such as S3 and DynamoDB, and associate them with the VPC. 
This enables secure and efficient communication between the instances in the VPC and the AWS services.

133-Advantage of S3 

Durability and availability: S3 provides high durability and availability for your data.
Scalability: You can store and retrieve any amount of data without worrying about capacity constraints.
Security: S3 offers multiple security features such as encryption, access control, and audit logging.
Performance: S3 is designed to deliver high performance for data retrieval and storage operations.
Cost-effective: S3 offers cost-effective storage options and pricing models based on your usage patterns.

134-hat is S3 Event Notifications and Triggers

>>S3 event notifications allow you to configure actions when specific events occur in an S3 bucket. For example, you can trigger AWS Lambda functions, send messages to 
Amazon Simple Queue Service (SQS), or invoke other services using Amazon SNS when an object is created or deleted.

135-is it possible to create a policy for one user dont access the efs 

>>yes its possible using file system policy 

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Action": "elasticfilesystem:CreateFileSystem",
      "Resource": "*",
      "NotPrincipal": {
        "AWS": "arn:aws:iam::<your-account-id>:user/anji"
      }
    }
  ]
}

136-what is snapshots

>>Snapshots are a point-in-time copy of your data and can be used to enable disaster recovery, migrate data across regions and accounts, and improve backup compliance. 

137-what is object-level logging 

>>you must enable cloud trail then it will store the logs in s3 bucket if you enable object-level-enabling then that logs will stored in s3 bucket or cloudtrail  

138-server-level logging in s3 

>>In s3 server and access log enabling in my account then I am choosing same bucket to store the logs what will happen. It will went to loop . It will create many log files because your access that bucket 
and the logs Or storing your seeing that logs it's repeat process. So it's not recommend so use different bucket
>>who are accessing through internet through your bucket you will see source ip and object will stored into thte bucket 

139-what is object lock in s3 

>>if you want protect the data from s3 bucket or knowown wants delete that data from your bucket you can use that object-lock but this would be enable while createing the bucket after that you can't modify
or edit   
>>version must be enabled otherwise . when you create the bucket must enable other wise you need to contact aws then you can enable 
>>if your enbale retention and compilance for 10 years that bucket data not delete for 10 years so becareful while createing . 

140-my bucket under so much data is there but iam deleteing the data its not delete what could be reason 

>>may be that bucket is enabled while createing the object-lock so that data is not abale to delete 

141-what is events in s3 

>>for example anyone uploading or deleting the data into your bucket if you configure the events then you will get mail or email if you configure SNS through the events only 

142-is it possible iam upload data into anji bucket the data must be upload to sampi bucket automatically how would you achive this 

>>you can replication in s3 bucket or else you can lambda then automatically will happen 

143-is it possible to give only specific ip to S3 bucket accees

>>yes its possible to give specific IP address will access the S3 Bucket using Bucket polciy 
>>yes you deny also possible

{
  "Id": "VpcSourceIp",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "VpcSourceIp",
      "Action": "s3:*",
      "Effect": "Allow",
      "Resource": [
        "arn:aws:s3:::anji159258/*",
        "arn:aws:s3:::anji159258/sampi/*"
      ],
      "Condition": {
        "IpAddress": {
          "aws:VpcSourceIp": [
            "10.1.1.1/32",
            "172.1.1.1/32"
          ]
        }
      },
      "Principal": "*"
    }
  ]
}

144-For example anji and sampi user will be aws account both have s3 permission I need to block the access vinod bucket to anji so you login 
root or admin account then vinod bucket click on permissions then select bucket policy paste the policy you can generate from policy. Aws.Com

{
  "Id": "Policy1707211544451",
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Stmt1707211542618",
      "Action": "s3:*",
      "Effect": "Deny",
      "Resource": "arn:aws:s3:::anji159258",
      "Principal": {
        "AWS": [
          "arn:aws:iam::637423512556:user/anji"
        ]
      }
    }
  ]
}

145-How interviews can trick this question My two user have s3 full access but one user not able to access one specific bucket what could be the reason.

>>The reason for this may be admin may create the bucket policy to deny specific user or your one of user

146-where we can use replication controller in s3 

>>mostly we use S3 logs when uploading server logs you can enale replication we use mostly 

147-what is metrics in s3 

>>you can see the storage and request how much data transfer and replication

148-what is Inventory in s3 

>>you can see the inventory report what files is stored you can craete the report and 

149-if your trying delete the bucket using cmd it not deleteing why 

>>may be the bucket under data is there and also version is enabeld so you need to disbale version or you can use recursive 

>>aws s3 rb s3://anji159258 --resursive  -->it will remove the 

150-upload file into s3 bucket using aws cli

>>aws s3 cp local-file s3://bucket-name/destination-prefix/
>>aws s3 cp example.txt s3://my-bucket/
>>aws s3 cp example.txt s3://my-bucket/my-directory/
>>aws s3 ls s3://my-bucket/

151-how to delete a file in s3 bucket using aws cli 

>>aws s3 rm s3://bucket-name/path/to/file
>>aws s3 rm s3://my-bucket/example.txt
>>aws s3 ls s3://my-bucket/

152-how to create or delete the s3 bucket using aws cli 

>>aws s3 api create-bucket --bucket BUCKET-NAME --region REGION
>>aws s3 rb s3://BUCKET-NAME --force
>>aws s3 mb s3://anji159258

153-can i restricat user for accessing s3 bucket in specific IP 

>>yes you can like IP and region and specific VPC etc . you can add multiple condition to single policy for s3  

154-i have one scenario i have given admin access and multiple allow option but i applied one deny option what will choose by default

>>if you applied 100 of allow option but you denyed one it will take default deny why means it will take deny is high priority

155-can tell this m7g  m7a m7i means in AWS instance

>>m7i  -->intel process 
>>m7a  -->amd based process  
>>m7g  -->gravition process 

156-iam created one instance with 30gb volume while my instance terminated but my volume dont want to delete is it possible

>>yes its possible you can select while launch instance select delete on termination no then it will not delete the volume until you delete

157-what is ec2 backup Strategies

>>we follow the lifecycle manager to take snapshot of ec2 volume based tags what required we maintaine retention period or age which time take backup this will help disaster recover of our application
>>based on credeintialty we maintaine the backup of retention of snapshot some days 

158-if you want upgrade t2.micro to t2.medium is it possible in AWS 

>>yes its possible you need stop the instance then click actions click in change instance type then select t2.medium but you need to stop the ec2 instance
>>this is -Vertical Scaling

159-In my AWS Account ec2 instance is stopped and s3 bucket delete something is changed how you can identify which user is change

>>you can use cloudtrail to identify the user what changes he has done terminated or deleted inside ec2 instance setting changed that will not see cloudtrail but you can use aws config will help you 

160-how to download or upload to s3-glacier directly 

>> Download Fast Glacier Then using fast glacier you can download the data from S3 Glacier and upload is also is possible then using your secret key and access key for authentication purpose 
>>data saving is but retiring is hard

161-what is a storage gateway and what types

>>AWS storage gateway using your uploading data from local or on-premises to cloud 
>>AWS Storage Gateway is a set of hybrid cloud storage services that provide on-premises access to virtually connect the cloud storage.

>>volume-gateway:- A Volume Gateway provides cloud-backed storage volumes that you can mount as Internet Small Computer System Interface (iSCSI)
devices from your on-premises application servers.

>>File-gateway:- File Gateway allows your existing file-based applications or devices to use secure and durable cloud storage without needing to be modified. 
With S3 File Gateway, your configured S3 buckets will be available as Network File System (NFS) mount points or Server Message Block (SMB) file shares.

>>Tape-gateway:-Tape Gateway enables you to replace using physical tapes on premises with virtual tapes in AWS without 
changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on premises for low-latency data access

162-What is auto-scaling?

>>Auto scaling groups must not be used for database
>> Auto scaling groups are used mostly used web and app servers because of static content
>>Auto-scaling is a function that allows you to provision and launch new instances whenever there is a demand. It allows you to automatically increase or decrease 
resource capacity in relation to the demand.Cloud Architect Master's
-----types
1-Vertical Scaling:-
>>t2.micro to t4.medium 
>> It needs machine shutdown or downtime to increase the size of the specification

2-Horizontal Scaling:-
>>t2.micro to t2.micro 
>> If you want to increase the system you need to use horizontal scaling

163-if you added one auto-scaling group under launch-configuration can add one more version 

>>yes you can added new version is into auto-scaling group
select the launch-configuration click on edit select the new launch template or updated 
>>you need to enable cloudwatch montoring eitherwise it will not help why because if you enable it will track for 1 minutes or takes data from last 1 minute or else it will take last 5 minutes it will cause 
an issue in production 

164-what is predictable scaling 

>>it will take the average of last one week or last one day or 8 week data it will identify the trend or traffic then it will deploy new instance this will managed by AWS 

165-what is dynamic scaling 

>>while creating the auto-scaling you can choose scaling policies \
in dynamic scaling in auto-sclaing simple and step scaling both are same 
step scaling is most cost effective
1-target tracking policy:-
>>it automatically create polices based on traffic in scale out and scale in without any further manula intervnetaion 
>>it is managed by completely AWS 
>>it will deploy based you mention min and max desired capacity right that will use 
2-simple scaling && step scaling 
>>both are similar for simple scaling step scaling we need to select cloudwatch matrix and threshold value 
>>step scaling providing more benfites than simple scaling (can add multiple condition to enhance capacity)
>>step scaling is more cost effective for high workload or suddden spike application
any dout see https://www.youtube.com/watch?v=LbXONaiEInE&list=LL&index=14
>>if you use step scaling add instance 50 to 70 add 2 instance and 70 to above addd two more instance like this 

166-can i create time based scaling in aws 

>>for suppose your in banking or any application getting more traffic in moring 9 to evening 8 you can create the schedule auto-scaling in aws then it will launch moring time how many you want it will
delete after the 8pm after and moring will create like keep on going daily  
>>in auto-scaling click on automatic scaling then select the scheduled actions then you can add time and how many you want 

167-so you have multiple ec2 instance using auto-scaling the load decrease you mention schedule based auto-scaling which instance will stop 

>>its based on this terminaligy if you have multiple subnets like us-east-1a under 3 and us-east-1b under 2 and us-east-1c under 1 it will reduce 2 means it will remove one instance in us-east-1a and check next billing which instance cause that will be terminated like this us-east-1b also 
>>its based on billing and number of instance in availability_zone based on time first in first out like this 

168-where we can use cloud watch mostly in AWS

1-Auto-scaling --scaling policies
2-

169-what is ASG and launch config

>>ASG:-ASG stands for Auto Scaling Group. It is a feature provided by Amazon Web Services (AWS) that automatically adjusts the number of instances in a group based on the specified 
scaling policies. ASGs are commonly used to maintain the availability, performance, and cost-efficiency of applications running on EC2 instances.
Key features of Auto Scaling Groups include:
1-Automatic Scaling: 
2-High Availabilty:
3-Elasticity
4Integration with load-balancer
5-Integration with launch configurations

>> launch-config launch configuration is a template that an EC2 Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the 
instances such as the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping.

170-what is lifecycle hook in autoscaling 

>>while your launching an auto-scaling using instance and also launch some data need to wait for some time that time you can create andd use 
in lifecycle hook in auto-scaling 
>>you can create for this create or terminate instance on that time 

171-Explain about ELB

>>A load balancer is a device or service that distributes network traffic across multiple servers to prevent any one server from becoming overwhelmed.
>>A load balancer acts as the “traffic cop” sitting in front of your servers and routing client requests across all servers capable of 
fulfilling those requests 
in a manner that maximizes speed and capacity utilization and ensures that no one server is overworked, which could degrade performance. 
>>load balancer is region dependent
>>you can add multiple target groups into load balancer
>>load balancer which load balance the request 
you can choose roundrobin or least outstanding requests algorithms in Application load Balancer while selecting the load-balancer
if you added three subnets while createing load balancer then after also you can add is possible 
you can share yor logs into s3 bucket if you want 


load-balancing algorithms
1-round robin
2-least connections 
3-least responce
4-weighted round robin
5-weighted least connection
6-ip hash 
7-content based load balancing
8-dynamic loa balancing
9-Session Persistence (Sticky Sessions) 

172-Types of Load-Balancer

>>type of load balancer.
1-network loadbalancer
2-application loadbalancer
3-gateway loadbalancer
4-classic loadbalancer

1- Network load balancer
>>A load balancer serves as the single point of contact for clients. The load balancer whenever the first system not availble then distributes incoming traffic across
multiple targets, such as Amazon EC2 instances.This increases the availability of your application. You add one or more listeners to your load balancer.
>>it is an layer 4 in transpot layer 
>>it is use TCP and UDP protocals
>>direct traffic based on ip address
>>you can add multiple target groups into load balancer
>>Network, or Layer 4, load balancers can provide faster responses,
>>the deregistation delay is 300 seconds
>>it will check every 30 seconds for health checks if you cant reach the server then decalre as not reachable then forward the request to another server
>>if you use one port or listner like 80 you can't use that port onemore time
>>you can save your load balancer logs in s3 
>>it will support path based routing but you need to every time differnt ports or different listners everytime
>>it is region dependent
>>by default cross-zone load balancer is not-enabled you should enable if the required happens
>>it is suitable for single application or easy application
>>you can assign static ip address to network load balancer but this not available in application load balancer 
>>if you want use nlb you can use rdp or ssh like this tcp protocals 

---What is an internal load balancer?
>>An internal (or private) load balancer is used where private IPs are needed at the frontend only. Internal load balancers are used to load balance traffic inside
a virtual network.A load balancer frontend can be accessed from an on-premises network in a hybrid scenario

---what is external load balancer
>>The external load balancer is used to route external HTTP traffic into the cluster

2- Application load balancer
>>ALB is a super advanced, next generation load balancer in AWS  
>>A load balancer serves as the single point of contact for clients. The load balancer whenever the first system not availble then distributes incoming traffic across
multiple targets, such as Amazon EC2 instances.This increases the availability of your application. You add one or more listeners to your load balancer

>>it is layer 7 in application layer
>>it uses https and http
>>it is only for web-applications
>>the deregistation delay is 300 seconds
>>you can save your load balancer logs in s3 
>>you can use one listner or port number of times
>>it is region dependent
>>by default cross-zone load balancer is enabled
>>it will check every 30 seconds for health checks if you cant reach the server then decalre as not reachable then forward the request to another server
>>it will support path based routing or url based routing /app1, /app3, /usermgmnt 
>>it support host based routing(apps.anji.com, sampi.anji.com) 
>>supports for routing based on fields in the request(HTTP Headers, HTTP Method, Query parameters, and Source IP Address)
>>you can transfer the traffic using paths based in application load balancer or  you can use path based routing
>>support for redirectin g reuest from one url to another url 
>>support registering lambda functions as targets 
>>support for returing a custom HTTP Response 
>>Support fro containerized application(AWS ECS)
>>you can create the multiple target group like home , movies, shows then use the path based routing or url based routing
>>it is used or suitable for complex applications 
>>you can't assign static ip address to application load balancer but this available in network load balancer 

173-what is the differnce between application load-balncer and network load-balancer and gateway load-balancer step by step comparision

1-Network-Load-Balancer:-
>>Operates at the transport layer (Layer 4) of the OSI model. It focuses on routing traffic based on IP protocol data.
>>Suited for scenarios where low-latency and high-throughput are essential, such as TCP/UDP-based applications.
>>Primarily works at the transport layer, supporting TCP, UDP, and TLS.
>>Performs health checks at the transport layer, ensuring the health of the underlying instances.
>>Employs algorithms such as Least Outstanding Requests and Round Robin.
>>Offers features like source IP preservation, allowing the backend servers to see the original client IP addresses
>>by default cross-zone load balancer is not-enabled you should enable if the required happens
>>it is suitable for single application or easy application
>>>>it will support path based routing but you need to every time differnt ports or different listners everytime

2-Application Load-Balancer:-
>>Operates at the application layer (Layer 7) of the OSI model. It can make routing decisions based on content, such as HTTP/HTTPS headers.
>>Ideal for applications that require advanced routing, content-based routing, and support for multiple protocols. Commonly used for web applications.
>>Supports application layer protocols such as HTTP, HTTPS, and WebSocket.
>>Supports health checks at the application layer, checking the health of the individual services.
>>Uses advanced algorithms like Least Outstanding Requests, Least Connection Count, and Round Robin.
>>Provides features like SSL termination and Web Application Firewall (WAF) to enhance security.
>>by default cross-zone load-balancer is enabled
>>it is used or suitable for complex applications 
>>it will support path based routing or url based routing

3-Gateway-Load-Balancer:-
>>Primarily designed for managing and distributing traffic at the edge of the network, often used in scenarios involving multiple virtual private clouds (VPCs).
>>Used in edge network scenarios where multiple VPCs need to be interconnected efficiently, making it suitable for hybrid cloud architectures.
>>Provides flexibility to handle various protocols at the edge of the network.
>>Incorporates health checks for the connected VPCs and services.
>>Typically employs algorithms suitable for global load balancing, considering factors like proximity and latency.
>>May include security features relevant to edge network scenarios, but specifics depend on the implementation.

174-what is stickness in apllication load balancer

>>if your enable this add 300 seconds or something if any user connect number of times 5 minutes of 10 minutes after they will connect only that server 
>>it will be useful for banking application. example you connected the server for first for any reason your disconnected the session after 3 minutes  you came for session you connect the first server 
what you connected that server only you can connect after 5 minutes it will work to another server 
>>if your adding three subnets then after created the ALB loadbalancer in then if you want you can add multiple subnets or new subnets 
>>if your adding the public subnets in load balancer it will give one susbnet to one public like this if you given 4 public subnets then you will get 4 public ips to  your load balancer
>>if my client asked the specific ip of your loadbalancer then you cna choose the elastic ip will . you can give that clinet to that ip that client side it person will open to that ip only through that network.
if your not using elastic-ip in loadbalancer tehn public ips will come but it wil change accordingly 
letme simplify:-
anji company is there full secuirty and tight network no option to connect the internet . sampi comapny is there they have load balancer elastic-ip or static ip is there. anji company want to access Sampi
company they are asking specific ip to connecct over the internet or they will open to firewall port or ip 
then sampi company will give the elastic-ip of the load balancer and port then you can add your elastic-ip and port into anji company firewall or security group  

175-If any one asking the my load balancer is in public my subnets is private can use this subnets to connect through internet 

>>yes you can connect through internet by using the load-balancer must be public .like when you create the load-balancer it will create in only public not private
you must put app and db servers in private only. you can connect or download patches using nat gateway

>>you can enable in load balancer to connect the particular ip only if your client asked to connect the your website through ip you can give this ip and 
and add that client ip in your load balancer only that client will connect

176-what is cross-zone load-balancer

>>The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer 
node distributes traffic across the registered targets in all enabled Availability Zones

>>you have load balancer and 1a availbility zone 4 servers and 1b availbilty zone have 2 servers if the traffic will then it will distribute 50 persent to 1a
and 50 persent 1b then it will get over trafficed by 1b so on that time you can use the cross-zone load balancer then it will distribute same amount of traffic to every servers in availbilty zones

177-what is ssl-off loading or termination

>>SSL termination refers to the process of decrypting encrypted traffic before passing it along to a web server.
            or
>>An SSL connection sends encrypted data between an end-user's computer and web server by using a certificate for authentication. SSL termination helps speed the decryption process and reduces 
the processing burden on backend servers

178- what is round-robin load balancer              -->video 17 40mins

>>Round-robin DNS is a technique of load distribution, load balancing, or fault-tolerance provisioning multiple, redundant Internet Protocol service hosts, 
e.g., Web server, FTP servers, by managing the Domain Name System's (DNS) responses to address requests from client computers according to an appropriate 
statistical model        (or)
>>Round robin load balancing is a simple way to distribute client requests across a group of servers. A client request is forwarded to each server in turn.
How does it works:
>>What Is Round-Robin Load Balancing? Round‑robin load balancing is one of the simplest methods for distributing client requests across a group of servers. 
Going down the list of servers in the group, the round‑robin load balancer forwards a client request to each server in turn.

179-what is target group in load balancer

>>collections of machines or servers and also specified protocols and ports to specified instances
         or
>>A target group tells a load balancer where to direct traffic to : EC2 instances, fixed IP addresses; or AWS Lambda functions, amongst others. When creating a 
load balancer, you create one or more listeners and configure listener rules to direct the traffic to one target group

180-what is the Use of TGs.

>>In the context of Elastic Load Balancing (ELB) within AWS, "Target Groups" (TGs) are used in conjunction with Application Load Balancers 
(ALB) and Network Load Balancers (NLB). They play a crucial role in routing traffic to backend instances or services based on rules and health
 checks.
Use Cases:
1-Application Load Balancing (ALB): When you configure an ALB, you create one or more target groups. Each target group is associated with a 
set of instances or IP addresses that serve as targets for the load balancer. ALB routes traffic to different target groups based on rules,
 such as path-based routing.
2-Network Load Balancing (NLB): Similarly, for NLB, you create target groups that define the set of targets for the load balancer. NLB routes
 traffic based on TCP or UDP protocol data.
3-Health Checks: Target groups perform health checks on registered targets, ensuring that traffic is routed only to healthy instances.
4-Dynamic Registration: Instances can dynamically register or deregister with target groups based on Auto Scaling or other criteria.
5-Weighted Routing: You can assign weights to targets within a target group, influencing the distribution of traffic among instances.
6-Traffic Analysis: TGs are used to specify where mirrored traffic should be sent for analysis. This could be an Amazon EC2 instance, 
an Elastic Network Interface (ENI), or an AWS Network Load Balancer.

181-what is the use of health checks in aws

>>Health checks are a way of asking a service on a particular server whether or not it is capable of performing work successfully. 
Load balancers ask each server this question periodically to determine which servers it is safe to direct traffic to.

182-what is the deregistration delay

>>Elastic Load Balancing stops sending requests to targets that are deregistering. By default, Elastic Load Balancing waits 300 seconds before completing the 
deregistration process, which can help in-flight requests to the target to complete
         or
>>Connection draining allows you to specify a maximum time (between 1 and 3,600 seconds and default 300 seconds) to keep the connections alive before reporting the
instance as de-registered. The maximum timeout limit does not apply to connections to unhealthy instances.

183-What is AZ affinity?

>>The AZ Affinity architectural pattern reduces the number of times an AZ boundary is crossed. In the example system we looked at in Figure 1, 
AZ Affinity can be implemented with two changes. First, the ALB is replaced with a Network Load Balancer (NLB).

184-what is a gateway load balancer

>>Gateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. It gives you one gateway for distributing traffic
across multiple virtual appliances while scaling them up or down, based on demand.

185-How have you ensured high availability and fault tolerance for an AWS application?

>>In a previous project, we implemented AWS Elastic Load Balancers and Auto Scaling Groups to ensure that our application could handle sudden spikes in traffic and maintain high availability.
We also set up Amazon CloudWatch to monitor the performance of the application and alert us in case of any failures, allowing us to quickly respond and ensure fault tolerance.

186-can I add multiple target groups in a load balancer?

>>yes you can add multiple target groups. for example, you created two servers server-1, and server-2 your created server-1 in the tg-1 target group and then created tg-2 target add server-2 then
create the application load balancer and the listeners tg-1 target group it will add 100% traffic to server-1. you can goto the load-balancer in listeners go to add rules edit 
load-balancer then add server-2 or target group and add 50% of traffic to server-1 and server-2 this will highly available

187-what are the load-balancing algorithms in load-balancers

1-Round Robin: In Round Robin load balancing, each new connection or request is directed to the next server in a circular order. It evenly distributes traffic among all available servers. 
It is simple and can be effective, but it doesn't take into account server load or response times.
2-Least Connections: The Least Connections load balancing method routes traffic to the server with the fewest active connections. This helps in distributing traffic based on server load and ensures
that servers are not overwhelmed.
3-Least Response Time: This method directs traffic to the server with the lowest response time. It is based on the idea that faster-responding servers should receive more traffic. 
This method can be effective in minimizing response time for end users. 
4-IP Hash:IP Hash load balancing uses a hash function to map the client's IP address to a specific server. This ensures that a specific client always connects to the same server, which can be useful
for session persistence or stateful applications. 
5-Weighted Round Robin: Weighted Round Robin allows you to assign different weights to servers. Servers with higher weights receive more traffic. This is useful when servers have 
different capacities or performance levels. 
6-Weighted Least Connections: Similar to Weighted Round Robin, Weighted Least Connections allows you to assign weights to servers but routes traffic to the server with the fewest connections,
taking server load into account. 
7-Session Persistence (Sticky Sessions): Sticky sessions, also known as session persistence, ensure that a user's requests are always directed to the same server for the duration of their session.
This is crucial for stateful applications that rely on a specific server for session data
8-Content-Based Load Balancing: Content-based load balancing examines the content or characteristics of incoming requests to make routing decisions. For example, it can route traffic based on 
URL paths or specific HTTP headers.
9-Geographic Load Balancing: Geographic load balancing directs traffic based on the geographical location of the client, which can be useful for serving content from servers closer to the user,
reducing latency, and complying with data sovereignty regulations.
10-Dynamic Load Balancing: Dynamic load balancing adapts to changing server conditions by continually monitoring server health and traffic conditions. It can automatically adjust the 
routing decisions to ensure optimal performance and high availability.

188-what is instance 

>>instance is nothing but server. "instance" typically refers to a virtual machine (VM) that you can run in the cloud. These instances are a fundamental part of AWS's Elastic Compute Cloud (EC2) service,
which provides scalable computing capacity in the cloud.
>>is is virtual machine 

189-what is bare images

>>bare images is nothing but it will not come with any software its base image 

190-what is the difference between reload and restart

Reload:-Reloading typically means reloading a specific component or configuration within a system without stopping the entire system.
Example: In the context of web servers, a reload might involve reloading the configuration files or restarting a specific module or application without stopping the entire server.
Restart:-Restarting usually involves stopping the entire system or application and then starting it again. This action results in a fresh initialization of all components.
Example: Restarting a computer, server, or service involves shutting it down completely and then starting it anew. This ensures that all processes and configurations are reloaded from scratch.

191-what is mutable and immutable

>>if use mutable always take longtime to deploy change the version or OS Anything 
>>if use immutable always take lesstime to deploy change the version or OS or Anything 

192-what is instance refresh in auto-scaling 

>>Instance refresh in auto-scaling refers to the process of replacing instances within an auto-scaling group with new instances running the latest version of the application or operating system.
This process helps ensure that the auto-scaling group maintains a healthy and up-to-date fleet of instances.
>>video 71 26mins 

193-how to protect a loadbalancer

>>by default there is option delete protection we can use that enable then no one is not able to delete 
>>and also you can enable access logs that will help you to montior who is accessing any load balancer or if your using cloud trail you can  that where also 

194-How to upgrade or downgrade a system with zero downtime?

>>To upgrade or downgrade a system with zero downtime, you can use the following steps:
1-Set up a load balancer in front of your system.
2-Deploy the new version of your system to a new set of servers.
3-Add the new servers to the load balancer and verify that they are healthy. (here your ading new servers using target group to blue-green deployment sharing the network traffic 10% then 20% like this )
4-Remove the old servers from the load balancer.
5-Monitor the system to ensure that everything is running smoothly.
By following this process, you can ensure that your system remains available throughout the upgrade or downgrade process.

195-How do you upgrade or downgrade a system with near-zero downtime?

1-Auto Scaling Groups: Use Auto Scaling Groups to manage the desired capacity of your instances. When upgrading or downgrading, you can change the launch configuration or the Amazon Machine Image (AMI) 
associated with the Auto Scaling Group. Auto Scaling Groups automatically replace instances to maintain the desired capacity, minimizing downtime.
2-Blue-Green Deployments: Implement a blue-green deployment strategy, where you maintain two identical environments (blue and green). You direct the traffic to the active environment (blue), 
and when you're ready to upgrade, you switch traffic to the new environment (green). This approach allows for a smooth transition with minimal downtime.
3-Elastic Load Balancers (ELB): Use Elastic Load Balancers to distribute traffic across multiple instances. When upgrading, you can take instances out of service, perform the necessary upgrades, 
and reintroduce them to the ELB. This ensures continuous traffic flow while updating the system.
4-Amazon RDS Read Replicas: If your system involves a database, consider using Amazon RDS Read Replicas. During upgrades, you can promote a read replica to the primary instance with minimal downtime.
This helps maintain database availability during the upgrade process.
5-Amazon EC2 Instance Termination Protection: When upgrading instances in an Auto Scaling Group, enable termination protection for instances you are about to replace. 
This prevents accidental termination of instances during the upgrade process.
6-Amazon Route 53 DNS Failover: If you are implementing a blue-green deployment, use Amazon Route 53 DNS failover to switch traffic between the blue and green environments. 
This allows you to control the traffic flow during the upgrade or downgrade process.
7-AWS Lambda: Use AWS Lambda for serverless computing. If your application architecture includes serverless components, updating Lambda functions can be done without downtime as each function execution
is stateless.
8-Snapshot and AMI Backup: Before making changes, take snapshots of EBS volumes and create AMIs of instances. This provides a backup in case something goes wrong during the upgrade or downgrade process.

>>You can upgrade or downgrade a system with near-zero downtime using the following steps of migration:

Open EC2 console
Choose Operating System AMI
Launch an instance with the new instance type
Install all the updates
Install applications
Test the instance to see if it’s working
If working, deploy the new instance and replace the older instance
Once it’s deployed, you can upgrade or downgrade the system with near-zero downtime.

196-you have a VPC with a public subnet and private subnet. Instances in the private subnet need to access the internet for software updates. How would you allow internet access for
instances in the private subnet?

>>To allow internet access for instances in the private subnet, we can use an Internet gateway or a NAT instance
>> We would place the NAT Gateway/instance in the public subnet and configure the private subnet route table to send outbound traffic to the NAT Gateway/instance. This way instances 
in the private subnet can access the internet through the NAT Gateway/instance.

197-you have launched EC2 instances in your VPC, and you want them to communicate with each other using private IP addresses. What steps would you take to enable this communication?

>>By default, instances within the same VPC can communicate with each other using private IP addresses. To ensure this communication, we need to make sure that the instances are launched in the same VPC 
and are placed in the same subnet or subnets that are connected through a peering connection or a VPC peering link. 
Additionally, we should check the security groups associated with the instances to ensure that the necessary inbound and outbound rules are configured to allow communication between them.

198-your organization has a VPC with multiple subnets you want to restrict outbound internet access for resources in one subnet but allow outbound internet access for resources in 
another subnet how would achieve this?

>>To restrict outbound internet access for resources in one subnet we can modify the route table associated with that subnet in the route table we can remove the default route (0.0.0.0/0)
that points to an internet gateway
>>This would prevent resources in that subnet from accessing the internet for the subnet where outbound internet access is required we can keep the default route pointing to the Internet gateway

199-you have been assigned to design a VPC architecture for a 2-tier application. the application needs to be highly available and scalable how would you design the VPC architecture

>>In this scenario, I would design a VPC architecture in the following way.

I would create 2 subnets public and private the public subnet would contain the load balancer and be accessible from the internet. the private subnet would host the application servers.
I would distribute the subnets across multiple Availability Zones for high availability. Additionally, I would configure auto-scaling groups for the application servers.
1-disatrey recovery strage 
2-multi availbilty zone 
3-multi region
4-blue-green deployment 

200-Your organization requires an isolated environment within the VPC for running sensitive workloads. How would you set up this isolated environment?

>>To set up an isolated environment within the VPC, we can create a subnet with no internet gateway attached. 
>>This subnet, known as an "isolated subnet," will not have direct internet connectivity. We can place the sensitive workloads in this subnet, ensuring that they are protected from 
inbound and outbound internet traffic. 
>>However, if these workloads require outbound internet access, we can set up a NAT Gateway or NAT instance in a different subnet and configure the isolated subnet's route table to
send outbound traffic through the NAT Gateway/instance.

201-You want to implement strict network access control for your VPC resources. How would you achieve this?

>>To implement granular network access control for VPC resources, we can use Network Access Control Lists (ACLs). NACLs are stateless and operate at the subnet level. We can define inbound and outbound 
rules in the NACLs to allow or deny traffic based on source and destination IP addresses, ports, and protocols. By carefully configuring NACL rules, we can enforce fine-grained access control for 
traffic entering and leaving the subnets.

202-You have a private subnet in your VPC that contains a number of instances that should not have direct internet access. However, you still need to be able to securely access these 
instances for administrative purposes. How would you set up a bastion host to facilitate this access?

>>To securely access the instances in the private subnet, you can set up a bastion host (also known as a jump host or jump box). The bastion host acts as a secure entry point to your 
private subnet. Here's how you can set up a bastion host: Create a new EC2 instance in a public subnet, which will serve as the bastion host. Ensure that this instance has a public IP 
address or is associated with an Elastic IP address for persistent access. Configure the security group for the bastion host to allow inbound SSH (or RDP for Windows) traffic from your
IP address or a restricted range of trusted IP addresses. This limits access to the bastion host to authorized administrators only. Place the instances in the private subnet and configure
their security groups to allow inbound SSH (or RDP) traffic from the bastion host security group.SSH (or RDP) into the bastion host using your private key or password. From the bastion host,
you can then SSH (or RDP) into the instances in the private subnet using their private IP addresses.

203-How have you ensured high availability and fault tolerance for an AWS application?

>>In a previous project, we implemented AWS Elastic Load Balancers and Auto Scaling Groups to ensure that our application could handle sudden spikes in traffic and maintain high availability.
We also set up Amazon CloudWatch to monitor the performance of the application and alert us in case of any failures, allowing us to quickly respond and ensure fault tolerance.

204-Can you explain a time when you had to troubleshoot an issue with an AWS application?

>>In my previous role, we had an issue with an EC2 instance not responding to incoming traffic. After reviewing the logs, we discovered that the security group rules were misconfigured,
causing the instance to reject incoming traffic. We adjusted the security group rules to allow incoming traffic, and the issue was resolved.

205-Explain the difference between elasticity from scalability.

>>Elasticity: The ability of a system to handle an increase in the workload by simply adding hardware resources when demand rises, and rolling back scaled resources when there’s no demand.

>>Scalability: the ability of a system to increase the hardware resources for handling an increase in demand. It can be achieved by either increasing the hardware specs or increasing the number of
processing nodes.

206-what is latency 

>> Latency is nothing but the time taken by request to reach the application and for the application and the server to send the response back to you
If there is a delay that is high latency
if there is no delay that is low latency

207-how to do disaster recovery in aws of my application

1-Assessment and Planning:
>>Identify critical components of your application, including databases, storage, compute resources, and networking.
>>Define your Recovery Time Objective (RTO) and Recovery Point Objective (RPO) to determine how quickly you need to recover and how much data loss is acceptable.
2-Backup and Snapshots:
>>Regularly back up your data and create snapshots of your volumes. AWS offers services like Amazon RDS for databases and Amazon EBS for block storage that provide automated backup solutions.
>>Utilize Amazon S3 for object storage and consider versioning to maintain multiple versions of your data.
3-Multi-AZ Deployments:
>>For critical services, deploy your application in multiple Availability Zones (AZs) within a region. This ensures high availability and fault tolerance.
>>Use load balancing to distribute traffic across multiple instances in different AZs.
4-Cross-Region Replication:
>>Replicate critical data and resources to a different AWS region. This helps in maintaining availability in case an entire region goes down.
>>AWS services like Amazon S3, RDS, and DynamoDB provide cross-region replication features.
5-Automated Infrastructure as Code (IaC):
>>Use tools like AWS CloudFormation or Terraform to define your infrastructure as code. This allows you to recreate your entire environment quickly in case of a disaster.
6-Disaster Recovery Testing:
>>Regularly test your disaster recovery procedures to ensure they work as expected.
>>Simulate various disaster scenarios, such as hardware failures, data corruption, or entire region outages.
7-Monitoring and Alerts:
>>Set up monitoring and alerting for your application using AWS CloudWatch or third-party tools. Monitor key metrics and set up alarms to be notified of any issues.
>>Utilize AWS Config to track changes to your AWS resources.
8-Documentation:
>>Maintain detailed documentation of your disaster recovery plan, including step-by-step procedures, contact information, and any dependencies.
9-Third-Party Solutions:
>>Consider using third-party disaster recovery solutions available in the AWS Marketplace for additional features and customization.
10-Collaboration with AWS Support:
Engage with AWS Support for guidance and assistance in creating and validating your disaster recovery plan.

208-highly availble Architecture in AWS or any Cloud 

>>Create a VPC 
>>then create multiple subnets with multiple avaailbilty zones 
>>then create the load balancer 
>>then create the auto-scaling 
>>then create route53 with failover for (disaster recovery)
>>and take backup of each and every snapshot to store s3(disaster recovery)

High-Level Architecture:
Incoming traffic hits Route 53, which directs it to the ALB.
ALB distributes traffic across instances in multiple AZs.
Auto Scaling ensures that the desired capacity is maintained.
Instances store static assets in S3, and shared files may be stored in EFS.
The database (if applicable) is configured for high availability and is preferably managed by Amazon RDS.

209-what is Route53 and what is the uses

>>Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web servic. Rout53 using we can host the domians.Route53 is global. 
route53 using you can do one region to another region load balancing.route53 using you can do multiple region load-balancing.  
>>Route53 provides DNS as a Service DNS->stands for Domain name service
>>DNS service is the one that maps your domain name with ip address or maps dns is that resolves your domain to the ip address if you assign the domain to the load-balancer
its resolve the domian name to the load-balancer
>>DNS keeps a lot of records
>>we use mostly routing policys simple, latency, falover, weighted

1-failover :- if we create two load balancer in differnt regions and create the helath checks in route 53 if we selectd the failover routing policy then we can 
create the same name of the website like www.anji.com is primary in mumbai region and www.anji.com is secondary in us-east-1 .whenever primary fails 
it will go secondary directly without checking the pings

2-weighted :- if we create weighted route policy route 53 it is like failover but the thing is you can mention the traffic which load balancer need to 
accessing the apllication over internet if we give 0 menas you can't access the application on that load balancer if we give 255 then we can access the 
loab balancer. the use case is we want maintaines on particular region you can diver the traffic into sepearte the specific load balancer
			(or)
Weighted round-robin:-two regions servers are the then we can divide the traffic based on priority based we can divide the traffic 
>>Weighted routing allows you to associate multiple resources with a single domain name (example.com) or subdomain (acme.example.com) and choose the amount of traffic routed for each resource.
This can be useful for a variety of purposes, including load balancing and testing new versions of software.

3-geo-DNS:-Geolocation routing can be used to send traffic to resources based on the geographical locations for example you may have presence in europe and Asia now you want user in the
Asia to be served in the Asia and rhose europe to be served by servers in europe
ex:-you can localize yourcontent present some or all of your website in the language of your user

4-latency:- latency is used same is the previous failover or weighted routing policy the differnt is you can mention to the particular in region if any one 
near by india that user will connect to the india based load balancer and if any us then that user will connect the us load balancer its region based. 
it is used to decrease the latency in your application to the users

210-Route 53 records.

1-A (Address) Record:
2-AAAA (IPv6 Address) Record:
3-CNAME (Canonical Name) Record
4-MX (Mail Exchange) Record:
5-TXT (Text) Record:
6-NS (Name Server) Record:
7-SRV (Service) Record:
8-SOA (Start of Authority) Record:
9-CAA (Certification Authority Authorization) Record
10-Alias Records:

211-use cases of records

1-Domain Registration:
2-DNS Hosting
3-Load-balancing
4-Failover and helath checks 
5-Global Server Load Balancing (GSLB)
6-Routing policies
7-Alias Records
8-Subdomain management
9-Traffic Flow
10-Query Logging:Route 53 provides query logging, allowing you to capture and analyze DNS query logs. This is helpful for monitoring and troubleshooting DNS-related issues

212-what is traffic policies in route53

>>it will give the graphical representaion of dns records

213-What is the difference between AWS latency-based routing and geo DNS

>>Amazon maps-out typical latency between IP addresses and AWS regions. Choose Latency-based Routing to have the fastest response. Geolocation maps the IP addresses to geographic locations. 
This permits rules like "send all users from Côte d'Ivoire to the website in France", so they see a language-specific version

214-What is dns how it works

>DNS used to identify computers, services, and other resources reachable through the Internet or other Internet Protocol
>>dns transfers human readble domain names for (example www.amazon.com)to machine  readble to ip address for example (192.168.0.24)
>>in your real time your isp will give dns address
>>most orgnazation disable ping to stop the ddos attacks by hackers
>>rembering name is easy but number are diffcult that we using dns names

215-what is hostname resolution 

>>HostName resolution is the process of associating names and IP addresses, and it's one of the most essential services on a network. 
People understand descriptive names, but network communications require difficult-to-remember addresses
>>hostname resolution is nothing but converting name into ip address . it is called full qualfied domain name (FQDN)

216-how to create ssl-certifictae in aws

>>go to aws certificate manager create your domain name anji.com it will ask the email or domain validation then go to domain it will give name and cname
and create the record set in your route53 and create a record set the giev name and select the cname and it will give you certificate then go to your load balancer 
and add listener select the port and certficate then you good to go then delete the 80 port number that it . it take 10 to 15 minutes and to avoid the security instrucations
								(or)
>>you can create the certifictaes using third-party also using csr generator it will ask your domain name and give you public and private key then go to comodsslstore then you can create the certificate
								(or)
>>digcert, entrust provide ssl certficate
>>you need to create private.key and .csr file in your system and give that keys into authority digcert entrust they will give 3 files certification.crt and private.key and ca_bundle.crt
then you configure this certifictaes to webserver

217-what is ACM and SSL

1-ACM(AWS Certificate MAnager) :-ACM is a service provided by Amazon Web Services (AWS) that simplifies the process of provisioning, managing, 
and deploying SSL/TLS 
(Secure Socket Layer/Transport Layer Security) certificates for use with AWS services and your own applications.
Key Features:
>>Securing websites
>>aws certificate manager is free it will work only in AWS only  
>>Securing communication between clients and AWS services
>>ACM provides free SSL/TLS certificates for use with AWS resources like Elastic Load Balancers (ELBs), Amazon CloudFront, and API Gateway.
>>Certificates issued by ACM are automatically renewed and can be easily integrated with supported AWS services.
>>ACM supports both RSA and Elliptic Curve Cryptography (ECC) algorithms for key pairs.
>>ACM also provides a management console for easy certificate management and monitoring.
2-SSL(Secure Socket Layer):-is a cryptographic protocol that provides secure communication over a computer network. It is the successor to 
TLS (Transport Layer Security), which is now the more commonly 
used protocol. SSL/TLS works by establishing a secure connection between a client (such as a web browser) and a server (such as a website).
 This connection is encrypted, which means that only the client
and server can read the data that is exchanged between them.
Key Features:
>>Encryption: SSL/TLS encrypts data to ensure that sensitive information, such as login credentials or credit card details, is transmitted 
securely.
>>Authentication: SSL/TLS provides a mechanism for authenticating the server to the client, assuring the client that it is communicating with
 the intended server.
>>Data Integrity: SSL/TLS ensures the integrity of transmitted data, preventing it from being tampered with during transit.
>>Widely Adopted: SSL has been succeeded by TLS, and both are widely adopted standards for securing online communication.
>>Securing websites and web applications to protect user data.
>>Encrypting data transmitted over email (SMTP/IMAP) or other network protocols.

>>Securing communication between servers and APIs.

217-what is Subdomain

>>A subdomain is a prefix added to a domain name to separate a section of your website. Site owners primarily use subdomains to manage extensive sections that
require their own content hierarchy, such as online stores, blogs or support platforms. Subdomains function as a separate website from its domain

218-what is mx record 

>>An MX record specifies the names of your mail servers and, if you have two or more mail servers, the priority order. Each value for an MX record contains
two values, priority and domain name. An integer that represents the priority for an email server.

219-what is simple record in aws

>>Simple routing lets you configure standard DNS records, with no special Route 53 routing such as weighted or latency. With simple routing, you typically 
route traffic to a single resource, for example, to a web server for your website. You can use simple routing policy for records in a private hosted zone.

220-What is C-Name Record 

>>cname is also called canincal name or alias 
>>CNAME records are Directs one name to another name or Points domain to Another Domain www.anji.com to sampi.anji.com
>>so when you move your servers you only have to change one set of records

221-what is A name record in aws

>>A known as Host records it converts host to ip address 
>>A record is called host record also 
>>A Records are IPv4
>>AAAA Records are IPv6
>>Amazon Route 53 offers a special type of record called an 'Alias' record that lets you map your zone apex (example.com) DNS name to the DNS name for your
ELB load balancer (such as my-loadbalancer-1234567890.us-west-2.elb.amazonaws.com).

222-what is the differnce between A and CNAME record

>>the difference between CNAME and A record in a traditional NameServer is that only CNAME can be used to point to a different subdomain and A record must 
point to a valid IP address. Since the AWS Route53 allows you to do use a subdomain with either A Rec or CNAME

223-what is NS record

>>>>it is name server records when you creating or hosting website it will create the name server reocrds that will be fault tolerance purpose if any one goes down your will not impact why 
because reaminig will run right
>>The NS records and A records are both used to provide DNS services. The difference between them is that the former define the servers that provide
DNS services for a domain name while the A records translate hostnames (such as www, FTP, mail) into IP addresses.

224-Is it possible to create one webserver under we can create 3 domains

>>yes its possible you can 3 folders like anji.com sampi.com vinod.com under deploy your code each domain code route the traffic based on DNS 
then you will access one-webserver using access 3 differnet domains in one ec2 system

225-what are the main reasons to server went down

>>ddos attacks
software issues
developer are human errors
check the memory and cpu utilization
check the network issues like pinging or not

>>prevention
>>implment firewalls and cybersecurity 
>>montior the requests 
>>check the cpu utilization and network and memoty 
>>add load balancer and auto-scaling groups on-demand

226-how to check the bucket size in aws

>>you can select the bucket and click on actions and see the bucket size

227-how to restrict specific user in specific region or specific ip range

>>you can create the policy and attach the policy to the user then he can access the only that ip range or that region only user IAM roles

228-it is poosible to chnage dns name in linux

>>nano /etc/resolv.conf   --->change DNS name server in linux

229-What are the tools can i use to do multiple region load balancing

>>Route 53 and global accelartor or global load balancer . this will give you fixed entrypoints 
>>You created load balancer us-east-1 and also us-east-2. Then you route 53 under helath-check 1-us-east-1 2-us-east-2 .Then you created the anji.com selected weighted value 1 selected alb us-esat-1 
selected helath check also same us-east-2 but weighted value 255
The traffic goes us-east-2 if the us-east-2 fails tgen it will go to us-east-1 by using helath checks if you given 0 means it will not send traffic to us-eadt-1 minimum 1 must

>>route53 is dns level load balancing 
>>if you use global accelartor it will give fixed entry point. you can do multiple region load balancing. it will be extra compared route53   

230-what are the instance status check failure will happen

>>exhausted memory 
>>corrupted file system 
>>incompatible kernel 
>>incorrect networking or startup configuration
>>you can check the instance status check in select instance click on actions and click on montior and troubleshoot see get systemlog and you can see boot level or kernel level issues 
>>also you can check reboot and replace the root volume it will also solve instance status check  

231-what is NSlookup

>Nslookup is the name of a program that lets users enter a host name and find out the corresponding IP address or domain name system (DNS) record. Users can also enter a command in nslookup to do a 
reverse DNS lookup and find the host name for a specified IP address.

232-what is bandwidth 

>>it is capacity 

235-what is Throughput

>>the total amount of data that travlled through network or internet 

236-how many certificate limits in aws 

>>in a year we can request 20 certificates in a year if you want you can add enbale limit extra quota 
>>20 certificate with region if you exclude that limit enable limit or change region in testing not production

237-On an EC2 instance, an application of yours is active. Once the CPU usage on your instance hits 80%, you must reduce the load on it.What strategy do you use to complete the task?

>>It can be accomplished by setting up an autoscaling group to deploy additional instances, when an EC2 instance's CPU use surpasses 80% and by
allocating traffic across instances via the creation of an application load balancer and the designation of EC2 instances as target instances.

238-what is ssh

>>the ssh protocal (also referred to as secure shell) is a method for secure remote login from one computer to another computer.it provides the secure connection
to network. it provides several alternative options for strong authntication, and it protects the communications security and integrity with strong encryption

239-what is Baston Host

>>Bastion Hosts are a special-purpose instanxe that provides seccure access to linux instacnes located in private subnets
>>Act as a jump server to access other instances in private subnets
>>protect instances in private subnets from potential attacks 

240-How Bastion Host work

>>Ec2 instances in private subnets are protected by security group and NACLs, only allow secure connection from Bastion Host

>>Security Group of Bastion Host only allow SSH from trusted IP Address
>>use ssh agent forwarding on a client do not store private key on bastion host

241-without password how can access linux machines

>>you need to create the sshkey it is uses in rsa alogirthum it will create 2 files public key file named . ssh/id_rsa. pub and a private key file named . ssh/id_rsa.
you can add that public key into the remote server you must enable the password less authentication in remote servers
>>vi /etc/ssh/sshd_config  
passwordauthentication yes
permitrootlogin yes
#passwordauthentication no  -->you need comment this 
>>scp -r /home/anji.tx  2.54.25.2:/root/anji    -->it will send the file  
two ways to send the file password less like add your ssh keys in destination server or password authentication need to give password

>>ssh-keygen
>>press enter
>>press enter
>>ls -a
>>cd .ssh
>>ls
>>ssh-copy-id anji@172.31.38.167   -->it will ask passwd give next time it will not ask do same in remaing systems

242-how you can see if the port is opened or not in linux

>>>>netstat -a | grep -i 8000     -->to check the port opening and listening
>>telnet "ip" port

243-What are the kinds of security attacks that can occur on the cloud? And how can we minimize them?

>>The kinds of security attacks that can occur on the cloud include DDoS attacks, phishing attacks, data breaches, and insider attacks. To minimize them, it is important to use
strong authentication methods, implement proper access control and monitoring, encrypt data, keep systems up to date with security patches, and regularly conduct security audits and training for employees.

244-how to port forwarding in linux or ec2

>>public ec2 and private ec2 you can login one public ec2 instnace using ssh then go to the vi /etc/multifortforwarding/multiforwad.config 
then add the private ec2 instance private ip then reboot using sudo reboot the you can login using ssh -i -/ssh/anji.pem anji@12.24.45.25 -p 2222

245-what ssh key types

1-RSA
2-DSA 
3-ECDSA.

246-what is AWS KMS Policys

>>AWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control the cryptographic keys that are used to protect your data
two types of keys 
1-symatric key : same key is used encrypt and decrypt 
2-asymatric ky : differnt keys used to encrypt and decrypt
>>you can assign the key is and attach that key to one user and then create the s3 bucket then attach the kms policy to one user then the user will get read or write 
permission no can get the write the permission to that bucket

>>Use AWS KMS to encrypt data across your AWS workloads, digitally sign data, encrypt within your applications using AWS Encryption SDK, and generate and verify
message authentication codes (MACs).

247-how many types of kms keys 

1-amazon managed keys   -->not possibe to delete
2-customer managed keys -->you can delete by createing retention period only you cant delete directly
>>you can disbale key instead of delete . for example why aws given retention period to delete that key . you created bucket and upload that add this key but you not disabled that bucket encryption you delete 
that keys means that bucket under data will be encrypeted not able to decrypt why you deleetd that key so becareful while deleteing kms keys . aws also not help in this case  

248-what is best practice for download key pair

>>If you download pem directly you can create ppk if you lost pem you can't create the pem Or ppk if you download pem you can create ppk if you lost pem you can't create one more time using ppk
>>Always try to download to pem  not ppk if you download pem you can create the ppk you can store pem somewhere for backup

Inlinux 
>>puttygen ppkkey.ppk -O private-openssh -o pemkey.pem
>>puttygen pemKey.pem -o ppkKey.ppk -O private
>>Pem is for linux purpose. ppk is for putty or windows

249-what is called in AWS everything

>>in AWS Everything is called as Resource

250-what is shared storage and distributed storage

Shared-Stoarge:-Shared storage consolidates files and other information in a central resource that can be accessed or shared among multiple users and data systems simultaneously. Gathering and storing
data in a single, shared resource streamlines archive and backup processes, and simplifies information security. Shared storage solutions include network attached storage (NAS), 
storage area networks (SAN), and redundant arrays of independent disks (RAID), as well as dedicated storage servers and cloud storage.
Distributed Storage:-Distributed block storage is a computer data storage architecture which the data is stored in volumes across multiple physical servers, as opposed to other storage architectures
like file systems which manage data as a file hierarchy, and object storage which manages data as objects

251-can i add the one instance in one load-balancer using anji target group created another target group using same instance is it possible to create that instance in one more load-balancer

>>no its possible if you created one instance in using load-blancer using the target group.then its not possible use one more load-balancer 

252-why the load-balancer and nat-gateway in public subnets

>>so when we create the load-balacner we must select the public subnets only in load-balancer or nat-gateway then we add the route into private route table and select the target groups under private instance    

253-can i create the target group under like not instances in that place is IPs and another load-balancer or lambda function

>>yes you can create rather than use instance you can use IPS and load-balancer and lambda functions 

254-how to login username and password to linux ec2 instance 

>>create ec2 instance connect to the instance using ssh keys then create root password
>>ssh -i /path/to/your/key.pem ec2-user@your-instance-public-ip
>>sudo passwd root
>>sudo nano /etc/ssh/sshd_config
>>PermitRootLogin yes
>>sudo service ssh restart
>>ssh root@your-instance-public-ip

255-what is IAM

>>AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. You use IAM to control who is authenticated 
(signed in) and authorized (has permissions) to use resources.

256-How many IAM roles can be attached to an EC2 instance at a time?\

>> one iam role will attahc to ec2 instance 

257-iam briefly 

1-Groups  --|
2-Users   --| admins and developers and users can also take or use  access_key and secret_key 

roles: for aws service (not for admins) ex: ec2 to access aws s3 can access . roles will also use in users but differnet format
>>create a role anji attach permission or policy then atatch to ec2-instance . roles also use access_key and secret_key also use create one token by default. the token will 
expire after 6 hours or 7 hours then it will create one more token  
>.one service can use another service that we use roles like aws ec3 need to access s3 then we can use role 
polices: permission in json format create a role and attach the polices to role. and you can attach polices to users and roles also 

i have two users testuser1 and testuser2 and given tags onwer=testuser1 for testserver1 and onwer=testuser2 for testserver2 and then 
i need to create a policy for testserver1 only access testuser1 like this testserver2 will access only testuser2 and start and stop only  

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "ec2:Describe*",
      "Resource": "*"      ##this three will be which service he can see then after lines permissions 
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:StartInstances",
		    "ec2:RunInstances",
        "ec2:StopInstances",
        "ec2:RebootInstances"
      ],
      "Resource": [
        "arn:aws:ec2:us-east-1:111122223333:instance/*"
      ],
      "Condition": {
        "StringEquals": {
          "ec2:ResourceTag/Owner": "testuser1"
        }
      }
    }
  ]
}
---
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "ec2:Describe*",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ec2:StartInstances",
        "ec2:StopInstances",
		    "ec2:RunInstances",
        "ec2:RebootInstances"
      ],
      "Resource": [
        "arn:aws:ec2:us-east-1:111122223333:instance/*"
      ],
      "Condition": {
        "StringEquals": {
          "ec2:ResourceTag/Owner": "testuser2"
        }
      }
    }
  ]
}

258-if you have 100 files in bucket instead of giving 1 object to 1 to public how is possible because its headek tds task rigt 

>>you can create bucket policy then give permission to all objects its will be public 

{
  "Version": "2012-10-17",
  "Statement": {
    {
      "sid": "Statement1",
      "Principal": "*",
      "Effect": "Allow",
      "Action": "s3",
      "Resource": ["arn:aws:s3:::anji159258","arn:aws:s3:::anji159258/*"]

    }
  }
}

259-i want to give anji user to create and delete only vpc  craete policy for vpc in aws

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ec2:CreateVpc",
        "ec2:DeleteVpc",
        "ec2:DescribeVpcs",
        "ec2:DescribeTags"
      ],
      "Resource": "*"
    }
  ]
}

260-what are the types IAM policies    -->see in iam.txt read inline policy

>>AWS managed policies:- aws managed by this policies, you can reuse the policies
>>customer managed policies:- you can create and attach the policies to user and groups like giving permissions and denying users or groups. you can reuse the polices
>>Inline policies:- its like same customer managed policies but its one to one like one user or role you cant reuse the policies it is not visible for everyone 
it is visible for only user and it is embedded directly into a single user, group, or role.
>>AWS Managed Job Function:- this will be created by aws by default for job role purpose 

261-what is IAM policy simulator?

>>Iam policy simulator is used to see the policy briefly what you attached to the users for permisiions what you given

262-what is the differnce between customer managed policy and inline policy

>>A customer managed policy is a standalone policy that you administer in your own AWS account. An inline policy is a policy that's embedded in an IAM identity (a user, group, or role).

263-What is IAM role.

>>AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. You use IAM to control 
who is authenticated  (signed in) and authorized (has permissions) to use resources.

264-where we can use roles

>>role is used manage the Accounts
>>AWS Identity and Access Management (IAM) roles are entities you create and assign specific permissions to that allow trusted identities such as workforce identities and applications
to perform actions in AWS. When your trusted identities assume IAM roles, they are granted only the permissions scoped by those IAM roles.role is not a user
>>roles is also use access-key and secret-key but will not show its automatically create that and also create the token in your system but it will be expiry in 6-8 hours then 
it will create the one more token. if you want use that credentials in without token it will not work

265-what is the difference between IAM and roles

>>The difference between IAM roles and policies in AWS is that a role is a type of IAM identity that can be authenticated and authorized to utilize an AWS resource, 
whereas a policy defines the permissions of the IAM identity.
>>A Role with no Policy attached to it won’t have to access any AWS resources.
>>A Policy that is not attached to an IAM role is effectively unused. Permissions listed in an IAM policy are only enforced when that policy is attached to an IAM identity.

266-what is Assume role

>>Assume role is nothing your giving temporary or prmenaent access another aws account user or group level access to another account.the user can access what permission your
giving that will be accessbile that user anytime briefly in below see

first-type
>>anji aws account go to roles create a role anji-role select the sampi account 11223344 he can manage the ec2 click on create. go to sampi aws account create a policy sts-assume role policy then 
paste the anji-role arn here assume polciy click on name sampi-policy  click on create . you have sampi user in sampi account and attach the sampi-policy to sampi user .
then go to sampi user console and switch role give anji account name and anji-role that's it your into the anji aws account. if you have vinod and gampa account also you create role and sampi wil assume 
the role gampa and vinod give the rule of arn then sampi will assume role then he can go to the sampi account switch to vinod and gampa like this you can do multiple accounts 
 
second-type
>>your in Anji account createing role in as anji1 role using another AWS account enter sampi AWS account number 11256 and add persmission like ec2 or s3 click on create.copy your ARN 
go to sampi account have vinod user go to policies create STS policy in that paste anji1 role ARN and create that policy. attach that policy into vinod user and login with vinod user
you can see option in support that see switch to role paste the anji Account number and role anji1 you will be able to access anji1 role what the permission you have

267-what is role-switching

>>When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. 
When you exit the role, your user permissions are automatically restored

>>your createing one role giving this access to another accounts then that account can access the your given resources this called role-switching

268-what is Service control policys

>>Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available 
permissions for all accounts in your organization.

269-how you can manage the 20 accounts as acccounts

>>we can use orgnzation create the some group to add and divide specific groups to aws accounts
                              (or)
>>if we have 20 accounts we use master and sub-account architecture we use. master is there we use organzations, properly use organzation units we kept as internal
external and third party we are properly segrgated. then we can apply security control policy ,we can restrict users using permisiion boundary

270-how to secure your system or aws accounts how to see logs 

>>using cloud trail feature enable it send the logs into s3 bucket

271-if your getting any error message in console how would you check that in cli using decode 

>>aws sts decode-authorization-message --encoded-message "paste the error message that you get in console"

272-how to check the user permissions to user 

>>you can use policy similator using you can select the user then run you will see the permissions and everything 
>>https://policysim.aws.amazon.com 
>>https://awspolicygen.s3.amazonaws.com/policygen.html

aws ec2-describe | jq  -->like this you can use 
aws ec2-describe-instance | grep -i "instanceid"

273-if you given allow ec2 start policy to user then after you given deny how it works 

>>if you given allow ec2 start policy to user then after you given deny ec2 start also same policy if you given that it will treat deny by default deny is powerful comes to allow comapred to in AWS 
>>deny is allow is more priority 

274-what is Access analyzer in IAM 

>>if your using access analyzer it will show the account deatils from outside account who are accessing the accounts of you 

275-what is permission boundry in IAM users under 

>>your given user to ec2 access and see the instance only. then after your releised that you mistkenlu given admin access. you can set the permission boundary under select the ec2 full access that user 
will admin permission that set only ec2-only either admin any policy . he can access only ec2 service your limiting the user in boundary or putting some gates 

276-if any new user addedd in AWS that user most get permission boundry how we can achive this 

>>craete one policy for every user then configure cloudwatch montior then configure to the lambda will trigger and set that 

277-an you explain the steps to secure an AWS environment using IAM, VPC security groups, and network ACLs

>>Certainly! Securing an AWS environment involves multiple layers of security controls, including Identity and Access Management (IAM), Virtual Private Cloud (VPC) security groups, 
and network ACLs. Here are the steps to secure an AWS environment using these components:

IAM (Identity and Access Management):

Create individual IAM user accounts for each person or system that requires access to your AWS resources.
Assign appropriate IAM policies to each IAM user, granting them the least privilege necessary to perform their tasks.
Implement multi-factor authentication (MFA) for IAM users to provide an extra layer of security.
Regularly review and audit IAM policies to ensure they align with the principle of least privilege and remove unnecessary access.
VPC (Virtual Private Cloud) Security Groups:

Define security groups within your VPC to control inbound and outbound traffic at the instance level.
Create separate security groups for different types of resources based on their security requirements.
Specify the inbound and outbound rules for each security group, allowing only necessary protocols, ports, and IP ranges.
Regularly review and update security group rules to ensure they align with your organization's security policies.
Avoid overly permissive rules and follow the principle of least privilege.
Network ACLs (Access Control Lists):

Network ACLs act as a firewall for controlling traffic at the subnet level within your VPC.
Define inbound and outbound rules in network ACLs to allow or deny traffic based on IP addresses, protocols, and ports.
Carefully evaluate and configure network ACLs to restrict access to your resources.
Regularly review and update network ACL rules as per your security requirements.

278-What is the difference between IAM users, groups, roles and policies ?

>>IAM User: An IAM user is an identity within AWS that represents an individual or application needing access to AWS resources. IAM users have permanent long-term credentials, such as
a username and password, or access keys (Access Key ID and Secret Access Key). IAM users can be assigned directly to IAM policies or added to IAM groups for easier management of permissions.
>>IAM Role: An IAM role is similar to an IAM user but is not associated with a specific individual. Instead, it is assumed by entities such as IAM users, applications, or services to 
obtain temporary security credentials. IAM roles are useful when you want to grant permissions to entities that are external to your AWS account or when you want to delegate access to 
AWS resources across accounts. IAM roles have policies attached to them that define the permissions granted when the role is assumed.
>>IAM Group: An IAM group is a collection of IAM users. By organizing IAM users into groups, you can manage permissions collectively. IAM groups make it easier to assign permissions 
to multiple users simultaneously. Users within an IAM group inherit the permissions assigned to that group. For example, you can create a "Developers" group and assign appropriate 
policies to grant permissions required for developers across your organization.
>>IAM Policy: An IAM policy is a document that defines permissions and access controls in AWS. IAM policies can be attached to IAM users, IAM roles, and IAM groups to define what actions 
can be performed on which AWS resources. IAM policies use JSON (JavaScript Object Notation) syntax to specify the permissions and can be created and managed independently of the users, 
roles, or groups. IAM policies consist of statements that include the actions allowed or denied, the resources on which the actions can be performed, and any additionalconditions.

279-What is the relation between AMI and Instance?

>>Instances can be launched by AMIs. One AMI can launch as many instances as required. An instance type defines the hardware of the host computer including information about computers and
its memory abilities. After launching an instance, it works as a traditional host and could be interacted with as with any other computer.

280-what is secret Management and why 

>>secret management is the practice of securly storing, distributing, and rotating sensitive information such as passwords, API keys and certificates
>>secrets are a critical aspects of modern software development  and often required to authenticated with external service and systems
>>secret management involves Implementing secuirty best practices such as encryption access controls, and auditing to ensure that secrets are protected throughout their lifecycle
>>adopting a robust secret Management strategy can help Organizations comply with regulatory requirements such as HIPAA, GDPR, and PCI DSS  

>>AWS Secret manager is a service for easier management of secret. Secrets can be passwords, DB Credentials API keys or any text
>>You can store and control to these secrets centrally by using one of the following
1-secret manager console
2-secret manager command line interface(CLI)
3-secret manager Https query API
4-AWS SDKS
>>secrets are secured by audting at a central location
>>secret manager integrates with AWS logging and monitoring service provides centrailized auditing
>>secrets are encrypted using AWS KMS key manager service
>>using cloud watch and cloud trail all events relating to secrets manager can be monitored

281-can i rotate my password in secret manager

>>you can rotate your username and password in secret manager you can mention one week or any days. if you want create amazon RDS username and password directly through secret manager. when you create
rds then create username and select secret manager then automatically create the password 
>>like DocumentDB database and Redshift also or any other service also 
>>you can do schedule rotation of password 

282-how to get username and password from secret manager to linux system 

vi rds.sh 

secrets=$(aws secretsmanager get-secret-value --secret-id 'rds!db-87429bi#this rds endpoint' --query SecretString --output text)

username=$(echo $secrets | jq -r '.username')
password=$(echo $secrets | jq -r '.password')

:wq

>>then you can connect the mysql like this 
mysql -h 'rds!db-87429bi#this rds endpoint' -u $username -p$password -P 3306

283-what is cloud-formation (it is infrastructure as a code)(IAAS) what is CFT template

>>A CFT (CloudFormation Template) is a JSON or YAML file used to describe and provision AWS resources in a repeatable and automated manner. It allows you to define your 
infrastructure as code, specifying the desired state of your AWS resources and their relationships.In a CloudFormation template, you define a collection of AWS resources, their properties,
and the relationships between them. This includes specifying dependencies between resources, which ensures that resources are created and configured in the correct order.

1-Infrastructure as a code service that automates the deployment and management of AWS Services
2-It is used for Infrastructure Automation
3-can deploy both infrastructure and application code
4-support wide range of AWS resources
5-offers a high degree of customization and control over the deployment process
6-high and flexible but requires more time and effort to set up
7-more complex requires a good understanding of AWS infrastructure and services
8-free pay only for the resources created
9-best for managing and automating complex infrastructure

Resources:
  MyS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: my-bucket

  MyEC2Instance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: ami-12345678
      InstanceType: t2.micro
      KeyName: my-key-pair
      DependsOn: MyS3Bucket

---Advantages
>>CloudFormation allows you to use programming languages or a simple text file to model and provision, in an automated and secure manner, 
all the resources needed for your applications across all regions and accounts. This gives you a single source of truth for your AWS and third-party resources.

---Disadvantages
>>CloudFormation uses either JSON or YAML. This makes CloudFormation easy to read and manage. But there is a constraint that doesn't allow AWS developers to 
create CloudFormation templates greater than 51MB in size. In case the template exceeds this size limit, developers need to create a nested stack for the templates.

>we can use this template to create the services we can use this code to version control and if we can modify the code we can easily undesrstand
>>aws cloud-formation uses YAML and JSON lanugauge
>>Save the code in an S3 bucket, which serves as a repository for the code.
>>Use AWS CloudFormation to call the bucket and create a stack on your template

284-what are important component of CFT

1-Resources:-
2-parameters:-
3-Mappings:-
4-Outputs:-
5-Conditions:-
6-Metadata:-
7-Mappings:-
8-Trandforms:-

285-if the replacement is true in cloud-formation

>>in cloud formation when your are deployed one server and then changed key pair or something nything in your cloud-formation and importing in resource 
through updating when its come true means it will destroy the server or ec2-system and install fresh system or ec2 or resource. if it comes false then it update the system or resources

286-what is import-resource in cloud-formation

>>in cloud formation import-resource using. when you acccidentaly created ec2-instance manualy by mistake without deleeting that
you can import that resource using the Import-resource option but you must mention "deletionpolicy": "retain" is mandatory and if you want destory that system using
cloud-formation means first system will delete but next time add system not gone delete because it will using retain policy

287-what is roolback failure in cloud-formation

>>roll-back failure is must be enable. when your deploying code using cloud-formation if gets an error then automatically delete the everything what is deployed  in cloud-formation 

288-what is a designer in cloud-formation

>>AWS CloudFormation Designer (Designer) is a graphic tool for creating, viewing, and modifying AWS CloudFormation templates. With Designer, you can diagram your 
template  resources using a drag-and-drop interface, and then edit their details using the integrated JSON and YAML editor

289-what is stack policy in cloud-formation

>>stack-policy is used to restrict the updates of your cloud-formation templates of any user anyone

290-what is nested template in cloud-formation

>>nested stack using you can create multiple resource like 3 differnet vpc inside the 3 servers using centrailized url or your cloud formation stored or saved 
in somewhere s3 any where 

291-what is stackset in cloud-formation

>>stacksets using can deploy cloud-formation using the resources in different accounts . this accounts added in your orgnaztion , first need to create the stackset 
role and then create the stack-set execcution rule and give master account id and do same in sub-accounts and give master account id and then copy the ou id 
and then deploy your resources in other accounts if you added two more accounts that accounts also deployed that resources

292-what is detect drift in cloud-formation

>>detect drift is used to identify what is changes done when you deployed in cloud-formation configuration it detect and show you either you can add or delete further
                          (or)
>>Drift is the difference between the expected configuration values of stack resources defined in CloudFormation templates and the actual configuration values of these 
resources in the corresponding CloudFormation stacks.

293-create full vpc setup using cloudformation 

AWSTemplateFormatVersion: '2010-09-09'
Description: Create VPC with public and private subnets, Internet Gateway, and NAT Gateway

Parameters:
  VpcCidrBlock:
    Type: String
    Default: '10.0.0.0/16'
    Description: 'CIDR block for the VPC'

Resources:
  AnjiVPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VpcCidrBlock
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: anji

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref AnjiVPC
      CidrBlock: '10.0.1.0/24'
      AvailabilityZone: 'us-east-1a'
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: 'PublicSubnet1'

  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref AnjiVPC
      CidrBlock: '10.0.2.0/24'
      AvailabilityZone: 'us-east-1b'
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: 'PublicSubnet2'

  PrivateSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref AnjiVPC
      CidrBlock: '10.0.3.0/24'
      AvailabilityZone: 'us-east-1a'
      Tags:
        - Key: Name
          Value: 'PrivateSubnet1'

  PrivateSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref AnjiVPC
      CidrBlock: '10.0.4.0/24'
      AvailabilityZone: 'us-east-1b'
      Tags:
        - Key: Name
          Value: 'PrivateSubnet2'

  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: 'AnjiInternetGateway'

  AttachGateway:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref AnjiVPC
      InternetGatewayId: !Ref InternetGateway

  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref AnjiVPC
      Tags:
        - Key: Name
          Value: 'PublicRouteTable'

  PublicRoute:
    Type: AWS::EC2::Route
    DependsOn: AttachGateway
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: '0.0.0.0/0'
      GatewayId: !Ref InternetGateway

  NatGatewayEIP:
    Type: AWS::EC2::EIP
    Properties:
      Domain: vpc

  NatGateway:
    Type: AWS::EC2::NatGateway
    Properties:
      AllocationId: !GetAtt NatGatewayEIP.AllocationId
      SubnetId: !Ref PublicSubnet1

  PrivateRouteTable1:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref AnjiVPC
      Tags:
        - Key: Name
          Value: 'PrivateRouteTable1'

  PrivateRoute1:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref PrivateRouteTable1
      DestinationCidrBlock: '0.0.0.0/0'
      NatGatewayId: !Ref NatGateway

  PrivateRouteTable2:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref AnjiVPC
      Tags:
        - Key: Name
          Value: 'PrivateRouteTable2'

  PrivateRoute2:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref PrivateRouteTable2
      DestinationCidrBlock: '0.0.0.0/0'
      NatGatewayId: !Ref NatGateway

294-What is Elastic Beanstalk

>>AWS Elastic Beanstalk is an orchestration service offered by Amazon Web Servicesand it is used deploying and scaleup the AWS services, 
including EC2, S3, Simple Notification Service, CloudWatch, autoscaling, and Elastic Load Balancers, health monitoring

>>Elastic beanstaclk is used  if the developer is developing java application he is checking and deploying in 3 days in a week so we can use elastic beanstaclk
using create servers and required configuration easily

295-what is service 

this controls the starting and stopping of services

296-what is chkconfig

>>this controls which services are set to start on boot

297-what is crontab

>>In any operating system it is possible to create jobs that you want to recoccur. this process know as job-scheduling
for any linux this process is handled by the cron service or daemon called crond

* * * *  *
minutes hour day month week

298-what is process management

>>when you start a program running an application in linux it actually run as a process

299-Difference between AWS CloudFormation and AWS Elastic Beanstalk?

AWS-CloudFormation:-
1-Infrastructure as a code service that automates the deployment and management of AWS Services
2-It is used for Infrastructure Automation
3-can deploy both infrastructure and application code
4-support wide range of AWS resources
5-offers a high degree of customization and control over the deployment process
6-high and flexible but requires more time and effort to set up
7-more complex requires a good understanding of AWS infrastructure and services
8-free pay only for the resources created
9-best for managing and automating complex infrastructure
AWS-Elastic-Beanstalk
1-Platform as a service(Paas) that automates the deployment and management of applications
2-Application Deployment Automation
3-only deploys application code
4-supports only certain languages and application types
5-provides pre-defined opinionated deployment process
6-limited flexibility but easier and quick to set
7-less complex requires less knowledge of AWS infrastructure and services
8-free pay only for the resources created
9-best for deploying and managing web-applications

300-what comes in aws IAAS , PAAS, SAAS 

1-IAAS -->will come ec2 you need to deploy and build 
2-PASS -->will comes elastic beanstalk select what you want and build  thats it 
3-SAAS -->will come to lightsail you dont know anything this will comes SAAS

301-when AWS launched 

>>2003 is designed for documentation and 2006 is AWS launched  2012 is re-invented the AWS  

302-is it possible to change free tier to business or enterprise 

>>yes we can go to the support and then click on change what you want 

303-how many mfa device in aws

>>You can register up to eight MFA devices, in any combination of the currently supported MFA types, with your root users and IAM users. Sign in to the AWS Management Console and do the following: 
For a root user, choose My Security Credentials. For an IAM user, choose Security credentials.

304-how will you manage the aws account

>>for example we have big organization we have 8 users for aws administration purpose we create each user for specific role like anji user can do only ec2 like any ticket raise ec2 problems will solve 
only anji user like sampi s3 like this. reaminig you have multiple accounts then we can use aws organization to segarte the accounts

305-types of access in aws 

1-aws console access 
2-programtic access like access and secret key 

306-is it IAM admin user will equla to root user in AWS 

>>Yes but its not equal to root there are some limitations like billing and support and account plans 
>>but you can enable billing access you can login as root then go the account then click on edit IAM user an role access to billing click on enable then user Can access the biliing section 

307-can i create instead of account id any username while any user login 

>>yes you can create click on IAM then create alias anji123 when any user login he cant rember account is right then he can enter anji123 then username and password 
>>you will see the option in IAM click on create alias create what name you want that will not created anyone before that will be unique to globally like s3  

308-Can you describe a scenario in which you would use AWS Elastic Beanstalk to deploy an application?

>>AWS Elastic Beanstalk is a fully managed service offered by Amazon Web Services that allows developers to easily deploy, run and scale web applications. 
The service is ideal for situations where a developer has created a web application using a popular framework, such as Ruby on Rails, Node.js, or Java, and is looking to quickly deploy
the application to a production environment with minimal overhead. AWS Elastic Beanstalk takes care of provisioning the necessary resources, such as EC2 instances, load balancers, 
databases and security groups, and sets up a fully functional environment for the application. This eliminates the need for the developer to manage the underlying infrastructure, 
freeing them up to focus on the development and maintenance of the application itself. Monitoring the health of the application can be done using AWS Elastic Beanstalk’s built-in monitoring 
and reporting features, and updates and changes can be made as needed. This makes AWS Elastic Beanstalk a simple, streamlined solution for deploying and managing web applications, 
particularly well-suited for developers who want a straightforward way to handle the deployment and management of their applications.

309-How would you implement a disaster recovery solution in AWS using RDS, EC2, and S3?

>>Create an Amazon RDS instance in a primary region, and configure automatic backups to Amazon S3.
>>Create an Amazon EC2 instance in the same primary region
>>Create an Amazon S3 bucket to store data backups, and configure the RDS instance to store backups in the S3 bucket.
>>Set up an Amazon EC2 instance in a secondary region and install the necessary software to access the S3 bucket.
>>Create an Amazon RDS instance in the secondary region and configure it as a replica of the primary RDS instance.
>>Configure the secondary RDS instance to automatically fail over to the primary RDS instance in case of a disaster in the secondary region.
>>Regularly test the failover process to ensure it is working as expected.
>>Use Amazon S3 versioning and object lifecycle management policies to retain backups for a desired period of time.
>>Use Amazon CloudWatch to monitor the RDS instances and EC2 instances, and set up alarms to trigger notifications or automated actions in case of a disaster.

310-What are the tools used to send logs to the cloud environment?

1-AWS CloudWatch Logs
2-AWS Elastic Beanstalk
3-AWS Lambda
4-Fluentd
5-Logstash

311-what is operating system

>>it is an user interface between user and hardware is called operating system
>>What is the operating system it is a combination of software and hardware
>>Like os will take the instructions from to convert into 0/1 binrays into hardware
>>Like clicking something converts into 0/1 in software or OS in the background you will not see that but it will work that way only the computer understands binaries we changed that like 
human-readable for understanding for example you want to run binaries it big headache Or it's tds task so that why the operating system this when we run instructions it runs in the background in binaries 
so this operating system

312-Can I Retrieve Data from a Deleted ELB or Ec2 Instance?

>>Metrics data for deleted ELBs and decommissioned EC2 instances are still available. MetricsData associated with a terminated instance will be removed when the Instance is shut down. 
In contrast, MetricsData associated with your account will remain on the MetricInfraService until you delete it. This information can be used for diagnosing problems with your ECS cluster or ELB.

313-If I disable monitoring for an Ec2 instance, will I lose the metrics data?

>>No, the metrics data will not be lost.
>>If you disable monitoring for an EC2 instance, the metrics you've collected are still safely preserved in a SQL database as part of the AWS CloudTrail logs.

314-What is RDS AWS used for?

>>Amazon RDS managed by Amazon and it makes it easy to set up operate and scale a relational database in the cloud it provides cost-efficient and resizable capacity
while automating time-consuming administration tasks. such as hardware provisioning database setup patching and backups
>>Amazon RDS is available on several database instance types optimized for memory performance I/O provides familiar databases like aurora, PostgreSQL, MySQL, MariaDB, Oracle database, SQL server
>>Amazon Relational Database Service (Amazon RDS) is a collection of managed services that makes it simple to set up, operate, and scale databases in the cloud.

>>RDS is structured database
>>RDs is platform as service(paas)
>>you can create read replcia in another region also us-east-1a to ap-south-1a
>>if you createing an database automatically it will create two databases in a region but they don't know
it will connect in primary database ,the primary will sync to secondary database ,if in case primary not availble then secondary will connect 
>>if the primary available zone damaged not available then it will create onemore database in availble region like us-east-1c
>>you will connect only primary database
>>you can't access secondary why because where the database is there don'tknow

315-what is the rds failover time

>>60-120 seconds

316-rds database tpes

1-master(primary)
2-read(secondary)replica       -->you can create read replica

>>some tester has come and asked we test the rds you're not giving the primary you dont
know how to access the secondary that time you can use the read replica
>>read replica just copy a primary you can put another region also
>>in read replica you can't write on the database its only read option

317-Will your standby RDS be launched in the same availability zone as your primary?

>>No, standby instances are launched in different availability zones than the primary, resulting in physically separate infrastructures. This is because the entire purpose
of standby instances is to prevent infrastructure failure. As a result, if the primary instance fails, the backup instance will assist in recovering all of the data.

318-How would you address a situation in which the relational database engine frequently collapses when traffic to your RDS instances increases, given that the RDS instance replica
is not promoted as the master instance?

>>A larger RDS instance type is required for handling significant quantities of traffic, as well as producing manual or automated snapshots to recover data if the RDS instance fails

319-What is SQL Server in simple words?

>>SQL Server is a database server by Microsoft. The Microsoft relational database management system is a software product which primarily stores and retrieves data
requested by other applications. These applications may run on the same or a different computer

320-What is AWS redshift used for?

>>it is type of database it belongs to postgress sql.it used for data ware-housing purpose
>>Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes,
using AWS-designed hardware and machine learning to deliver the best price performance at any scale

321-what is elastiCahe in aws

>>elastcahe is not regular database it is web-service that makes easy to launch manage and scale distributed in memory cache in the cloud
it is used to improve your website or web-application performance. it will cache the services in that memory it will increase the better performance
>>it like ec2-service
 
two types:-
1-memcahed 
2-redis

322-What is DocumentDB used for?

>>it like mongo database
>>A document database is a type of nonrelational database that is designed to store and query data as JSON-like documents. Document databases make 
it easier for developers to store and query data in a database by using the same document-model format they use in their application code.

323-how do you backup rds server

>>you need to create first option group and select the version and type of your database and then create your option group. then go to s3 create bucket and role
for s3 and then go that group attach the role and bucket and select the backup and restore it will 

324-how to make rds highly available

>>multi AZ deployment high availbilty & fail over for disaster recovery.
>>read replicas provide high performance for read heavy workloads
>>automatic database software updates
>>easy to scale compute memory stoarge
>>automated backups and manula backups
>>encryption at rest (AWS KMS) andd transit SSL

325-how to make rds highly availble

>>you can create the one RDS-Database then created the read replicas in 2 more availbilty zones then and also your servers are in different availbility zones like 
az1->RDS(read-write) az2->RDS(read only), az3->RDS(read only) if the any traffic will go az2 then it will take data from az2->RDS only if any want write then it will az1->RDS
why because it will have only read and write permisssion this will make your RDS highly availble and fast your database

326-How have you implemented disaster recovery for an AWS application?

>>In a previous project, we implemented disaster recovery for an AWS application by using AWS Route 53 to route traffic to a secondary region in case of a failure in the primary region. 
We also set up Amazon S3 cross-region replication to ensure that the data was replicated to the secondary region in real-time. Additionally, we created automated backups of the
application data using Amazon S3 and Amazon Glacier to ensure that we could quickly restore the data in case of a disaster.

327-Can you describe a time when you had to troubleshoot a performance issue in an AWS application?

>>In a previous project, we had an issue where the application was slow to respond to user requests. After reviewing the logs, we identified that the application was experiencing high CPU
usage. We then analyzed the application’s resource utilization using Amazon CloudWatch and identified that the database queries were taking longer than expected. We optimized the database
queries and improved the performance of the application.

328-Can you explain a time when you implemented security measures for an AWS application?

>>In my previous role, we implemented several security measures for an AWS application, including setting up Multi-Factor Authentication (MFA) for all AWS accounts, enabling AWS CloudTrail
to monitor and log all API activity, and encrypting data at rest using AWS Key Management Service (KMS). We also regularly conducted vulnerability assessments and penetration testing to
identify and mitigate potential security risks.

329-How would you use AWS S3, CloudFront, and Route 53 for a scalable and highly available static website?

>>AWS S3, CloudFront, and Route 53 can be used together to build a scalable and highly available static website. Here’s an overview of the process:

>>Store the website files in an Amazon S3 bucket and make the files publicly accessible.

>>Create a CloudFront distribution, which is Amazon’s global content delivery network (CDN), and configure it to use the S3 bucket as its origin. This allows CloudFront to serve the 
website files from edge locations around the world, ensuring fast and low-latency access for visitors.

>>Use Amazon Route 53, the highly available and scalable DNS service, to associate a custom domain name with the CloudFront distribution, so visitors can access the website using a 
familiar and memorable domain name.

>>Configure Route 53 to use the health checks and failover features to ensure that the website is always available, even if one of the edge locations becomes unavailable. Finally, 
use S3’s versioning and lifecycle policies to automatically store multiple versions of the website files and to transition older versions to less expensive storage options over time.

330-what Required to connect the MySQL or RDS or any databse

>>The account that is used to connect to the MySQL database must have at least CONNECT permissions. This enables SSMA to obtain metadata from schemas owned by the connecting user.
1-connection permission any network or locally
2-mysql endpoint
3-port number should be open
4-username and password
5-mysql workbench or jdbc connection

331-what is AWS Backup

>> AWS Backup features .AWS Backup is like snapshots. in every organzation need to take a backup some of the particular resources with using of aws backup take some of major 
resources in aws backup automatically or daily backup no need to write script you can do automated backup on time bases like you can schedule and store the backup in s3 directly
>>your specify the some of the resource in aws to take backup on AWS Backup service. on which time you need . you can take partcular time take backup between
partcular  resource to take the backup. its nothing but snapshots

>>ec2 
>>rds 
>>EFS
>>s3 
>>dynmodb 

332-How to take backup timely.

>>you can specify the time and it will take automatically when you choose the valut then store that automatically

333-what is backup vault

>>its like glacier vault where we can storing data or saving the data is called backup vault

334-how many types of backup

>>two types
1-automatic
two types
a-daily backup
b-transactionlogs
2-snapshots

two types of RDS backup 

1-Automated backups:- it will store in s3
>>RDS automatically create backs up your database and transaction logs
>>RDS automcatically create a storage volumes snapshots of your DB instances backuing up the entire DB intsance
>>Automated backups occur daily during the preferred nackup window
>>Automated backups are kept for the Backup Retention period (0 to35 Days)
>>you can restore the DB instances to specify point in time during the bcakup retention period
creating a new DB instances

2-Database snapshots

>>Database snapshots are user-initiated backups of the DB instance stored in s3
>>DB snapshots are kept intil explicitly delete them
>>you cna creae a DB instance by restoring from DB snapshots
>>the first snapshots contains the data for the full DB instance subsequent snapshots of the
same DBinstance are incremenetl
>>ypu can copy a sanpshot in the same region acrss regions and across acounts
>>you can directly resore a DB instance from unencrypted a shared snapshot

335-how many days take AWS-backup can retrive

>>minimum 35 days

336-what is the parameter group in MySQL or RDS in AWS 

>>If you want to modify any setting or any changes then like if you want to change the max=connection for example your RDS max connection 20 it small instance but that pods connected 2 pods is okay 
you used hpa the pods increased to 20 it will not connect because of the RDS instance type max connection also you need to change 

337-i have rds primary my-sql in ap-south-1 is there i want to create read replica in us-east-1 and read replica will be amazon aurora is possibe 

>>yes its possible you can create the read replica in another region and you can select read replcia in multiple availability_zones in another region and 
you can change read replica in my-sql to amazon aurora also possible 
>>select the db you want click on actions then good to go select option   

338-can i promote read replica to primary permanent

>>yes you can promote replica to primary it will treat as a separte instance 

339-can i share my rds snapshot to another account

>>yes you can share to another account and another regions also 

340-my database runing longback and something happen it crashed can i take ontime backup on that database

>>yes you can select that base click on actions select restore ontime backup  it will create ontime backup and create database and if you want you can choose certain time and date also 

341-how much rds mysql size in aws 

>>upto 64 tebibytes   -->mysql 
>>upto 128 tb         -->aurora-mysql 
>>Supports up to 15 Read Replicas per instance, within a single Region or 5 read replicas cross-region.  -->mysql 
>>Six-way replication across three Availability Zones            -->amazon aurora-mysql

342->>amazon auora supports serverless 
>>amazon auoura supports global database feature if you enbale this it will store multiple locations like ex:- iam created mumbai and one more create usa or london maybe not sure 
>>nosql is dymnodb is like mongodb 

343-if any one is put my bucket or rds snapshot in public we need to get notification how its possible 

>>you can create the aws config rule that will help you to trigger automatically 

344-What is the difference between the Amazon Rds, Dynamodb, and Redshift?

Amazon RDS: Relational database service that allows you to set up, operate, and scale a relational database in the cloud easily.

DynamoDB: Fully-managed NoSQL database that provides fast and predictable performance with seamless scalability.

Redshift: Data warehousing service that provides fast querying and analysis of data using SQL and business intelligence tools.

345-what is virtual private gateway

>>A virtual private gateway is a tool establishig a safe connection or tonnel to connect over the Internet . it will create secure connection over the
Internet and it reliable. virtual network gateway is to connect your on-premises network with VPC
>>A virtual private gateway is a tool for establishing a safe tunnel to connect and transport encrypted data between devices, the cloud, and enterprise servers
across the internet. Use a virtual private gateway to create a VPN connection that is both secure and reliable.

346-What is site-to-site-vpn

>>A site-to-site virtual private network (VPN) is a connection between two or more networks, such as a corporate network and a branch office network. the site-to-site-vpn creates a tonnel to connect a
secure way over the ineternet .Many organizations use site-to-site VPNs to leverage an internet connection for private traffic as an alternative to using private MPLS circuits
>>it will give two ips you can create the two tonnles if first one fails then you can use or set second one 
>>tonnel is nothing but connecting the secure way in the ineternet

347-What is point-to-site VPN?

>>A Point-to-Site (P2S) VPN gateway connection lets you create a secure connection to your virtual network from an individual client computer.
A P2S connection is established by starting it from the client computer.
>>you launch public and private certificates in your system and connect to the aws VPCS

348-what is client vpn-endpoint

>>AWS Client VPN is a fully-managed remote access VPN solution used by your remote workforce to securely access resources within both AWS and your on-premises network. 
Fully elastic, it automatically scales up, or down, based on demand
>>it is like openvpn or vpn to connect your private network thrugh client vpn endpoint of was 

In Linux this also you can do windows also refer this url https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/cvpn-getting-started.html#cvpn-getting-started-certs

>>git clone https://github.com/OpenVPN/easy-rsa.git
>>cd easy-rsa/easyrsa3
>>./easyrsa init-pki
>>./easyrsa build-ca nopass
>>./easyrsa --san=DNS:server build-server-full server nopass
>>./easyrsa build-client-full client1.domain.tld nopass
>>mkdir ~/custom_folder/
cp pki/ca.crt ~/custom_folder/
cp pki/issued/server.crt ~/custom_folder/
cp pki/private/server.key ~/custom_folder/
cp pki/issued/client1.domain.tld.crt ~/custom_folder
cp pki/private/client1.domain.tld.key ~/custom_folder/
cd ~/custom_folder/
>>need to upload this key in certficate manager so use iam role for proper access 
>>aws acm import-certificate --certificate fileb://server.crt --private-key fileb://server.key --certificate-chain fileb://ca.crt --region us-east-1
>>aws acm import-certificate --certificate fileb://client1.domain.tld.crt --private-key fileb://client1.domain.tld.key --certificate-chain fileb://ca.crt --region us-east-1
>>go the AWS client VPN endpoint 
>>create anji-vpn client ip range any range 12.0.0.0/16 and select server authentication certficate and click on use multiple authentication
and select client certficate and select vpc and security group cliCk on create 
>>select the vpn and click on target group assocation select the private subnet and select vpc click on ok 
>>select vpn and the security allow all traffic 
>>select the vpn and click on authorization rule add the vpc cide like 10.1.0.0/16 click on create 
>>select the vpn click on client configuration and downalod
>>download aws client vpn to local laptop 

>>.vpn file you need to edit crt certificate in this maaner 
https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/cvpn-getting-started.html#cvpn-getting-started-certs
<cert>
Contents of client certificate (.crt) file
</cert>

<key>
Contents of private key (.key) file
</key>
:wq
>>then connect you will get 

349-what is direct connect in aws

>>direct connect is like vpn but it like leased line or dedicted line to internet . you can create your virtual private network then you can connect your aws to your on-premises. 
AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between your internet and Amazon VPC. it is compared to vpn it is costly
>>AWS Direct Connect is a networking service that provides an alternative to using the internet to connect to AWS.

533-direct connect use cases

1-Disaster Recovery and Backup
2-Secure Connectivity
3-High-Performance Computing (HPC)
4-no packet dropping 
5-Global Enterprises
6-Low-Latency Trading
7-Improved User Experience
8-Data Transfer Efficiency
9-Fast Content Distribution:
10-Cost Optimization: Reduces costs associated with data transfer over the internet and optimizes network performance for enterprise IT operations.
11-Quick Failover: 
12-Enhanced Security: Offers a private, dedicated connection that is isolated from the public internet, reducing the risk of data breaches and enhancing security for sensitive data

350-What is OpenVPN used for?

>>Open source OpenVPN uses VPN technologies to secure and encrypt data sent over the internet. Its custom VPN protocol uses SSL/TLS for key exchange. 
Since its creation in 2001 it has become the de facto standard in the open source networking space with over 60 million downloads 

===openvpn 

>>create vpc name Green and ip 10.1.0.0/16 

>>create subnet name Green-public and select vpc Green and ip 10.1.1.0/24

>>create subnet name Green-private and select vpc Green and ip 10.1.10.0/24

>>create internet gateway name Green-igw and attach to Green

>>create route table name Green-pub and select vpc Green and edit add route 0.0.0.0/0 and select internet gateway Green-igw and subnet associations select Green-public

>>create route table name Green-priv and select vpc Green and edit subnet association select Green-private

>>launch one instance using Green vpc and Green-public subnet and region us-east-1a and enable public ip and all protocal allow

>>launch one instance using Green vpc and Green-private subnet and region us-east-1a and disable public ip and allow all protocals

>>click on launch instance and select aws marketplace and do select vpc Green and subnet Green-public enable public ip and allow all protocals and launch

>>connect that vpn using putty 3.6.86.195 username openvpnas or root then yes and enter and ok

>>ssh -i anji.pem root@3.6.86.195 or openvpnas@3.6.86.195 
>> create password of openvpn 

>>sudo su -

>>passwd openvpn
openvpn
openvpn

>>then copy the url and past it url bar then download windows 

>>install that software in your system and then give username and password and and enable openvpn

>>now your good to go

>>you can ping 192 to 10    -->your system to aws environment

>>if you want ping the system to aws to local system you need to add routes in Green-pub add 192.168.0.0/16 and select network intrafce of openvpn and then ok

>>now ping 10 to 192

9-types of VPN providers or third-party providers  

1-cicso any connect 
2-zscaler 
3-openvpn
4-filezilla 

351-what is AWS Config 

>>if you enable your ec2 service or s3 ebs if any changes happen to your ebs or ec2 it will gather that information store 
cloudtrail also do but not efficient way and it will not give detailed information  
>>Aws config will monitor configuration changes and compilnace need to enable then you will see what changes happen
>>if you enable compilnace create set of rules for example you create a rule use AMI if someone is not used that AMI you will see or configure
 the SNS also    
>>if you enable aggregator then you can attahc multiple accounts 

352-What are the native AWS Security logging capabilities?

>>Most of the AWS services have their logging options. Also, some of them have an account level logging, like in AWS CloudTrail, AWS Config, and others.
Let’s take a look at two services in specific:

1-AWS CloudTrail:
>>cloudtrail is used to see th login logut details and activitesits not give detailed information
>>cloudtrail is data will save only 90 days, so we can import that into aws s3 or any centrailzied locations

2-AWS Config:
>>first enable select the resource what you need the resource type and enable recording on 
>>AWS config is used to track the details of what you changed on your instances or modfied your instances.  if you chnaged something in your ec2-instances 
it will automatically shows what you updated in your system it will and it will save in until 7 years

>>AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, and configuration change notifications to enable security and governance.

353-How would you use AWS CloudTrail and CloudWatch to monitor and log AWS resource activity and events?

>>AWS CloudTrail and CloudWatch are two services that can be used to monitor and log AWS resource activity and events. CloudTrail records API calls made to AWS services and stores 
information about the caller, time of call, source IP address, request parameters, and response elements. This information can be used to track changes to AWS resources and identify 
security threats. CloudTrail trails can be created to specify the AWS resources for which API calls should be recorded and logs can be sent to CloudWatch Logs for central storage and analysis.

>>CloudWatch Alarms can be set up to trigger when specific conditions are met, such as when an API call is made to a sensitive AWS resource. Alarms can be configured to send notifications 
to specified Amazon SNS topics or to stop or terminate an Amazon EC2 instance. The logs collected by CloudTrail and stored in CloudWatch Logs can be analyzed to identify trends, 
track resource activity, and diagnose issues.

>>By using CloudTrail and CloudWatch, organizations can have a comprehensive view of AWS resource activity and events, enabling them to identify security threats, troubleshoot issues,
and meet compliance requirements. The centralized logging and analysis provided by these services helps organizations to ensure the security and availability of their AWS resources, 
giving them greater control and visibility into their cloud infrastructure.

354-how many ways to montior cloudtrail

>>aws s3 
>>cloudwatch 

355-what are the events in cloudtrail

1-data-event
2-management event
3-insight event

356-what is management event in cloudtrail

>>management evnet provides information about management operations that are preformed or resources in your aws account these are also known as control plane operations 
eg:
who accessing to aws account 
changing configuration such as routetable, security group 

357-what is data event in cloudtrail

>>data evet provides information about the resource operations performed on or in a resource . these are also known as data plan operations
eg:-
1-s3 bucket operations
2-lambda 
3-ebs 

358-what is insight event

>>if you have insight events enabeld and cloudtrail detects unusual activity insight evnets are logged to a different folder or prefix in the destination s3 bucket for your trail 
eg:
20 buckets deleted at a time 
multiple time authentication failed to access a resource 

359-what is evnet history in cloud trail 

>>cloudtrail event history provides a viewable, searchable and downloadable record of the past 90 days of cloudtrail evnets .
you cna use this history to gain visibility into actions take in your aws management console, aws sdk, command line tool and other aws service 

360-if your created some error in aws account your trying to delete that log in cloudtrail its not possibe escape . you have to accept the error A

>>if your want store the cloudtrail logs you must be create trail then select the bucket and if you wnat cloudwatch an loggroup and select event then good to go 

361-What is the difference between CloudWatch and CloudTrail?

>>CloudWatch is a tool provided by Amazon Web Services that monitors and reports on the performance of various AWS resources and services.
>>CloudTrail, on the other hand, is a log of all actions in your AWS environment since it was first launched.

362-What services can be used to create a centralized logging solution?

>>The essential services that you can use are Amazon CloudWatch Logs, store them in Amazon S3, and then use Amazon Elastic Search to visualize them.
You can use Amazon Kinesis Firehose to move the data from Amazon S3 to Amazon ElasticSearch.
>>if your service is stopped are anything happens you can create a rule to trigger your script and send the notifications
(like if your lamabda function to stop the start and specific time then youy can use the rules it will be changed)

363-what is firewall

>>firewall is allow or disallow the traffic in your network
>>firewall is security group it is used to control the incoming and outgoing traffic

>>firewall are two types

1-stateless
>> It is not a two-way handshake
>>Stateless firewalls make use of a data packet's source, destination, and other parameters to figure out whether the data presents a threat. These parameters have to be entered by either
an administrator or the manufacturer via rules they set beforehand

2-stateful firewall
>> It is a two-way handshake  process
>>A stateful firewall inspects everything inside data packets, the characteristics of the data, and its channels of communication. Stateful firewalls examine the behavior of data packets,
and if anything seems off, they can filter out the suspicious data. Also, a stateful firewall can track how the data behaves, cataloging patterns of behavior. 

>> It is used to allow one side no need to allow the other side will remember and send the packets
>> A stateful firewall remembers the connection details it will give a replay

3-In-Bound rule

>> Inbound firewall rules protect the network against incoming traffic, such as disallowed connections, malware, and denial-of-service (DoS) attacks

4-Out-Bound rule

>>Outbound firewall rules protect against outgoing traffic, originating inside a network.

364-What is a DDoS attack, and what services can minimize them?

>>Distributed denial of service DDOS The main intention is to make it unavailable over the internet on a website or some resources. the hacker sends the multiple
requests into website is going unavailble over the internet

AWS Shield
AWS WAF
Amazon Route53
Amazon CloudFront
ELB
VPC

365-What is whitelisting an IP address?

>>IP whitelisting is when you grant network access only to specific IP addresses. Each employee (or approved user) shares their home IP address with the 
network administrator, who then enters their IP address on a “whitelist” that grants them network access.

366-what is WAF in AWS

>>it can provide the additional security of your web application. it will hide your servers on the internet it will prevent cyber attacks like sql injection and xss and ddos attacks 
>>you have vpc public an priate subnet installed web servers and app-server web-server connected load balancer then that load balancer connected to web application firewall then connected to route53 open to anji.com internet 
>>you can create ipsets and block the ips by regions are subnets 
12.34.78.0/16
12.34.79.0/16
>>you can block certain countries also 
>>create a web-acl select region select load balancer -->add rules i you select ip sets your adding IPS to allow ro deny if you use rulebuilder under rulevisualeditor type regular rule matches staement origin from a country you can block certain country also  then click on create 

internet -->cloudfront(maybe iam not sure ) -->route53 -->firewall -->loadbalancer -->web-servers-->
>>User's request -> AWS Global Accelerator -> CDN (if configured) -> DNS (Route 53 or another DNS service) -> Firewall -> Load Balancer -> Reverse Proxy -> ElastiCache (Redis) -> Web Server  -->this is correct 

367-How to secure a Website hosted on Apache Web Server?

>>There are multiple ways to secure the Apache webserver including the following.

Implementing SSL
Integrating with WAF (Web Application Firewall) like ModSecurity, etc.
Using a cloud-based security provider

368-what is resource access manager 

>>you can share your subnets or image builder and share to another account 
>>video 38 52mins 
1-fsx Volumes
2-subnets
3-code build projects 
4-meshes
5-network firewall polices
6-transit gateways 

369-what is aws security hub 

>>AWS Security Hub will help to manage and improve your secuirty posture 
>>it will help to findout the security issues like s3 buckets open are any security group ports open and all check PCI DSS standard if you enabeld 
>>in best practice if we enable secuirty hub it will help to un-expose our data into internete . i have seen many companys dont enabeld or dont check whether that ports or pubic accees buckets becuase of pricing issues that will lead to security attacks and expose data into internet 
>>if you enable security hub this will not enbable automatically if you want enable you can 
1-Amazon Detective 
2-Amazon inspector
3-AWS Firewall Manager 
4-Amazon guardDuty
5-AWS Helath 
6-AWS IAM Analyzer 
7-Amazon Macie 
8-AWS Audit Manager

370-What is CDN and how it works?

>>A CDN or cloud-front is the same. content deliver network decreases the latency in your website and it provides the cache server in all edge locations 
on your website.it is used to speed up your website over the internet. it will be cache all the content to the all edge point using of your provider . it will sit infront of your route53 
>>its like firewall if any doing some ddos attack or ratelimiting it will block that user certain time 
>>if your using cloudfront you can block or allow certain countries and you can create ttl 1 min 31536000 max for cache 86400 as default 
for oneday   
>>you can block some of the path based routing like its lead to lfi attacks you can restricat like this 
>>The CDN (Content Delivery Network) is often used to cache and deliver content closer to the user, improving performance. The DNS resolution happens first to translate the domain (like anji.com) 
into an IP address, then the request goes through the specified network components.
>>you can block or put some restriction in cloudfront also like firewall
>>User's request -> AWS Global Accelerator -> CDN (if configured) -> DNS (Route 53 or another DNS service) -> Firewall -> Load Balancer -> ElastiCache (Redis) -> Web Server  -->this is correct 

371-what is global acclerator

>>it is like cloud-front or CDN. CDN provides cache server but global-acclerator gives to allow ports and give you can configure fixed entry-points to ip address  
>> AWS Global accelerator is a networking service that helps you to improve the availability zones and performance of the applications that offer you to your 
golbal users. aws global acclerator easy to setup and manage and configure. it provides static ip address and fixed entry point your application. 
global acclerator is basically global load balancer which load balancing the regions as well .it is costly compared to dns load balancer
>>AWS Global Accelerator helps improve the availability and performance of applications by using static Anycast IP addresses and routing traffic over the AWS global network to optimal endpoints.
It sits in front of your resources, like a load balancer, and directs traffic based on health, geography, and routing policies you configure.

>>AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world.
 CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and 
 dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the
  edge to applications running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), 
  IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional
   failover. Both services integrate with AWS Shield for DDoS protection.

372-when someone asked need to improve website performance

>>in your web application is there any photos or videos is there like need to cache the data then we can good to go with cloud front
>>if there is no cacheing your web application like there is no photos or video then you can global accelartor

container is just process runing on a linux server 

VMWARE:- Infrastructure-->Hypervisior-->Guest OS -->your app1
                                       |Guest OS -->Your app2  -->like many things 
                                       
Container:-Infrastructure-->Hypervisior-->Guest OS -->Container Engine -->your app1
                                                                      |-->your app2 
>>this like you can create multiple apps . in this scenario until guest os managed by cloud provider                                                                            

>>for suppose your taken ubuntu base image 
docker run --rm -dit --name nginx -p 8000:80 ubuntu:latest 
docker exec -it nginx bash  -->your inside the container but nothing is there
>>apt upate && apt install net-tools iputils-ping wget curl nginx unzip nano vim -y    
>>service start nginx  -->it will start nginx 
>>docker commit nginx anji1592/utils 
>>docker push anji1592/utils 

373-Is there any other alternative tool to log into the cloud environment other than console?

Putty
AWS CLI for Linux
AWS CLI for Windows
AWS CLI for Windows CMD
AWS SDK
Eclipse

374-what key you need to send the servers to connect 

>>Create public and private keys pass the public key into other servers then connect using the private key 
>>You need to add public key to any server and then connect by using a private key only 

375-how to do cost optimization database

1-Analyze Your Database Workload:
>>Identify usage patterns: Examine database usage metrics to understand peak hours, idle times, and overall resource consumption.
>>Categorize data: Classify data based on access frequency (hot, warm, cold) to determine appropriate storage classes.
>>Measure query performance: Analyze query performance to identify bottlenecks and optimization opportunities.
2-Right-Size Resources:
>>Choose appropriate instance types: Select instance types that align with your workload's CPU, memory, I/O, and network requirements.
>>Adjust instance size: Scale instances up or down as needed to match actual resource needs.
>>Utilize reserved instances: For predictable workloads, commit to reserved instances for significant discounts.
3. Optimize Storage:
>>Select suitable storage classes: Choose cost-effective storage classes based on data access patterns (e.g., SSD for hot data, archive storage for cold data).
>>Implement data compression: Reduce storage footprint and costs by compressing data effectively.
>>Tier data based on access: Automatically move less frequently accessed data to cheaper storage tiers over time.
4. Implement Cost Management Tools:
>>Utilize cloud provider tools: Leverage built-in cost management tools from your cloud provider to monitor usage, set budgets, and identify optimization opportunities.
>>Consider third-party tools: Explore third-party cost optimization tools for advanced features and analysis.
5. Optimize Database Queries:
>>Identify inefficient queries: Use query optimization tools to pinpoint slow-running queries and improve their performance.
>>Tune database configuration: Adjust database settings and indexes to enhance query efficiency.
6. Consider Serverless Options:
>>Evaluate serverless databases: For workloads with variable usage patterns, explore serverless database options that offer automatic scaling and pay-per-use pricing.
7. Monitor and Adjust:
>>Review usage patterns regularly: Continuously monitor database usage and costs to identify areas for further optimization.
>>Adjust strategies as needed: Adapt your cost optimization approaches based on changing workloads and usage trends.

376-what are serverless computing and how does it differnet from traditinal server architecture 

>>>>imageine that im ceo of startup and i have 10 clients and idont know when the 10 clients will access or loggedin  my application. if i use fro this clinets in traditional server is runing 24/7 the 
cost will be high . what i can do whenever my client access my application the server will up and it will reduce the cost how much you used then you pay . if you used the scaling the headche will taken by 
AWS provider 
>>lambda , SNS, SQS it is for application more robust 

Serverless computing is a cloud computing execution model that enables developers to build and run applications without provisioning or managing servers. It's essentially a way to offload the operational
burden of managing infrastructure to the cloud provider, allowing you to focus on writing code and building your application logic.

1-Focus on code: You write code in the form of "functions" that are triggered by events, like user interactions, API calls, or scheduled tasks. You don't worry about servers, operating systems, 
or patching software.
2-Pay-per-use: You only pay for the execution time of your code, not for idle servers. This makes serverless ideal for applications with variable workloads and can lead to significant cost savings compared
to traditional server architectures.
3-Automatic scaling: The cloud provider automatically scales your functions up and down based on demand, ensuring optimal resource utilization and avoiding performance bottlenecks.
4-Faster development: By eliminating server management tasks, serverless frees you to focus on writing and deploying code, potentially leading to faster development cycles.
5-High availability: Cloud providers typically offer high availability guarantees for serverless functions, ensuring your application is always accessible.

Here are some examples of use cases for serverless computing:

>>Backend logic for web and mobile applications
>>Processing data streams from IoT devices
>>Triggering automated workflows based on events
>>Building serverless API gateways
>>Running scheduled tasks
>>However, serverless computing also has some limitations:

1-Vendor lock-in: You can become dependent on the specific cloud provider's offering, making it difficult to switch providers later.
2-Cold start latency: The first time a function is invoked, there can be a slight delay while the cloud provider spins up resources.
3-Limited debugging and logging: Debugging and logging can be more challenging in serverless environments due to the ephemeral nature of functions.

377-what is Distributed and Centralized database 

Distributed database:-
1-consists of multiple database files located at different sites 
2-Allows multiple users to access and manipulate data 
3-Files delivered quickly from location nearest the user 
4-If one site fails data is retrievable 
5-Multiple files from dispersed databases must be synchronized 
Centralized Database:-
1-consists of single database files will be central database file 
2-Bottlenecks when multiple users access same file simultaneously
3-Files may take longer to deliver to users 
4-Single site means sowntime in case of system failure  
5-simpler to upload and manage data in single central system 

378-what is lambda and what is use of lambda

>>AWS Lambda is a server-less computing platform that allows engineers to create a small function, configure the function in the AWS console and have the
code executed without the need to provision servers-paying only for the resources used during the execution
>>lambda runs your code high-availbilty compute infrastructure and performs all the administration of your compute resources
>>simply its like an editor(vim,pycharm,sublime text,atom)with some extra features
>>it supports to run differnet languages like python,go,nodejs,java,..etc
>>it is installed or running on amazon linux server and we can access /tmp using lambda function
>>aws lambda needs server it is installed and running amazon linux but your not maintaing thats it
>>lambda execution maximum time is 15mins. maximum file size in lambda is 75gb in a region. maximum thousnd instance in a region

Requirements for AWS Lambda function:-
1-the handler is the entry point for the lambda. the lambda function accepts json formated input and will usaully return the same
2-we usually correlate directly with the language selected to write your function
3-manual trigger or rnby us you can configure a lambda invocation to an event such as upload to s3,dynamodb,API Gateway,AWS Cloudwatch

>>you can stop and start ec2-instance using python boto3 and lambda, Alerting mails if ec2 stop or start and creating EBS snapshots,copy of EBS snapshots

379-differnce between ec2 and lambda

ec2:-
1-ec2 instance how much used you should pay the bill
2-on-demand price. for example your launch one ec2 not using but you should pay the bill
3-install required software
4-user can host the complete application
5-need to update and patches user should maintain

lambda:-What are the two types of AMI?

1-when we runing functions on that time you should pay the bill into lambda service
2-AWS Lambad you pay only what you used . your charges based on the number of requests on your functions and duration
3-selecting only required software or runtime
4-it is just peace of code to host the code
5-no need to update the updates and maintian by aws

380-what is concurrency in lambda

>>concurrency is the number of requests that your function is serving any given time when your function is invoked lambda allocates 
an instance of it to process the event when the function code finishes running it can handler another request

381-what is Boto3

>>Boto3 is python libray for aws to make simplefiy your work 
>>Boto3 makes it easy to integrate your Python application, library, or script with AWS services including Amazon S3, Amazon EC2, Amazon DynamoDB, and more.

382-what is authentication and authriozation

>>authentication your accessing or login into the applicatio or portal entry only
>>authrozation is nothing but you should be assign some roles are features

383-What is an Elastic Transcoder?

>>You can use Amazon Elastic Transcoder to convert video and audio files into supported output formats optimized for playback on desktops, mobile devices, tablets, and televisions

384-what is AWS Service Catalog

>>AWS Service Catalog is like menu what you want give the resources to your users you give that images or cloud-formation templates or anything specfiy the 
role to aws service catalog then it give the permisssions to that user. it will use in big environments like so many developers or there you can specif the role 
to access the particular only one cloud-formation template you can specify that using AWS service catalog

>>if you have multiple accounts you need to share this file into someone then you cna create the service catalog add your file and give that permission 
create product and upload file then create portfolio create . then go the the portfolio add product and then give that permission to user anji 
>>then if anji user login anji console he will get the service catalog under he will have file you can execute that file 

385-what is AWS SSM

>>service manager is  configuration management tool. we can use deployment and like ansible. we need ssm agnet then we can do deployment without ssm agent
we can't do anything. ssm agent is like role add to ec2-instance then we can patches and updates over the deployment and configure the resources

386-what is cognito

>>Amazon Cognito lets you easily add user sign-up and authentication to your mobile and web apps. Amazon Cognito also enables you to authenticate users through an 
external identity provider and provides temporary security credentials to access your app's backend resources in AWS or any service behind Amazon API Gateway

>>you can intregrate the third-party signin and login functionalitys

387-what is OpsWork

>>AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. Chef and Puppet are automation platforms that allow you to use 
code to automate the configurations of your servers.

388-What is Trusted Advisior

>>you can check your helath and security issues and performance and fault tolerance if the account is secured or not all the snapshots taking perfectly 
load balancer and cost utilization and if the coudtrail enable or not you can see everything in trusted advisior
it will show the accout problems or helath of the account 
>>if you enable business plan you created rds high cpu but that much load is not there then it will recomended to cost optimization to decrease the cpu and also show performance and security and fault tolerance nad service limits

389-so one client or manager came and given one aws account check what are the problems have in the account

>>you can go the trusted advisor check what are the helath isses and snapshots going well any performance issues is there and security isses
 you can see 
>>and also you can read and tell the roles in interview also 

390-what is gurad duty in aws 

>>it is used to find the any issues of your aws any credentailas exposed or any isses will give the information and giveing the warning to the aws user to secure the AWS Account
>>Aws guardduty is security service provided by aws the main use case of aws guardduty is security montring service that analysis and process the datasource cloudtrail and s3 logs and who is abousing the credentials from aws that we can easily track using aws guard duty
>>Like vpc flowlogs and cloud trail management logs and dns logs and amazone ebs data rds log and activity who is using root credentials
>>Ip address of particular person and list the malicious ip address if anybody to some an activity like bruteforce and ratelimting like this will also show
>>And you can see the severity patterns like p1 or p2 Or p3

391-what is control tower

>>control tower is used to create setup Set up and govern your multi-account AWS environment and  accounts like create the sub accounts and organizations and creating who are going to 
create aws environment newly then this will help you
>>you can Enable security and compilance  using establishing guardrails which prevent resources from being deployed  that dont conform policies plus you can detect and remediate 
non-compliant accounts and resources as your team provisions them 
>>montior compilance you'll get visual summaries of your AWS environment through dashboard taht lets see your accounts guardrails and compliance status all in one place  

392-How do you set up a system to monitor website metrics in real-time in AWS?

>>Amazon CloudWatch helps you to monitor the application status of various AWS services and custom events. It helps you to monitor:

State changes in Amazon EC2
Auto-scaling lifecycle events
Scheduled events
AWS API calls
Console sign-in events
amazon cloud watch

393-What is whitelisting an IP address?

>>IP whitelisting is when you grant network access only to specific IP addresses. Each employee (or approved user) shares their home IP address with the 
network administrator, who then enters their IP address on a “whitelist” that grants them network access.

394-why AWS block port 25 in outbound ports

>>Amazon Web Services' EC2 instances are throttled on port 25 by default as a spam prevention measure. This can cause connection issues when attempting to use SMTP to relay emails 
through Postmark in your EC2 instance

395-you can block the certain ip and some countries using AWS WAF 

>>Yes its possible to block certain region

396-what is cloudwatch

>>Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, 
and on-premises applications and infrastructure resources and also analyze the logs by using iam role
>>If you want create a alram in cloudwatch go the alarm create metric and then search if you want 'cpu' information or 'memory' then select ec2 or asgs Or rds like this .Then after statics then minimum or maximum then select period 1 minute
>>you can create a dashboard and shared to someone create username and password or you can give public access url link or ssl login also instaed of createing iam role and policy you can directly share that  
 
1-cpu Utilzation
2-Memory utilization
3-harddisk
4-application montoring

2-types of montoring in AWS-Cloudwatch

1-Basic Montoring
>>it comes by default
>>it is free of cost
>>every five minutes logs will be updated 

2-Detailed monitoring
>>you need enable 
>>and its not free of cost
>>every one minute logs will updated 

397-What is an AWS CloudWatch Alarm?

>>An AWS CloudWatch Alarm is a monitoring and management solution for AWS resources. It enables the configuration of alarms based on individual indicators or as part of a larger monitoring plan.
>>you can set the alarm you set sns then it whenever it will trigger you will get an notification
>>you can also billing to the alarm like if we bill some amount then it will trigger 

398-types of alarm in cloudwatch 

1-Ok -The metric is below the threshold
2-Insuffiecnt data -Not enough data available for the metric to determine the alarm state.
3-ALARM — When the metric crosses the threshold.

399-What are Cloudwatch Logs?

>>CloudWatch Logs is a tool that helps us monitor and troubleshoot our systems and applications. It uses our already-established systems, applications, and custom log files. 
In addition, it allows us to monitor our logs in real-time for particular phrases, values, or patterns.

400-How Long Do Cloudwatch Logs Store My Data?

>>Your information will be kept in Cloudwatch Logs for up to 7 days. After that point, only the most current logs will be kept, with older ones automatically deleted to save space.

401-what is the AWS CloudWatch Agent?

>>The AWS CloudWatch Agent is software that can be deployed to any server, whether hosted on Amazon EC2, Amazon ECS, or on-premises. Using the AWS CloudWatch Agent, you can quickly gather 
and send system-level performance information from specific AWS resources.

402-What are three things you can do in CloudWatch?

>>Real-time monitoring is available for AWS resources and apps.
>>Create alerts that will respond automatically to changes in your AWS resources.
>>AWS CloudWatch Logs allows you to see and search log data.

403-What Types Of Cloudwatch Alarms Can Be Created?

>>Any CloudWatch measure can be displayed via an alarm you set up. You can also set off an alert based on metrics specific to your application or system.

404-What Actions Can We Take From a Cloudwatch Alarm?

Share an email with the team.
Schedule a Cron task to run.
Share on Slack or HipChat.
Run a script on this machine, such as one that imports data into a spreadsheet.

405-What Types Of Cloudwatch Alarms Can Be Created?

A warning about a specific resource (e.g., a virtual machine).
A regular notification for a specific metric (e.g., CPU usage).
A user-defined alarm (for example, if your application runs slowly, the alarm will trigger when the CPU reaches 80% and notify you immediately).
A personalized alarm based on user-defined rules.

406-What Statistics Can I View And Graph In Cloudwatch?

>>Disk I/O Activity
>>Memory Share 
>>CPU Usage
>>Memory Usage
>>Network Interface Usage.

407-What is the data retention period in CloudWatch?

>>You can change the log data retention setting for CloudWatch logs. By default, logs are kept indefinitely and never expire. You can adjust the retention policy for each log group, 
keeping the indefinite retention, or choosing a retention period between 10 years and one day.

408-What is CloudWatch events used for?

>>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. 
Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams.

409-How do you set up a system to monitor website metrics in real-time in AWS?

>>Amazon CloudWatch helps you to monitor the application status of various AWS services and custom events. It helps you to monitor:

State changes in Amazon EC2
Auto-scaling lifecycle events
Scheduled events
AWS API calls
Console sign-in events
amazon cloud watch

410-what is ARN

>>Amazon Resource Names (ARNs) uniquely identify AWS resources. We require an ARN when you need to specify a resource unambiguously across all of AWS, 
such as in IAM policies, Amazon Relational Database Service (Amazon RDS) tags, and API calls.

411-what is the schedular and event pattern in events in Cloudwatch

>> Schedular is like you can schedule a certain time
>> Event pattern you can create a pattern and you do whenever the event triggers it will execute

412-what will montior by default 

>>Cloudwatch will se by default cpu and network if you want memory you need to install cloudwatch agent
>>If you want monitor your mem info you can create dashboard in cloudwatch and create cronjobs in linux every second
>>You can use htop for better represention of your cpu memory info
>>If you want monitor the logs in aws cloudwatch install awslogs in your server edit awslogs. Conf file give
>>In aws cloudwatch will monitor for comming container you can use container insights in cloudwatch to monitor the continers

413-how to create the budget to your aws account

>>you can go the billing dashboard click on budgets you can set the monthly your $5000 if its higher than $5000 then you will get an mail 
>>you can do billing alarm in cloudwatch also 

414-what is session manager

>>Session Manager is a fully managed AWS Systems Manager capability that lets you manage your Amazon EC2 instances through an interactive 
one-click browser-based shell or through the AWS CLI. You can use Session Manager to start a session with an instance in your account.
>>if your using private instance need to connect the ec2 using session manager create role for ssm access required then attach that role ec2 
instance .if that server is public you can go to the session manager select the public instance automaticlaly connect to the browser. 
but do same private instance it will not work so create interface endpoint select the required permission like ssm.messages select the vpc 
select the subnets then create .like this you can create ssm.access select the vpc and subnets select the security group and create vpc 
endpoint like this three or four endpoints you need to create. rember its chargeble its not free   
>>go to endpoint create endpoint select ssm.messages select availability zones also see required only dont select any unused subnet it will chargble  
>>go to endpoint create endpoint select ssm.access
>>go to endpoint create endpoint select ec2.message 
>>then go the iam role create ssm permission 
>>now you can go AWS systems manager you will see after the reboot of the system you see and you can connect 

>>systems manager using we can do patching or updating or script execution using run commands so many option is there according you nessecity you can use 
>>first create role for necessary and attach the role to the ec2-instance then you can run the script if your not attaching role it will not work or show the instance  
>>if you have 100 servers you will manage hard use tags . without login to ec2 you can do 

#!/bin/bash
I=0
while [ $I -lt -11 ]
do 
eho "welcome to ssm $I times..."
I=$(( $I + 1 ))
sleep 1
done 

#!/bin/bash
echo "installing docker..."
sudo yum update -y 
sudo yum install docker -y 
sleep 30
sudo service docker start 
sudo docker run --rm -dit --name=container1 -p 8000:80 sreeharshav/rollingupdate:v3

415-using session manager through login ec2instance using browser 

>>go to IAM and click on roles

>>select ec2 and select ssmfull-access and name green-endpoint click on ok

>>go to ec2 instance select vpc green and subnet green-public and iam green-endpoint an launch

>>go to system manager and click on session manager and select the ec2 and start session    -->you will get the shell screen

415-what is aws helath dashboard

>>it will aws service getting any problems and any schedule changes you can see 
>>you can montior your aws account  

416-what is FSx in aws 

>>it is like share folder in your network you can mount this you can assign the mount to windows and you can create folders and assigh anji folder to anji user create gampa folder and assign to gampa folder like you created before windows server users right using that directoy service like this 
>>FSx for Windows File Server resources: file systems, backups, and file shares. The primary resources in Amazon FSx are file systems and backups. A file system is where you store and access your files and folders. A file system is made up of one or more Windows file servers and storage volumes.
>>Maximum storage capacity, SSD and HDD	64 TiB

417-what is directory service in AWS 

>>it is like Active Directory it is managed by AWS you can configure your accounts or users you can manage centralized way or you can configure AWS SSO alSo by using this 

>>for example you createing anji.com in Directory service password anji123 then selected the vpc and two subnets. then you installed windows server installed go the ncpa.cpl select the properties select and click on ipv4 then advance go the DNS add the DNS server this will be given by while we creating the Directory service in aws 
now sysdm.cpl then click on change add anji.com and give password anji123 your system added into active diretory restart the system. now to server manager click on add and remove rules click on next clikc on remote server administration under AD DS and AD LDS tools and click on install . now go the dsa.msc click you annji.com create sampi user and anji user 
click on SSO cliCk on choose identify service click on or select the active directory  select anji.com create . then go the permission set create policy either aws managed or inline policy create. then click on aws organization select you have 10 more accounts example select anji and smapi account click on assign users and add anji user you have created anji user in windows server 
and select the role like gampa also add user and attahc policy . you can pass the url to users then they can access that url give username and password suppose anji login through SSO he will see the 2 accounts then select which account he need they can work like that gampa also 
>>your managing full users using diretory service(AD) through users and permission   

418-service control policy using you can restrict account or users what to access what not acces 

>>create organization then segarte like internal and external i added the internal into two accounts then i add 3 accounts into external i 
created the service control policy per deny IAM and attached to internal if internal account under any user can access IAM he will get an 
error your managing from here 

419-what is patch manager in service manager

>>you can do your systems patches or updates patch manager in aws tool using 
>>not necessary to use this instead you can use anisble or puppet 

420-i want install one package or shell script into the systems without login how we can achive this using aws tools

>>you can use without login into ec2-instance you can AWS SSM systems manager using you can install or execute your package but that system need to require permission to execute attach AWS SSM FULL ACCESS or reuired you cna give attach that IAM role to that ec2-instance 

421-what is eventbridge 

>>A serverless service for building event-driven applications . you can create a rule trigger for any specific other service to trigger either schedule or rule-event pattern like using lambda functions you can use 

422-what is resource group 

>>you can create the resoruce group adding your servers like group adding using tags or then create one group and when you run command in systems manager then you can use that resource group automatically that script will execute this one example whereever accept the resource group you can use that 

423-how to send a logs to nginx loggroup to aws cloudwatch loggroup 

>>you need to install awslogs  then have aws logs permission role 
>>conigure log group name and file where to find the logs 

>>yum install awslogs -y 
>>yum install nginx -y 
>>service restart nginx 
>>servic enable nginx 
>>vi /etc/awslogs/awslogs.conf  

[/var/log/nginx/access]
datetime_format = %b %d %H:%M:%S
file = /var/log/nginx/access.log
buffer_duration = 5000
log_stream_name = {instance_id}
initial_position = start_of_file
log_group_name = /var/log/nginx/access

[/var/log/nginx/error]
datetime_format = %b %d %H:%M:%S
file = /var/log/nginx/error.log
buffer_duration = 5000
log_stream_name = {instance_id}
initial_position = start_of_file
log_group_name = /var/log/nginx/error
>>systemctl start awslogsd.service
>>systemctl status awslogsd.service
>>then you will see the logs in cloudwatch log group 

>>memory montior 
>>yum update -y 
>>yum install amazon-cloudwatch-agent.x86_64 -y
>>sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a stop  -->stop the service 
>>sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a start -->start the service 
>>systemctl status amazon-cloudwatch-agent.service   -->see the status  
>>/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard     -->execute this command it will ask some questions enter the answers 
>>/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s 
>>then go the all metrics see cwsagent click on select machine and create the alarams 

>>if you want to put some stress or any commands 
>>sudo amazon-linux-extras install epel -y 
>>yum install stress -y 
>>sudo stress --cpu 8 --vm-bytes $(awk '/MemAvailable/{printf "%d\n", $2 * 0.9;}' </proc/meminfo)k --vm-keep -m 1  

==============ubuntu===========================

>>wget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb
>>dpkg -i -E ./amazon-cloudwatch-agent.deb
>>apt-get update && apt-get install collectd
>>/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard
>>/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status
>>/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json
>>systemctl status amazon-cloudwatch-agent.service   -->see the status  
================logs=======================
sudo apt-get update -y
sudo apt-get install python
curl https://s3.amazonaws.com/aws-cloudwatch/downloads/latest/awslogs-agent-setup.py -O
sudo python ./awslogs-agent-setup.py --region us-west-2
>>awslogs.conf file content located at /var/awslogs/etc/
>>sudo tail -f /var/log/awslogs.log
sudo service awslogs start
        sudo service awslogs stop
        sudo service awslogs status

cd /etc/systemd/system
vim awslogs.service
[Unit]
Description=Service for CloudWatch Logs agent
After=rc-local.service

[Service]
Type=simple
Restart=always
KillMode=process
TimeoutSec=infinity
PIDFile=/var/awslogs/state/awslogs.pid
ExecStart=/var/awslogs/bin/awslogs-agent-launcher.sh --start --background --pidfile $PIDFILE --user awslogs --chuid awslogs &

[Install]
WantedBy=multi-user.target
:wq
systemctl start awslogs.servicesy
stemctl stop awslogs.service
systemctl restart awslogs.service
systemctl enable awslogs.service
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/create-cloudwatch-agent-configuration-file-wizard.html
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_GettingStarted.html
https://gauravguptacloud.medium.com/aws-cloudwatch-agent-installation-for-memory-metric-integrate-with-grafana-365404154
https://repost.aws/questions/QU7pECA2AcRDWRkRdWiN54tA/awslogs-no-longer-runs-after-upgrading-to-ubuntu-22-04
https://www.rapidspike.com/blog/how-to-send-log-files-to-aws-cloudwatch-ubuntu/
https://www.rapidspike.com/blog/how-to-send-log-files-to-aws-cloudwatch-ubuntu/  -->see how we can configure logs from ubuntu to cloudwatch 
http://www.xavierdilipkumar.com/post/how-to-install-and-configure-the-cloudwatch-logs-agent-on-a-running-ec2-ubuntu-linux-server-instance

424-What is a meaning of archive files?

>>An archive is a collection of data moved to a repository for long-term retention, to be kept separate for compliance reasons or for moving off primary storage media. 
It can include a simple list of files or files organized under a directory or catalog structure, depending on how a particular program supports archiving.

425-what is amazon light-sail

>>Amazon Lightsail is a virtual private server (VPS) provider and is the easiest way to get started with AWS for developers, small businesses, students, and other users 
who need a solution to build and host their applications on cloud.Lightsail provides developers compute, storage, and networking capacity and capabilities to deploy 
and manage websites and web applications in the cloud. Lightsail includes everything you need to launch your project quickly – virtual machines, containers, databases,
CDN, load balancers, DNS management etc. – for a low, predictable monthly price.

426-create ec2 only view and delete the policy for users in AWS

>>To create an AWS Identity and Access Management (IAM) policy that allows a user to view and delete Amazon Elastic Compute Cloud (EC2) instances, you can use the following JSON policy document as a
starting point:

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:Describe*",
                "ec2:List*",
                "ec2:TerminateInstances"
            ],
            "Resource": "*"
        }
    ]
}

427-how to track or see the login into server using bastion server 

>>using bastillion tool 

428-what is bastillion tool what is the use of it 

>>

429-what are tags in what is the use of tags?

>>A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value. For each resource, each tag key must be unique, and each tag key can have only 
one value. You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level.

430-what is aws migration

>>Cloud migration is the process of moving a company's digital assets, services, databases, IT resources, and applications either partially, 
or wholly, into the cloud. Cloud migration is also about moving from one cloud to another.

types of migration
1-rehosting.
2-re-platforming.
3-repurchasing.
4-refactoring.
5-retaining.
6-retiring.

431-what is ECS

>>Amazon Elastic Container Service (ECS) is a highly scalable, high performance container management service that supports Docker containers and allows 
you to easily run applications on a managed cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances.
>>when you creating an ecs cluster it will automatically install the ecs agent in your nodes 

1- tasks:-when any node is down the application will down it will not start another node or any ec2 machine
2-service:-if you using the service if service will down in any node it will move to any other node where is availble it will deploy

432-What is difference between ECS and EC2?

>>The largest difference for EC2 is that it deploys isolated VM instances with auto scaling support, 
and ECS deploys scalable clusters of managed Docker containers. Enterprises can use ECS to scale web applications, perform batch processing, 
and run services in a hybrid environment to deliver better services to users
two services is there

433-what is Fargate in aws

>>AWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers. 
AWS Fargate is compatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).

434-What is the difference between EC2 and Fargate?

>>Amazon EC2 manages or deploys your own EC2 instances to effectively run applications. Fargate AWS allows you to run containers without the need for EC2 instances.
Both are excellent methods for managing or scaling your containers in a dependable manner, but deciding which service to use is always a difficult task.

435-what is AWS API-Gateway

>>Amazon API-Gateway is a fully-managed service that makes it easy for developers to create, publish, maintain, montiro, and secure APIs at any Scale
>>APIs acts as the "front-door" for applications to access data business logic or functionality from your backend services
>>API-Gateway helps us to create a RESTful APIs and Web-Socket APIs that enable real-time two-way communcication

436-where security comes in 

>>when we build the application then it go the qa team and test then it any changes happen then it will take the build then go the security testing and then deployment

what precautions would take the security of your application

>>we can build the feature development then security testing then sends to qa testing and then security testing then releasing the production
>>detecting issues early shortens development time

why security issues comes into production what bases it will come

1-time restrictions:- need to relase the relases within a week without checking the secuity issues deploy application into production
2-hard to get answers:- where the problems hard to find out the issues need to clarify with the developers to take understand the issues
3-unfamiliar with pipelines 

437-what is SNS

>>Amazon Simple Notification Service (Amazon SNS) is a managed service that provides message delivery from publishers to subscribers (also known as producers and 
consumers). Publishers communicate asynchronously with subscribers by sending messages to a topic, which is a logical access point and communication channel

438-what is Simple Queue service

>>Amazon Simple Queue Service (SQS) lets you send, store, and receive messages between software components at any volume, 
without losing messages or requiring other services to be available.

439-what is first in first out

>>FIFO (First-In-First-Out) queues have all the capabilities of the standard queues, but are designed to enhance messaging between applications
when the order of operations and events is critical, or where duplicates can't be tolerated.
>>For FIFO queues, there can be a maximum of 20,000 in flight messages (received from a queue by a consumer, but not yet deleted from the queue). 
If you reach this quota, Amazon SQS returns no error messages. A queue name can have up to 80 characters

440-Your business prefers to use its email address and domain to send and receive compliance emails. What service do you recommend to implement it easily 
and budget-friendly?

>>This can be accomplished by using Amazon Simple Email Service (Amazon SES), a cloud-based   email-sending service.

441-Describe SES.

>>Amazon offers the Simple Email Service (SES) service, which allows you to send bulk emails to customers swiftly at a minimal cost.
>>it will everyday 50000 emails every day like 24 hours. 14 emails per second 

442-what is docker-ECR

>>Amazon Elastic Container Registry (Amazon ECR) is an AWS managed container image registry service that is secure, scalable, and reliable. Amazon ECR supports 
private repositories with resource-based permissions using AWS IAM
>>aws ecr get-login-password --region <your-region> | docker login --username AWS --password-stdin <your-account-id>.dkr.ecr.<your-region>.amazonaws.com
>>docker pull <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/<repository-name>:<tag>
>>docker build -t <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/<repository-name>:<tag> .
>>docker push <your-account-id>.dkr.ecr.<your-region>.amazonaws.com/<repository-name>:<tag>

443-What is Inspector in AWS

>>Amazon Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure.
>>amazon inspector will use only for ec2 instance only. for example you created infrastructure you handover to client how will he will belive . then you can use amazon inspector this will scan ec2 or infrastructure then give any known vulnerlibilites or known issues 
>>you can scan 
1-network reachability 
2-security best practices
3-common vulnerabilites and exposures 
4-CIS operating system security

444-What are the two types of AMI?

>>AMI is divided into two categories:
1-EBS-backed Instances.
2-Instance Store-backed Instances.

445-what is the full of form WAR file

>>Web Archive File 

446-while installing any package its not installing what will be the problem

>>you have privellges or not

447-what is CORS and SOP

--sop->same origin policy
>>The same-origin policy is a browser security mechanism that restricts how a
document or script loaded by one origin can interact with a resource from another
origin.
It helps isolate potentially malicious documents, reducing possible attack vectors
>> The browser will implement the SOP in all browsers beacuse of security
>> This implementation happiness we can't see the data of your own subdomain also
>>it checks the three objects
>>protocal >https:// (or) http://
>>site >google (or) facebook
>>port number >80 (or) 443
--cors->cros origin resource policy
>>Cross-Origin Resource Sharing (CORS) is an HTTP-header based mechanism that
allows a server to indicate any origins (domain, scheme, or port)
other than its own from which a browser should permit loading resources.
Acess-control-allow-origin==>true,get,post,put,delete
>>the sop implmented in browser policy we cant read the data in your subdomain of
the same domian.
if we can use the cors we can read write the data from main domain to subdomain
>http://www.google.com
>http://admin.google.com

448-What are the tools and techniques that you can use in AWS to identify if you are paying more than you should be, and how to correct it?

Check the Top Services Table
It is a dashboard in the cost management console that shows you the top five most used services. This will let you know how much money you are 
spending on the resources in question.Cost Explorer
There are cost explorer services available that will help you to view and analyze your usage costs for the last 13 months. You can also get a cost
forecast for the upcoming three months.AWS Budgets
This allows you to plan a budget for the services. Also, it will enable you to check if the current plan meets your budget and the details of how you 
use the services. Cost Allocation Tags
This helps in identifying the resource that has cost more in a particular month. It lets you organize your resources and cost allocation tags to keep 
track of your AWS costs.

449-What are the meaning of different error codes such as 202, 404, 501

>> 100=continue (or) Informational
2XX-->>Success
>> 200=ok
>> 201=created
>> 202=Acepted
>> 203 Non-Authoritative Information
>> 204=Delete or no content
3XX-->>Redirects
>> 301=moved permanently
>> 302=found
>> 304=Not modified
4XX-->>Client side derror
>> 400=Bad request
>> 401=unauthorized
>> 402=payment required
>> 403=forbideen=>not allowed and is similar to 401
>> 404=not found
>> 409=conflict
5XX-Server side errors
>> 500=internal server error
>> 501=not implemented
>> 502=bad gateway
>> 503= service unavailable
>> 504=gateway timeout
>> 509=network timeout

450-what is the advantages of using API-Gateway 

>>using API-Gateway in micro-services or monolithic application the request will come api-gateway like authentication or authorization it will fetch the data using APIS 
>>it will also store or maintain the cache mechnisam the performance will be improved latency will reduced 
>>it will cpature the metricss like
1-it will capture the how many API-call will recieved 
2-how much latency will caused between the request and responce 
3-how many call is failed 
4-which backend is not responding using APIS

451-how to send a nginx docker container logs to AWS cloud watch if you use ECS also  

1-Install the AWS CloudWatch Logs Agent: First, you need to install the AWS CloudWatch Logs agent on your EC2 instance. This agent will be responsible for streaming logs from your Docker containers to
CloudWatch.
2-Create an IAM Role: Ensure your EC2 instance has an IAM role that grants permissions to write logs to CloudWatch Logs. Attach the role to the EC2 instance where your Docker containers are running.
3-Configure Docker Logging Driver: To send Docker container logs to AWS CloudWatch, you need to configure Docker to use the awslogs logging driver. You can do this by adding the --log-driver and --log-opt
options 
when starting a container.

For example:
>>docker run -d --name nginx-container --log-driver=awslogs --log-opt awslogs-region=your-region --log-opt awslogs-group=your-log-group nginx
>>Replace your region with your AWS region and your log group with the name of the CloudWatch Log Group you want to send logs to.

4-Configure the AWS CloudWatch Agent: The AWS CloudWatch Logs agent should be configured to watch the log files generated by the Docker containers and send them to CloudWatch.

>>Edit the CloudWatch agent configuration file, typically located at /etc/awslogs/awslogs.conf, and add an entry for each log stream you want to send. For Nginx, the log path 
might be /var/log/nginx/access.log and /var/log/nginx/error.log. An example entry might look like:

[nginx-access-logs]
file = /var/log/nginx/access.log
log_group_name = your-log-group
log_stream_name = nginx-access
datetime_format = %d/%b/%Y:%H:%M:%S %z
Make sure to adjust the file, log_group_name, and log_stream_name to match your specific setup.

5-Restart the CloudWatch Agent: After editing the configuration file, restart the AWS CloudWatch Logs agent to apply the changes:

sudo service AWS-logs restart

6-View Logs in CloudWatch Console: Your Nginx container logs should now be streaming to the specified CloudWatch Log Group. You can view and analyze the logs from the AWS CloudWatch Console.

Please note that this is a high-level overview, and you may need to adjust the configuration based on your specific requirements and log file paths. Make sure to follow AWS security best practices 
when setting up log forwarding to CloudWatch.

ECS using:-
1-Update Task Definition:
>>In your ECS task definition, you need to specify that you want to use the awslogs logging driver for the Nginx container.
>>For example, in your task definition JSON or in the AWS Management Console, you can add a log configuration section for the container:

JSON
Copy code
"logConfiguration": {
  "logDriver": "awslogs",
  "options": {
    "awslogs-group": "your-log-group",
    "awslogs-region": "your-region",
    "awslogs-stream-prefix": "nginx-container"
  }
}
Replace your log group with the name of the CloudWatch Log Group you want to send logs to, and your region with your AWS region.
2-Create an IAM Role:-Ensure that your ECS task definition uses an IAM role with the necessary permissions to write logs to CloudWatch Logs. The IAM role should have a policy allowing logs: CreateLogStream,
logs: PutLogEvents, and logs: CreateLogGroup actions on the specified log group.
3-Configure the AWS CloudWatch Agent: You'll need to install and configure the AWS CloudWatch Logs agent on the EC2 instances that run your ECS tasks. The agent will be responsible for streaming the 
container logs to CloudWatch.
4-Run ECS Tasks:-Launch your ECS tasks as usual. Ensure that they use the updated task definition that specifies the AWS logs logging driver for the Nginx container.
5-View Logs in CloudWatch Console: Your Nginx container logs should now be streaming to the specified CloudWatch Log Group. You can view and analyze the logs from the AWS CloudWatch Console.
>>Please note that this approach is for ECS tasks running on EC2 instances. If you are using AWS Fargate, the setup may be slightly different, as Fargate abstracts the underlying infrastructure. 
Ensure that the AWS CloudWatch Logs agent is properly configured and running on the EC2 instances that host your ECS tasks.

452-what is the parameter store

>>AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, 
and license codes as parameter values.
>>this will not rotate your password but the Sceret manager will do
>> Some kind of the features are not available in comparison to the secret manager 

453-an user, only he can terminate the ec2 and start only creating iam policy and full access code

Start&Terminate:-
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ec2:TerminateInstances",
        "ec2:StartInstances"
      ],
      "Resource": "arn:aws:ec2:region:account-id:instance/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "iam:CreatePolicy"
      ],
      "Resource": "*"
    }
  ]
}
Full-Access:-
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "ec2:*",
      "Resource": "*"
    }
  ]
}

454-what is client-server explain day to day also

>>Who is server the who is asking for some information for someone that means your asking information from your friend your friend is server your client
>>Who is client asking information from someone like your asking details from your friend here you is client your friend is server

455-what is physical servers

hardware-->operating-systems-->application 
--Problams
1-Take time to boot 
2-physical server is down no backup until you create extra server 
3-need to configure everything 

1-vm is faster to boot 
2-if the server is down we can configure next one is easy 
3-no need configure everything

456-what is Caching 

>>Caching is nothing but storing some data in your cache server dont hit every request on the real webserver any db-server 

457-what is TTL in Route53 

>>The time for which a DNS resolver caches a response is set by a value called the time to live (TTL) associated with every record. Amazon Route 53 does not have a default TTL for any record type.
>> It is like cache your request on the server by default 300 seconds is there you reduce how much you want 
>>Best practice 
1-Short TTLs (e.g., 300 seconds or 5 minutes):
2-Medium TTLs (e.g., 3600 seconds or 1 hour):
3-Long TTLs (e.g., 86400 seconds or 1 day, or even longer):

458-what is CRUD

>>In computer programming, create, read, update, and delete are the four basic operations of persistent storage. CRUD is also sometimes used to describe user interface conventions that
facilitate viewing, searching, and changing information using computer-based forms and reports.

C    -->Create
R    -->Read
U    -->Update
D    -->Delete

459-what is HTTP Methods in Web-Application

GET:-GET Request is used to request information from a resource such as a website or server 
POST:-It creates a new resource on the backend(server)we can send data to the server in the request body 
PUT:-IT will update the name or any details in the server or appliation
PATCH:-it will work same put,patch the updates of resource it updates data in server or application
DELETE:-it will delete the data in server or application
HEAD:-the HEAD method is similar to GET method but its doesn't have any response body 
OPTIONS:-this method is used to get information about the possible communcation option for the give URL or asterisk 
TRACE:-the trace method is for diagnosis purposes it createa loop-back test with the same request body that the client sent to the server before 
CONNECT:-the connection method is for making end to end connections between client to server     

460-how to configure one domain www.anji.com to another sales.anji.com its not a subdomain sub-sub-domain

>>you have two aws accounts for example 1st account anji.com 2nd account sales that domain need add this 1st why because takenover by my company how we achive this 
>>first create the www.anji.com in 1st account then name server you are pasting godday or hostinger right then go the second create sales.anji.com this will give you name servers then paste that 
servers into 1st account like this create subdomain sales.anji.com select the NS record paste that 2nd account NS record into 1st account then automatically name resloution will happen then you can create
the subdomain 2nd account like 
sampi.sales.anji.com in 2nd account only . like that you can you can AZURE also    

461-what is the difference between SQL or NO-SQL 

SQL:-sequl database it means store the data rows and columns data only 
NO-SQL:-non-relastional database it will store rows and columns and more than that like images or videos 

462-what is rabbitmq 

>>RabbitMQ is an open-source message-broker software that originally implemented the Advanced Message Queuing Protocol and has since been extended with a plug-in architecture to support Streaming 
Text Oriented Messaging Protocol, MQ Telemetry Transport, and other protocols
>>RabbitMQ is a messaging Queue which is used by some components of the applications.
>>port number of rabbitmq 5672 
>>for example take Whatsapp you send message to your friend how the message will go your friend in the middle their is message streaming platform like rabbitmq or kafka like your friend number always
pull the request from rabbitmq your pushing rabbitmq then it will send to friend and their is Queue option if your not online it will be on Queuewhenever it will be online then that messages will be pulled
you from your rabbitmq  
>>your publisher your-friend subscriber to buy something the for ecomerce for example only 

463-use case of rabbitmq

1-Messaging System
2-Activity Tracking
3-Gather metrics from many differnet locations
4-Application logs gathering
5-Stream processing(with the kafka Streams API fro example)
6-De-coupling of system dependencies
7-Integration with spark, Flink, Storm, Hadoop, and many other Big Data technologies
8-it uses microservice public

464-how to install specific version of nginx or httpd in linux 

>>sudo apt install nginx=1.16.* 
see video of cloudwithsiva video no 10

465-what is the difference between public and private nat-gateway in aws

public-nat-gateway:-

>>Use Case: A public NAT gateway is typically used in a public subnet. Instances in public subnets can have public IP addresses and are intended for resources that need to be directly accessible from 
the internet. Public NAT gateways allow these instances to initiate outbound traffic to the internet while also providing a level of security.
>>Configuration: To set up a public NAT gateway, it's placed in a public subnet and associated with an Elastic IP address. The route tables for private subnets are configured to route outbound traffic
to the public NAT gateway. Instances in private subnets send their internet-bound traffic to the public NAT gateway, which then performs Network Address Translation (NAT) to make the traffic appear as
if it's coming from the public NAT gateway's Elastic IP address.
>>Security: A public NAT gateway is typically used when you want your private instances to have limited inbound traffic from the internet. In this configuration, the security groups for the private 
instances should be configured to allow only the necessary outbound traffic.

private-nat-gateway:-

>>Use Case: A private NAT gateway is used in a private subnet and is designed for resources that should not have direct public internet access. Instead, private NAT gateways are used for outbound traffic
from private instances that need to access the internet, such as for software updates or retrieving external data. This approach provides enhanced security for the private instances.
>>Configuration: Similar to a public NAT gateway, a private NAT gateway is placed in a public subnet, but it doesn't have a direct public IP address. Instead, it routes traffic to the internet through a 
Network Address Translation (NAT) gateway service managed by AWS. The route tables for private subnets are configured to route outbound traffic to the private NAT gateway. The instances in private subnets
send their internet-bound traffic to the private NAT gateway, which then routes it to the AWS NAT gateway.
>>Security: Private NAT gateways provide a higher level of security for private instances. Since they don't have a public IP address, they can't receive direct inbound traffic from the internet. 
This configuration helps to protect sensitive or critical resources.

466-what is mean by database sharding 

>>sharding helps you to keeping data is highly available
>>you have two mongo primary servers then secondary replicas each primary that will help you to make highly availble
>>video 54 01-03-00 time 
>>videos 58 32 mins 

467-What is the maximum number of instances allowed in an Amazon DocumentDB cluster?

>>Instances	40
>>Manual cluster snapshots	100
>>Read replicas per cluster	15
>>Subnet groups	50

468-what is placement group in ec2 

>>if you want install all the instance in single availability zone you can use placement group while launching ec2 instance.
 but its not recomended why because if anything happens to the availability zone you will interput or face issues 

469-what is desire-capacity in Auto-scaling group 

>>how many system do you want to launch instances in AWS 

470-how role and policy works 

>>first create the role which service like ec2 then attach that poilcy to ec2 then create the policy permission like ssm parameter permission then attach this policy to role then that role attached 
to ec2 will bring the data from aws what permission you given 

471-how to replace any server ip in route53 using shell script 

IP=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=jenkins"  --query 'Reservations[*].Instances[*].PublicIpAddress' --output text)

echo '
{
  "Comment": "CREATE/DELETE/UPSERT a record ",
  "Changes": [{
    "Action": "UPSERT",
    "ResourceRecordSet": {
      "Name": "jenkins.devopsb70.online",
      "Type": "A",
      "TTL": 15,
      "ResourceRecords": [{ "Value": "IPADDRESS"}]
    }}]
}' | sed -e "s/IPADDRESS/${IP}/" >/tmp/jenkins.json

ZONE_ID="Z0366464237Z7LZLZPKFA"
aws route53 change-resource-record-sets --hosted-zone-id ${ZONE_ID} --change-batch file:///tmp/jenkins.json | jq .

472-is document db and mongodb is same 

>>documnet db is mongodb alternative in aws 

473-how we can new servers ip address into prometheus server 

1-Create a CloudWatch Events Rule:

Open the AWS Management Console.
Navigate to the CloudWatch service.
In the left sidebar, click on "Rules" under "Events" and then click the "Create rule" button.
Select "Event Source" as "Event Source Type," and choose "EC2" under "Service Name."
Select the specific event(s) you want to trigger the Lambda function (e.g., "EC2 Instance Launch Successful").
Under "Targets," choose "Lambda function" and select your Lambda function.
Click "Configure details," provide a rule name and description, and click "Create rule."

2-Modify Lambda Function for CloudWatch Event:
>>Update the Lambda function to handle the CloudWatch event and extract the necessary information.
import boto3
import paramiko
import os

def lambda_handler(event, context):
    # Extract the instance ID from the CloudWatch event
    instance_id = event['detail']['EC2InstanceId']

    # Use the EC2 client to get information about the new instance
    client = boto3.client('ec2')
    instance_info = client.describe_instances(InstanceIds=[instance_id])['Reservations'][0]['Instances'][0]
    ip_address = instance_info['PublicIpAddress']

    # Update prometheus.yml with the new target
    with paramiko.SSHClient() as ssh:
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(hostname='prometheus_server_ip', username='user', key_filename='/path/to/key.pem')
        sftp = ssh.open_sftp()
        with sftp.open('/path/to/prometheus.yml', 'a') as f:
            f.write('- {target: "%s:9100"}\n' % ip_address)

    # Reload Prometheus configuration (replace with your method)
    os.system('curl -X POST http://prometheus_server_ip:9090/-/reload')

3-IAM Role Permissions:
>>Ensure that the IAM role associated with your Lambda function has the necessary permissions to describe EC2 instances and execute SSH commands.
4-Lambda Trigger Permissions:
>>If you are using the Lambda console to create the CloudWatch Events trigger, make sure to configure the necessary permissions for CloudWatch Events to invoke your Lambda function.

like this any instance is stoped remove the IP from prometheus server 

import boto3
import paramiko
import os

def lambda_handler(event, context):
    # Extract the instance ID and lifecycle event from the CloudWatch event
    instance_id = event['detail']['EC2InstanceId']
    lifecycle_event = event['detail']['LifecycleTransition']

    if lifecycle_event == 'autoscaling:EC2_INSTANCE_LAUNCHING':
        # Use the EC2 client to get information about the new instance
        client = boto3.client('ec2')
        instance_info = client.describe_instances(InstanceIds=[instance_id])['Reservations'][0]['Instances'][0]
        ip_address = instance_info.get('PublicIpAddress')

        if ip_address:
            # Update prometheus.yml with the new target
            with paramiko.SSHClient() as ssh:
                ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
                ssh.connect(hostname='prometheus_server_ip', username='user', key_filename='/path/to/key.pem')
                sftp = ssh.open_sftp()
                with sftp.open('/path/to/prometheus.yml', 'a') as f:
                    f.write('- {target: "%s:9100"}\n' % ip_address)

    elif lifecycle_event == 'autoscaling:EC2_INSTANCE_TERMINATING':
        # Remove the target from prometheus.yml
        with paramiko.SSHClient() as ssh:
            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            ssh.connect(hostname='prometheus_server_ip', username='user', key_filename='/path/to/key.pem')
            sftp = ssh.open_sftp()
            with sftp.open('/path/to/prometheus.yml', 'r+') as f:
                lines = f.readlines()
                f.seek(0)
                for line in lines:
                    if not line.startswith(f'- {{target: "{ip_address}:9100"}}'):
                        f.write(line)
                f.truncate()

        # Reload Prometheus configuration (replace with your method)
        os.system('curl -X POST http://prometheus_server_ip:9090/-/reload')

474-how to configure AWS SSO to jenkins 

>>for example you have installed jenkins then go the SSO select the saml2.0 and select jenkins download saml metadata gievn by aws sso 
then give application metadata giev jenkins url http://44.220.161.15:8080/securityRealm/finishLogin second box also same give user who want acess this anji 
and mail-id and click on ok you will get mail and confirm that you will get aws sso url then you can access the jenkins before accessing jenkins or ssl go the jenkins install SAMl plugin and upload the you download saml file in jenkins then 
then go the aws sso application click on actions edit attributes mapping 
add $(user:preferredUsername)
$(user:emails)
${user:AD_GUID}
you can access the jenkins using aws sso    
 
475-while iam creating sns topic i given wrong mail how to delete that topic 

>>you can't delete that one if your not accepted before 72 hours it will automatically delete that 

476-best practice for reduce cloud cost 

1-aviod data charges -->cross az charges are there if you need high availability the use or if your testing then use only single availability zone to avoid data charges  
2-switch off dev resource in off business hours  
2-dont use public ips if you public ip use private ips only the data will transmit or travel availability_zone to data center then come back so sont use public IPs 
4-use private link gateways or gateway endpoints 
5-effectively select instance types 
6-AWS Cost management then see the what your not using then avoid those then that will suggestion but dont take blindly observe one or two month then take the right instance 
7-Identify Amazon EC2 instances with low-utilization and reduce cost by stopping or rightsizing
8-delete unused EBS volumes 
9-Analyze Amazon S3 usage and reduce cost by leveraging lower cost storage tiers
10-Review networking and reduce costs by deleting idle load balancers
11-Use Reserved Instances (RI) to reduce RDS, Redshift, ElastiCache and Elasticsearch costs
12-aws cost optmizer use that this will give you suggestion under provision or over provision 
13-implement uto-scaling to avoid to use number instance un-necessary
14-dont take blindly recomendedation on cloud beacuse we faced some issue we used to montior the application we used 5 servers then aws recomended 2 servers then we implemented then we face some issue they dont which server or application running they dont know 
check the load test then conform then you can analyse an implement
15-remove unnesaary snapshot maintain retention period 
16-aviod or elastic ips if dont required   

477-Your using AWS cloud but suddenly the provider has changed the charges has high what will you do 

1-Assess the Situation: The first step would be to thoroughly analyze the extent of the price increase and its impact on our current AWS usage. This involves reviewing billing statements, understanding the new pricing structure, and estimating the potential financial impact on our budget.
2-Evaluate Alternatives: I would then explore alternative cloud service providers and compare their pricing structures, features, and performance with AWS. This could include providers like Microsoft Azure, Google Cloud Platform, or others. Additionally, I would assess the feasibility of leveraging hybrid cloud solutions or even on-premises infrastructure for certain workloads.
3-Optimize Resource Usage: I would review our current AWS infrastructure and identify opportunities to optimize resource usage and eliminate any inefficiencies. This might involve rightsizing instances, implementing auto-scaling policies, optimizing storage solutions, or utilizing reserved instances or savings plans to reduce costs.
4-Negotiate with AWS: Depending on the nature of our relationship with AWS and the scale of our usage, I would consider reaching out to our AWS account manager to discuss the price increase and negotiate for potential discounts or credits. AWS often values long-term relationships with their customers and may be willing to offer concessions to retain business.
5-Implement Cost Controls: Implementing stricter cost controls and budgeting measures within our organization would be essential to prevent future cost overruns. This could involve setting up billing alerts, establishing spending limits, and enforcing resource usage policies across teams.
6-Diversify Cloud Strategy: To mitigate reliance on a single cloud provider and minimize the impact of future price fluctuations, I would advocate for a diversified cloud strategy. This could involve multi-cloud deployments, where different workloads are distributed across multiple cloud platforms based on their specific requirements and cost considerations.
7-Monitor and Adapt: Continuously monitoring our cloud usage and staying informed about pricing changes and industry trends would be crucial. This allows us to adapt our strategy proactively and make informed decisions to optimize costs while ensuring the performance and scalability of our infrastructure.

see azure subscription in shared or reserve instance in 1:41 resume prepartion cloudwithshive 

>>shared reserved instance like you reserved instance to anji subscription then if you can share that instance to sampi subscription also possible 

478-Is node.js will supoort backend and frontend

>> Yes node.js will support frontend and backend also
3 tier arch
database:- MongoDB Postgres or MySQL
frontend-web-application:- nodejs (webapp)
backend-end-application:-  NODEJS (app-servers) or java

479-what is SAST

>>SAST stands for Static application security testing and its one of the easier types of security testing to implement in an existing or a new DevOps or DevSecOps pipeline
>>SAST tools run somewhere between the code and build stage, may be it'll be included in the test satge, but typically it's run pretty close to the code 
>>SAST we're thinking of statically testing the application. SAST is also known as Source Code Analysis 
>>it is white-box testing it cheks all the packages in your code it has 
>>it may false reports also some time
>>less expensive to fix the vulnerabilites 
>>it check differnet authentication patterns and scan config files potential security issues 
Tools like:-
1-sonarqube
2-Fortify.
3-Horusec
4-Snyk
5-Insider
6-HuskyCI

480-what is DAST

>>DAST stands for Dynamic application security testing. we check the application while running application for an issues . we are not loooking code directly
>>DAST is block box testing the tester dont know anything try to break the code or application logics 
>>more expensive to fix the vulnerabilites
>>we can discover run-time environment related issues
Tools:-
1-PortSwigger Burp-Suite
2-OWASP-ZAP
3-Veracode
4-Nuclei
5-HCL AppScan

481-what is Fortify security tool

>>Fortify offers the most comprehensive static and dynamic application security testing technologies, along with runtime application monitoring and protection, backed by indus try-leading\
security research. Solutions can be deployed in-house or as a managed service to build a scalable, nimble Software Security Assurance program that meets the evolving needs of today’s
IT organization.

482-what is code smell

>>Code smells indicate a deeper problem, but as the name suggests, they are sniffable or quick to spot. The best smell is something easy to find but 
will lead to an interesting problem, like classes with data and no behavior.

483-what JSON (JavaScript Object Notation)

>>JSON stands for JavaScript Object Notation . JSON is a lightweight format for storing and transporting data
(e.g., sending some data from the server to the client, so it can be displayed on a web page, or vice versa).

{
"Name": "Anji",
"Age": "26",
"DOB": "01-01-1996",
"Address": [
    {
   	"Type": "permenanent",
	"DoorNo": "1-80/1",
	"Address-line": "ganesh nagar"
     },
     {
	"Type": "current",
	"DoorNo": "1/90",
	"Address-line": "neknampur"
      }
   ] 
}

484-what is YAML

>> Yaml is yet another markup languge.YAML is a human-readable data-serialization language
>>YAML is a human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted
>>xml,json,yaml are DTO  -->data transfer object

first type(key: value or list)
Name: Anji
Age: 26
DOB: 01-07-1996
Address:
- present: VDk
  street: ganesh nagar
  city: peddapelli
- current: hyd
  street: neknampur
  city: hyd 

==second type(map type)
Name: Anji
Age: 26
DOB: 01-07-1996
Address:
  perm-address:
    present: VDk
    street: ganesh nagar
    city: peddapelli
  current-address:
    present: VDk
    street: ganesh nagar
    city: peddapelli

485-what is XML

>>XML stands for eXtensible Markup Language. XML is a markup language much like HTML
>>XML was designed to store and transport data. XML was designed to be self-descriptive

ex:-
<person>
	<Name>anji</Name>
	<Age>26</Age>
	<DOB>01-07-1996</DOB>
	<Address>
		<H-No>1-80/1</H-No>
		<City>VDK</VDK>
	</Address>
</person>

486-what are code standards and code best practice

code standards:-
1-code should Never be kept in local(Laptop/Desktop)
2-always try to push code to central like (github)
3-code should developed in local only 
4-always choose editors as per your comfort
5-editors always improve productivity 
best practice:-
1-code should never hardcode the username and passwords 
we can passthat password along with the server system only like export root_mysql_password 
sudo -E anji.sh  ##this anji.sh has written code if [ -z "${root_mysql_password}" ]; then
  echo "Variable root_mysql_password is missing"
  exit 1
fi  ##this root password has given some other command has variable
mysql_secure_installation --set-root-pass ${root_mysql_password}  ##like this this password will be taken direcctly from the server it self because we written environment variables if not will fail   
2-code DRY vs WET, Always try to make code DRY 
3-rerun of automation should never fail 

487-what are the strategies to follow cost optimization in cloud

>>Cost optimization in cloud strategies for databases and instances is a crucial aspect of managing your cloud expenditures. 
There are several strategies you can implement to achieve significant cost savings without compromising performance or availability. Here are some key areas to focus on:

>>Right-sizing instances: Analyze your database workload and choose the appropriate instance type and size. Don't over-provision resources you won't utilize, as you'll be paying for unused capacity.
>>Utilize reserved instances: If your workload has predictable usage patterns, consider buying reserved instances for a significant discount. You can commit to a specific instance type and duration
for a pre-defined price.
>>Leverage Spot Instances: For non-critical workloads, look into using Spot instances, which are unused cloud resources offered at a significantly lower price. However, be prepared for potential 
interruptions if demand increases.
>>Automate scaling: Implement autoscaling mechanisms to automatically adjust resources based on real-time demand. This will prevent resource underutilization and reduce costs during low-traffic periods.
>>Stop idle resources: Shut down database instances or applications when they're not in use, especially for development or testing environments.
>>Utilize cloud cost management tools: Most cloud providers offer built-in or third-party tools that provide detailed cost insights, identify optimization opportunities, and set budgets for cost control.
>>Monitor and analyze usage: Regularly monitor your cloud resource usage and analyze cost trends to identify areas for improvement. Set up alerts for anomalies or unexpected spikes in spending.

488-could you please explain your environemnt 

>>we follow the aws best practices we follow the hub and spoke architecture for our environment the hub which is called as management vpc have
all the shared resoures combining with bastion nat logging appliances montoring everything and each production vpc or application vpc is 
hosting only the work loads of the application like databases application servers and webservers there is nothing related management hosted on
the production vpcs all the management related will be deployed in management VPCs this is only if you use aws 

489-what is CSPM 

>>Cloud Security Posture Management is the process of Identifying misconfigurations and compliance issues with cloud 
>>whatever the configurations that are security engineer or devops engineer might be settingup with cloud infrastructure there can be issue can configuration we are bound to follow secuirty compilance like hippa or PCI-DSS tools like AQua it will help to find the issues in any cloud like AWS, AZURE, GCLOUD 

490-what is BridgeCrew tool 

>>BridgeCrew is IAC tool to find vulns in infrastructure to secure infrastructure as you build it 

491-what is AWS Audit manager 

>>video 11

492-create alarm slect the metric cpu utilization in auto-scaling i have already created anji auto-scale then select the statistic is minimum period is 1 minute when  name higher cpu lowerthen 40% and select the in alarm and select the sns topic anji click on ok craete alaram name anji and one more alarcm name is lower and is cpu higherthen 40% 
craete a vpc with 10.1.0.0./16 and then craete two publi subsnets and two private subnets then craete internete gateway and attach to public subnets then create nat gateway attach private subnets then create auto-sclaing name anji-scale craete launch template install nginx min =2 max= 10 and create dymanic scaling policy under step scaling if the cpu utilization wil go 40% up then add 3 server using higher alarm and  if the trafic goes down below 40% remove three insatance using lower alaram using this all terrafrom 

493-windows useful commands 

firewall.cpl 
ncpa.cpl 
mstsc 
service.msc
appwiz.cpl 
sysdm.cpl 
dsa.msc 
dnsmgmt.msc 
lusrmgr.msc

494-former2 extenstion will help you to create the cloudformation

495-What is Nginx?

>>Nginx is a type of open-source web server that is especially used for reverse proxy, load balancer, mail proxy, and for the HTTP cache. 
Nowadays NGINX software is most commonly used by various companies. NGINX can run on Linux, Mac OS X, Solaris, AIX, HP-UX, and the BSD variants.
>>Nginx is an open supply web server and an opposite proxy server for HTTP, SMTP, POP3, and IMAP protocols with robust attention on excessive concurrency,
performance, and low reminiscence utilization. It is stated as “engine x”.

496-use case of nginx web-server 

>>it is webserver and it is used for reverese proxy for webserver. and load-balancing the request and email proxy and http cache service 
>>it serves the static content 
>>you can setup reverse proxy videos in cloudwithsiva video10 

497-What is the default root folder nginx?

>>The way nginx and its modules work is determined in the configuration file. By default, the configuration file is named nginx. conf and placed in the
directory /usr/local/nginx/conf, /etc/nginx, or /usr/local/etc/nginx 

498-Can we deploy the Node.js application on the Tomcat server?

>>You can run Nodejs such as react or angular on JavaPipe's Tomcat service. You will need to use the npm run build to package the Nodejs for production.
It creates a build folder where you can package it as a war file. cd into the build directory and then run jar cvf webui.

499--what is the node.js build command

>>npm install 

500-where I can see the logs of nginx server or Tomcat server

>>By default, the access log is located at /var/log/nginx/access. log
>>You can override the default settings and change the format of logged messages by editing the NGINX configuration file ( /etc/nginx/nginx.)
>>The main log is in /opt/tomcat/logs/catalina. <date>. log 

501-In what language was the Nginx software being written?

>>The language in which the Nginx software is written is ‘C’ Language.

502-What are the main tasks of the Nginx web server?

>>The main task of the Nginx web server is to deploy dynamic HTTP content on a network using SCGI, Fast CGI handlers for scripts, 
WSGI application servers or Phusion passenger module.
>>Nginx is also used to serve as a load balancer.

503-What are the difference between Nginx and Apache?

Firstly, Nginx is an event-based web server and Apache is a process-based server.
Nginx is best known for its speed and Apache is best known for its power.
Nginx is the best when it comes to memory consumption and connection whereas Apache is not best in this category.
In Nginx, a single thread is handling all of the requests whereas in Apache single thread handles a single request.
Nginx is best when you want the load balancing. But Apache will refuse the new connection when traffic reaches the limit of the process.
Apache provides lots of functionality as compared to Nginx.

>>A web server is a software application that delivers web content, such as web pages, to client computers that request it via the Internet. The server receives and processes HTTP requests
from clients and responds with HTML documents or other types of content that the client’s web browser can display.
>>Apache and Nginx are two popular web server software applications that are used to serve web content. Both are open-source and widely used in production environments. 
While both can serve static and dynamic content, there are some differences between them.
>>Apache is a very popular web server that has been around since the mid-1990s. It is known for its flexibility and ease of use, and it supports a wide range of modules and plugins
that can be used to add functionality. Apache is also widely used on shared hosting environments, as it can be easily configured to serve multiple websites from the same server.
>>Nginx, on the other hand, is a newer web server that was first released in 2004. It is known for its high performance and scalability, and it is often used to serve high-traffic websites.
Nginx uses an event-driven architecture that allows it to handle a large number of requests with low resource usage. It is also commonly used as a reverse proxy and load balancer, 
in addition to its role as a web server.
>>In terms of working mechanisms, both Apache and Nginx work by listening on a specific port for incoming requests. When a request comes in, the server processes it and generates a response,
which is sent back to the client. Apache uses a multi-process or multi-threaded model, where each incoming request is handled by a separate process or thread. Nginx, on the other hand, 
uses an asynchronous, event-driven model, where a single process can handle many connections simultaneously.

504-What are the controls used in the Nginx web server?

Stop
Quit
Reopen
Reload

505-What Are the Features Of Nginx?

Simultaneous Connections with low reminiscence
Auto Indexing
Load Balancing
Reverse Proxy with Caching
Fault Tolerance

506-How to start and stop the nginx server?

>>sudo systemctl start nginx

507-What do you mean by Apache Web Server?

>>Apache web server is the HTTP web server that is open source, and it is used for hosting the website

508-What is the port of HTTP and https of Apache?

The port of HTTP is 80, and HTTP is 443 in Apache.

509-Where are the configuration directories of the Apache webserver?

>>cd /etc/HTTP and type ls -l

510-What do you mean by the log files of the Apache webserver?

>> We can access the log files of the Apache server from the below location: /var/log/httpd/access_log and error log from /var/log/httpd/error_log

511-What do you mean by Apache Virtual Hosting?

>>Hosting multiple websites on a single web server is known as Apache Virtual Hosting. There are two types of virtual hosting:
Name-Based Virtual Hosting and 
IP-Based Virtual Hosting.

512-How to stop/start Apache Web Server?

>>You can restart by going to the Apache instance location >> bin folder and execute Apachectl script.

./apachectl stop
./apachectl start
You may also use a script located in /etc/init.d/. Mostly it will be named either “Apache” or “HTTPD”

/etc/init.d/apache stop
/etc/init.d/apache start

Another procedure would be to use services

httpd stop
service httpd start

513-What are the log files generated by Apache?

>>There are two popular log files created;

access.log – all request details with the status code
error.log – capture all the errors within Apache or connecting to the backend

514-What’s the difference between Apache Web Server and Apache Tomcat?

>>Apache Web is an HTTP server to serve static contents whereas Tomcat is a servlet container to deploy JSP files.

>>You can always integrate Apache HTTP with Tomcat, however, based on the requirement you need to choose either one. If you need a proper web server, 
then Apache HTTP else Tomcat for JSP-Servlet Container.

515-where is the Apache root directory

>> The Apache web server serves documents stored in the var/www/html directory by default. 

516-What is Apache Tomcat?

>>Apache Tomcat is basically a Web Server and Servlet system that is open-source (i.e. freely available on the Internet)
It is the server mostly used by Java Developers.

The server is nothing but a computer program that provides service to other computers.

There are basically two types of servers:

Application Server
Web Server
Apache Tomcat offers HTTP protocol, which means the user can connect with the server from anywhere by the URL provided and can access the Java application.

This is very easy and simple to install and is compatible with any Operating System.

517-What are the types of batch files with the help of which we can Start and Stop the Apache Tomcat Server?

>>There are basically two types of batch files with which we can Start and Stop the Server.

Startup.bat
Shutdown.bat

518-What are the vital benefits of Running Tomcat as a service?
The benefits of Running Tomcat as a service are:

>>Automatic Startup – If the Tomcat window service starts up automatically then it would be helpful when we want to start the system remotely.
>>Security – It allows you to execute under a special account which is protected from the other accounts.
>>Starting off the server without active user login: So even if there is no active user, the available server can be started.

519-you can configure the online commands like this to start and stop

>>ln -s /opt/tomcat/bin/startup.sh /usr/local/bin/tomcatup
>>ln -s /opt/tomcat/bin/shutdown.sh /usr/local/bin/tomcatdown

520-how to copy your war file into your tomcat 

>>>cd /usr/local/tomcat
>>cd webapps.dist
>>cp -R * ../webapps/ 

>> You can copy your build file into the web apps directory

521-how to add SSL certificate into Apache server

>>need to create the .crt file and private.key then add it into the conf folder and add it 
>> Then open .conf file 
add this details
<VirtualHost *:443>
    ServerName your_domain.com
    DocumentRoot /var/www/HTML
    
    # SSL Configuration
    SSLEngine on
    SSLCertificateFile /path/to/your_domain.crt
    SSLCertificateKeyFile /path/to/your_domain.key
    SSLCertificateChainFile /path/to/intermediate_certs.crt
</VirtualHost>
>>and replace your domain 

522-While we start the tomcat server its not starting

>>check first java installed and set the home path or not
>>chek any system or server running in same port 8080
>>check the you hav sufficent permissions is there are not user permissions
>>check file permissions
>>check the system resources like memory and cpu and disk-space availble
>>Review configuration files: Double-check the configuration files, such as server.xml, web.xml, and any application-specific configuration files, for any errors or misconfigurations

523-when npm install used build once build happens what you will get 

>>package-lock.json and node_modules this two files will get 
>>if you build using java application then you will get .Jar or .War files

524-if you compile java code where the jar file will go 

>>if you compile or build the java code by default it will go target/shipping.jar it will go target folder under anyfile name what you will give 

525-how to build angular application

>>npm install -g @angular/cli    -->Make sure you have the Angular CLI installed globally on your system. If you haven't already installed it, you can do so using the following command
>>ng build   -->command Run the ng build command to build your Angular application. By default, this command will create a dist/ directory with the compiled and optimized application code. 
You can specify various options and flags with this command, such as the --prod flag for a production build

526-how to control java memory in java application or package 

>>uWSGI is a fast, open-source application server container that is widely used to deploy and run web applications and frameworks written in various programming languages. It acts as a bridge between
web servers and web applications, providing a communication interface for handling requests and responses.
>>if using package Running. The most basic way to run uWSGI is to tell it to start an HTTP server and import your application. If you're using the app factory pattern, you'll need to create a small Python
file to create the app, then point uWSGI at that.
ExecStart=/bin/java -XX:MaxRAMPercentage=90 -XX:MinRAMPercentage=50 -XX:InitailRAMPercentage=50 -jar /app/shipping.jar 

527-what is uwsgi configuration

>>vi /app/payment.ini 
[uwsgi]
wsgi-file = payment.py
callable  = app
master    = true 
processes = 4
threads   = 2
lazy-apps = true 
enable-threads = true 

socket  = 0.0.0.0:8080
protocal  = https 
uid  = 1
gid  = 1

528-heap space or out of memory

>>The Java heap is the area of memory used to store objects instantiated by applications running on the JVM. When the JVM is started, heap memory is created and any objects in the heap can be shared between
threads as long as the application is running.
>>your not mentioning how much memory to consume you need to mention if your not mention by default it will consume more memory

529-is this possible to change the log format in nginx logs 

>>yes you can see video 65 40 mins how to change
>>vi /etc/nginx/nginx.conf 

log_format main 'remote_addr $time_local $request $status $body_bytes_sent $request_time'  
>>this format used to collect required information ex: you have send the data elk you can search easily what you want 

530-what is Amazon landling zone how we can configure 

531-what is monorepo & multirepo what is the difference between this

>>all the repos is one place or all the state files in one place is called monorepo 
>>one place to bother
>>runing multiple modules in order is needed means multiple executions
>>sharing this someone easy 
>>terraform apply -target=module.mysql  -->>you can apply single module if you want in terraform 

>>Making repos like state files any files in differnet repos is called multi-repos  

532-what is the difference between SRE and DevOps

SRE:-
>>A site reliability engineer's job is to ensure the high availability, reliability, and resilience of production systems and services. SRE responsibilities can encompass on-premises, hybrid cloud, and
public cloud environments in any given system.
>>Performance tuning and optimization fall on the SRE team, even in complex hybrid and multi-cloud environments. This requires automation and centralized tooling to ensure maximum team productivity.
The SRE team automates deployment, scaling, monitoring, and related tasks across these environments.
>>SRE teams also define and maintain customer SLAs within their area of responsibility. In addition, they provide technical and operations support to remediate cases of SLA system violations.
>>Designing, testing, and implementing disaster recovery plans is also an SRE responsibility. This requires proactivity and ownership by SREs to ensure their team's response to a disaster situation
is well-rehearsed and on point. Disaster recovery plans aren't meant to be shelfware; SRE teams should constantly test and improve their plans and practices.
SRE use cases:-
Proactive monitoring of system health to identify problems before they become significant issues that might impact operations and customer experience.
Automation of routine site monitoring and related tasks to improve SRE productivity, reduce human error, and free up SREs to work on more strategic tasks to improve site reliability and operations.
Incident management, which includes both resolving the incident swiftly and putting the tools and playbooks in place to ensure that the incident won't happen again.
Problems SREs solve:-
>>SRE teams focus on large-scale problems that could cost organizations money due to system outages. Here are some of the problems they solve:
>>Service disruptions. SREs have the monitoring, alerting, and incident response tools and playbooks to mitigate problems that disruptions bring.
>>Scalability challenges. These threaten the operations of commercial and public sector systems. Planning for increased workloads and traffic requires the specialized architectural expertise that
SREs bring to large-scale enterprise operations.
>>Slow response time. SREs address bottlenecks, optimize code, and implement caching strategies to improve response times and meet customer expectations.
SRE-Tools:-
1-Grafana
2-Prometheus
3-Kubernetes
DevOps:-
>>DevOps teams implement CI/CD pipelines to manage and maintain their organization's development infrastructure, including public cloud environments.
>>DevOps is responsible for automating the build, test, and deployment processes to increase the speed and efficiency of application delivery. This isn't a one-and-done task; DevOps teams must approach 
this task with an eye to continuous improvement.
>>DevOps teams should aim to continuously improve the deployment process by making it faster, more reliable, and more scalable. This requires the team to document and communicate improvements to the 
SRE team and other technical stakeholders.
>>Other DevOps responsibilities include ensuring the high availability and scalability of the systems they develop. DevOps also monitors and troubleshoots technical and security issues in development
and testing environments.
DevOps team use cases:-
>>Use cases for DevOps teams bridge the gap between development and operations, enabling faster software deployment, continuous improvement, and the continuous delivery of services. DevOps teams automate
repetitive tasks to remove human error and improve the overall efficiency of software development, delivery, and operations. Automation accelerates software delivery velocity, giving organizations a 
time-to-market advantage.
>>Another DevOps team use case is software testing -- DevOps teams are responsible for testing software before they deploy it to internal and external customers.
>>Monitoring and logging are also DevOps responsibilities. DevOps teams must continuously monitor the systems to detect any anomalies or potential issues. As more DevOps teams move to the cloud,
AI could begin to play a bigger part in data consumption and logging.
Problems DevOps teams solve:-
>>DevOps teams are responsible for alleviating and mitigating numerous problems that could crop up across the delivery pipeline. Lack of visibility into the delivery pipeline is one problem that
DevOps teams rectify. By using observability tools, DevOps teams capture and interpret actionable data on the current state of their development software.
>>DevOps teams also face problems with the upheaval of waterfall software development. DevOps teams are responsible for pivoting their organization from legacy processes to the agility of DevOps. 
This pivot improves software delivery velocity, software quality and security.
>>Lastly, not all the problems that DevOps teams solve deal with technology. DevOps teams must also tackle cultural barriers such as communication silos that stymy collaboration amongst developers
and stakeholders across their organization.
DevOps-Tools:-
1-Jenkins
2-Git
3-Docker
4-Kubernetes
5-AWS
6-Terraform

Collaboration points and similarities between SRE and DevOps:-

>>To deliver secure and quality software, SRE and DevOps teams must collaborate on a few essential points.
>>When the organization launches a new feature or service, SRE teams should collaborate with their DevOps counterparts to ensure the scalability and reliability of the new offerings. 
This responsibility ties back to site reliability engineers' SLA and performance-tuning work.
>>SRE and DevOps work together to monitor their areas of responsibility and collaborate on responses when incidents occur. They must also collaborate on incident postmortems and root cause analysis,
aiming to identify and resolve the underlying causes of the incident so that it won't happen again.
>>Security throughout the development lifecycle is becoming increasingly critical as teams try to do more with less while facing an ever-evolving cyber threat landscape. Both DevOps and SRE teams 
can automate and secure toolchains to ensure the organization can deliver new features and bug fixes to its customers continuously and securely.
>>Configuration management and capacity planning are other areas that require DevOps-SRE collaboration. Each group can suffer if configuration issues arise in an application across their environments.
Likewise, expertise and data from both groups are necessary to scale software to meet business needs while staying within budget.
>>Finally, SRE and DevOps can come together to communicate about technical projects outside the IT department. Using shared project management reporting and collaboration tools, DevOps and SRE teams 
can give executive stakeholders the end-to-end picture of a project's status or an incident in the organization's IT environment.

533-how to raise a ticket in AWS 


534-what is reverse proxy

>>A reverse proxy is a server or service that sits between client devices and backend servers. When a client makes a request, it is forwarded to the reverse proxy, which then acts as 
an intermediary and routes the request to the appropriate backend server. The response from the server is then returned to the client through the reverse proxy. The client is unaware of
the backend server and communicates only with the reverse proxy.

535-what is mod_proxy 

>>mod_proxy and related modules implement a proxy/gateway for Apache HTTP Server, supporting a number of popular protocols as well as several different load-balancing algorithms. 
Third-party modules can add support for additional protocols and load balancing algorithms.

536-what is the difference between reverse proxy and mod_proxy

>>In summary, a reverse proxy is a general concept referring to a server or service that acts as an intermediary between clients and backend servers. On the other hand, mod_proxy is a 
specific module within the Apache HTTP Server that enables reverse proxy functionality and provides additional features and configurations for proxying requests.

537-what is the difference between reverse proxy and forward proxy

>> If you use Reverse proxy user doesn't know the proxy details and everything this request will be forwarded to another endpoint or any other servers also we use SSL termination in 
as reverse proxy
>>if you use a forward proxy user knows everything which will help you this things
1-client citrix
2-Anonymus
3-content filter
4-cacheing 

538-what is forward proxy and reverse proxy 

>>forward proxy:-
1-A Forward proxy sits infront of users then after the traffic will go the internet then hit the servers 
>>if you use forward proxy server dont understand real clients. hiding our identity for security purpose companies can impose restrictions not to use particular sites access not do naything from internet  
>>Clients configure their applications or systems to use a forward proxy to access external resources.
>>When a client makes a request, it first goes through the forward proxy, which then forwards the request to the target server.
>>The target server sees the request as coming from the forward proxy, not the original client.
Use-Cases:-
1-Client Anonymity
2-Caching
3-Traffic Control
4-Logging
5-Request/Response Transformation
6-Encryption
Examples:- 	Squid, Proxy, Tor 
>>Reverse proxy:-  
1-A reverse proxy sits in front of servers and acts on behalf of those servers to handle incoming client requests.
>>to hide the web appliation so organization puts the web server infront load-balancer then webserver then app-servers then db-servers when user hit the url it will connect the loadBalancer then 
web-server this act as reverse proxy  
>>Clients send requests to the reverse proxy, thinking it is the server.
>>The reverse proxy then forwards the request to the appropriate backend server.
>>The backend server processes the request, and the reverse proxy sends the response back to the client.
>>for example you configured the frontend connected with backend and backend connect with database then you created nginx reverse proxy another server and added forntend ip as reverse proxy if any hacker known 
Ip or anything your application will not effect until you making public  
Use-Cases:-
1-Server Anonymity
2-Caching
3-Load Balancing
4-DDoS Protection
5-Canary Experimentation
6-URL/Content Rewriting
7-increase security
8-scalability and flexibility
9-SSL termination
Examples:- Nginx, Apache, HAProxy
https://www.youtube.com/watch?v=1uZQ0DaNGjM

539-mostly frontend will work 80 port 

>>backend will work on 8080 either java or python or nodejs also 
>>if you use java most compile if you node or python no need to compile only java required to compile  
>>python for pip , nodejs for npm , java for maven , dotnet for msbuild 
>>APP/WEB apps are called stateless -->database will follow the CRUD operations  
>>DB apps are called stateful. it take the request and give back the responce to customers that why called stateful. request come and take the responce back is called stateful   
>>nginx used as reverse proxy for web applications. like any if the servers crash we can create immediatly but coming to DB its not possible if loose the data we can't recreate it until you have backup  

540-what is SLA(service-level agreement)

>>They provide information on all contracted services and on the agreed expected reliability and they clearly set out the parameters, responsibilities and expectations 
so that in the event of a problem with the service, no party can plead ignorance

541-what is soft and hard link in cmd or symbolic link

>>Symbolic links (also called "soft" links) are files that point to a file or directory in your system, but don't mirror the other file's data.
>>soft link is an actual link to the original file, whereas a hard link is a mirror copy of the original file. 
>>ln -s /opt/tomcat/bin/startup.sh /usr/local/bin/tomcatup    -->it will create short-cut to start and down the server
>>ln -s /opt/tomcat/bin/shutdown.sh /usr/local/bin/tomcatdown         -->this soft link or symbalic link
>>ln /opt/tomcat/bin/shutdown.sh /usr/local/bin/tomcatdown   -->this hard link
>>tomcatup    -->it will start the server
>> If you delete the original file, the soft link has no value, because it points to a non-existent file. But in the case of hard link, it is entirely opposite.

542-what is absolute path and relative path in linux

>>In simple words, an absolute path refers to the same location in a file system relative to the root directory, 
whereas a relative path points to a specific location in a file system relative to the current directory you are working on.

543-Is this possible to send the mail using cli in command using shell -script

>>yes you need to configure the mail in your system its posssible to send the mail using cli in 
mail -s "$(date): Report on $(hostname)" "anji159258@gmail.com"

544-how to configure your mail server in linux

>>>>sudo apt-get install mailutils >>select internet site >>give domain name >>click yes
>>sudo vi /etc/postfix/main.cf
realyhost = [smtp.gmail.com]:587

#Enable SASL authentication

>>sudo vi/etc/postfix/sasl/sasl_passwd
[smtp.gmail.com]:587 anji1592@gmail.com
>>sudo postmap /etc/postfix/sasl/sasl_passwd

echo "test alert" | mail -s "system down" anji1592@gmail.com

tail -f /var/log/mail.log

>>not complted see the video in liked videos

545-what is the shebang line

>> Shebang is nothing but which shell are using to run your script that is shebang line

546-what is daily activites in shell-script

>>delete old files using shell script
>>taking inventory files like version and ips and those
>>monitoring files 
>>installing packages
>>to see versions
>>to see backup
>>if server stop sending a mail 

547-how to stop all services in linux terminal

>>killall

548-how to clear history in linux

>>history -c  //it will delete all history
>>history -d //it will delete the one line particular you can mention

549-how to check whether the server is running or not connecting 

1-check the server is running or not (ps -ef)
2-check the port is open or not (netstat -lntp)
3-check the ports open security group or not 
4-In case that service is accessed  by another server then you can check the reachability using telnet 14.98.164.230
5-restart the services or server 
6-repeat step 4
7-finally you can check the server 

550-how to do MongoDB bindIp address to listen to another service while connected to MongoDB when we give MongoDB URL to any other service 

>>vim /etc/mongod.conf 

net:
  port: 27017
  bindIp: 127.0.0.1 ##replace 0.0.0.0 or your web server IP like any public IP or private of your service 
:wq

>>systemctl restart mongod 

551-what is systemd service file

>>A systemd service file is a configuration file used by the system init system in many modern Linux distributions to define and manage services, daemons, and other system tasks. Systemd is a system 
and service manager that plays a central role in the boot process and service management on Linux systems. The service file specifies how a service should be started, stopped, and managed by systemd.

>>vi /etc/systemd/system/catalogue.service 
[Unit]
Description = Catalogue Service

[Service]
User=roboshop
Environment=MONGO=true
Environment=MONGO_URL="mongodb://<MONGODB-SERVER-IPADDRESS>:27017/catalogue"
ExecStart=/bin/node /app/server.js
SyslogIdentifier=catalogue

[Install]
WantedBy=multi-user.target
:wq

>>For example, your installed fronted and Mongodb service then mongod must talk this catalog service right you need to give the endpoint of Mongod 

552-what is mounting in Linux

>>In Linux, "mounting" refers to the process of associating a filesystem with a specific location (mount point) in the directory tree of the 
file system hierarchy. When you mount a filesystem, 
you are essentially making the contents of that filesystem accessible at the specified mount point in the directory structure.
1-mountpoint mount point is a directory on the file system where an additional filesystem is attached. This is the location where the contents 
of the mounted filesystem become accessible.
Common mount points include directories like /mnt, /media, or custom directories created for specific purposes.
2-Filesystem:A filesystem is a hierarchy of directories and files that is used to organize and store data on a storage device. Examples of 
filesystems in Linux include ext4, XFS, and others.
3-Mount Command:The mount command is used to attach a filesystem to the system's directory tree. The basic 
mount [options] device|mount_point
4-Unmount Command:The umount command is used to detach a mounted filesystem. The syntax is:
umount mount_point

this is the process:-
>>fdisk -l or lsblk 
>>Identify the device name of your 1 TB hard disk. It will typically be something like /dev/xvdb or /dev/xvdc.
>>Type n to create a new partition.
>>Choose the primary partition type (usually the default, press Enter).
>>Specify the starting and ending sectors. For a 500 GB partition, you might specify +500G as the ending sector.
>>Type t to change the partition type (if needed) and set the file system type.
>>Type w to write the changes to the disk and exit.
>>After you've created the partition, you should have a new partition (e.g., /dev/xvdb1)
>>sudo mkfs.ext4 /dev/xvdb1   -->You need to create a file system on the new partition. The following example creates an ext4 file system:
>>sudo mkdir /mnt/mynewdrive   -->Create a directory where you want to mount the new partition.
>>sudo mount /dev/xvdb1 /mnt/mynewdrive  -->Mount the new partition to this directory
>>vi /etc/fstab
/dev/xvdb1 /mnt/mynewdrive ext4 defaults 0 0
:wq   -->To ensure the partition is mounted at boot, add an entry to the /etc/fstab file. Edit the file using a text editor (e.g., sudo nano /etc/fstab)
>>df -h   -->You can verify that the partition is correctly mounted by running
>>df -Th   -->it will give good understanding 

========attach extra volume =======

>>launch ec2 instance
>>creat new volume and attach to ec2 instance  4gb
>>sudo su -
>>lsblk            -->to see the list of drives
>>df -h            -->to see the list of drives
>>file -s /dev/xvdf   -->it will show any files is there or not  
>>fdisk /dev/xvdf                        
>>mkfs.ext4 /dev/xvdf1
>>mkdir /anji          -->createing drive name anji
>>mount /dev/xvdf1  /anji/
>>nano /etc/fstab

/dev/xvdf1   /anji/  ext4 defaults,noatime  1  1

:wq

>>restart the system you can check the drive attached or not
>>sudo su -
>>lsblk
you will see the drive and files

second-type:-
>>lsblk   -->see the volumes
>>df -Th   -->show list of the volumes with size in gb or mb 
>>file -s /dev/xvdf    -->it will show any files there or not in your volume show 
>>mkfs -t xfs /dev/xvdf  -->create a file system 
>>file -s /dev/xvdf
>>mkdir anji 
>>ls 
>>mount /dev/xvdf anji/
>>df -Th  
>>cat /etc/mtab   -->it will show the details where your volume attaching or permanent storing any restart happen without lossing data 
>>nano /etc/fstab

/dev/xvdf1   /anji/  ext4 defaults,noatime  1  1

:wq

>>restart the system you can check the drive attached or not
>>sudo su -
>>lsblk
you will see the drive and files
>>you change that volume 50gb to 100gb 
>>lsblk 
>>yum install xfsprogs 
>>xfs_growfs -d /dev/xvdf   -->it will grow 
--second-type to grow 
>>growpart /dev/xvdf   

553-what are the types of units of memory 

>>Bit 
>>Nibble 
>>Bytes 
>>Kilo Byte 
>>Mega Byte 
>>Giga Byte 
>>Tera Byte 
>>Peta Byte 
>>Exa Byte 
>>Zetta Byte 
>>Yotta Byte 

554-what is ssl-encryption

>>SSL(Secure Socket layer) Encryption is a standard secuirty technology that establishig and encrypeted connection between user and client
>>It ensures that data trasnmitted between the client and server remains confidential and secure 
>>SSL encryption is commonly used in web-applications, including grafana to project sensitive information during transmission

555-what are the SSL Best Practice

1-Use Trusted SSL-Certificates
2-Keep certificate up to date 
3-Configure Strongg encryption algorithm 
4-Implement proper certificate management
5-Regularly Test SSL-Encryption 

556-what is kafka what is the advantages of kafka

>>Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in real-time. Streaming data is data that is continuously generated by thousands of data sources, 
which typically send the data records in simultaneously.
>>it is availble in managed service in AWS name AmazonMSk 
>>kafka port number 9092
install kafka in local system then enter the folder and run this command 
>>bin/kafka-topics.sh --create --topic anji-1 --bootstrap-server Kafka-endpoint-here:9092

557-how many rules can i add up to in the application load balancer

>>100 total rules per application load balancer
>>5 condition values per rule 
>>5 wild cards per rule 
>>5 weighted target groups per rule 

558-what is the maximum size of userdata that can be provided to an Ec2 instance

>>16 KB of data 

559-what is synthectics canary in aws 


27-whay is aws x-ray in aws 

560-what is Outpost rack in AWS 

>>this is like private cloud if you request AWS OutPost Rack AWS person or team will come and install the server in on-premise . run your servers on your on-premise this private cloud feature Availble in AWS 
like you think your data is secure no need to put the cloud then you can request the AWS OutPost it will create the private cloud on your company

561-can i block the certain countries like or ips using geolocation in route53 

>>yes you can use this to block the ip address or countries 

562-i have snapshot taken 2018 ebs volume i forgotted to delete then later I created policy to take backup every two days retention period so that snapshot will cost more or medium any price changes will effect
totl three shanpshot two or latest one is 2018 snapshot

>>The snapshot from 2018 will incur ongoing storage costs as long as it is retained.
>>AWS charges for snapshot storage are based on the amount of data stored, so older and larger snapshots can accumulate significant costs.
>>Snapshot storage is billed at a per-GB-month rate.
>>The cost for snapshots can vary based on the region and current AWS pricing.

563-what are the aws secuirty servcie  for AWS account

Identity and Access Management
1-AWS Identity and Access Management (IAM):
>>Manage user access and encryption keys.
>>Control access to AWS services and resources.
2-AWS Single Sign-On (SSO):
>>Centrally manage SSO access to multiple AWS accounts and business applications.

Infrastructure Protection
3-AWS Shield:
>>DDoS protection service that safeguards web applications running on AWS.
>>Standard tier (automatic) and Advanced tier (additional cost and enhanced protection).
4-AWS Web Application Firewall (WAF):
>>Protects web applications from common web exploits.
>>Customizable web security rules.
5-Amazon GuardDuty:
>>Threat detection service that continuously monitors for malicious activity and unauthorized behavior.
6-AWS Network Firewall:
>>Managed network firewall to protect your VPCs.
>>Offers protection against network threats.

Data Protection
7-AWS Key Management Service (KMS):
>>Create and control the encryption keys used to encrypt your data.
8-AWS Certificate Manager (ACM):
>>Provision, manage, and deploy SSL/TLS certificates.
9-Amazon Macie:
>>Uses machine learning to automatically discover, classify, and protect sensitive data in AWS.

Threat Detection and Monitoring
10-Amazon Inspector:
>>Automated security assessment service that helps improve the security and compliance of applications.
11-AWS Config:
>>Enables assessment, auditing, and evaluation of the configurations of your AWS resources.
12-AWS CloudTrail:
>>Logs, continuously monitors, and retains account activity related to actions across your AWS infrastructure.
13-AWS Security Hub:
>>Provides a comprehensive view of your security alerts and security posture across your AWS accounts.

Security Management and Governance
14-AWS Organizations:
>>Centrally manage and govern your environment as you grow and scale your AWS resources.
15AWS Control Tower:
>>Set up and govern a secure, multi-account AWS environment based on best practices.
16-AWS Service Catalog:
>>Create and manage catalogs of IT services that are approved for use on AWS.

Application Security
17-Amazon Detective:
>>Analyze and visualize security data to rapidly get to the root cause of potential security issues.
18-AWS Secrets Manager:
>>Helps protect access to your applications, services, and IT resources without the upfront cost of hardware.
19-AWS CloudHSM:
>>Hardware-based key storage for regulatory compliance.

Incident Response
20-AWS Step Functions:
>>Coordinate multiple AWS services into serverless workflows to automate and orchestrate your incident response.
21-AWS Lambda:
>>Run code in response to triggers such as changes in data or system states to automate incident response.

Compliance and Data Privacy
22-AWS Artifact:
>>On-demand access to AWS’ compliance reports and select online agreements.

Security Best Practices and Frameworks
23-AWS Well-Architected Tool:
>>Provides architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud.

Edge and Hybrid Security
24-AWS Outposts:
>>Extend AWS infrastructure and services to your on-premises environment for a truly consistent hybrid experience.

Security Information and Event Management (SIEM)
25-Amazon Athena:
>>Analyze logs from AWS services, such as CloudTrail and VPC Flow Logs, for security insights.



111-
>>mysql -u root -p     -->login 
>>select user from mysql.user;  -->to see user 
>>show databases;
>>create user 'anji'@'%' IDENTIFIED BY 'admin123';   -->create anji user 
>>Ggrant privilege on *.* TO 'anji'@'%'with grant option;      -->give admin permission to anji user 
>>FLUSH PRIVILEGES;    -->it will affect the user  
>>mysql -h 77.39.56.69 -u anji -p    -->connecting from another server  
>>create database sampi;
>>use database

----creating user only for only single database access  -->you can manage
>>mysql -h <your-rds-endpoint> -u <your-username> -p
>>CREATE USER 'anji'@'%' IDENTIFIED BY '<your-password>';
>>CREATE DATABASE anjidb;
>>GRANT ALL PRIVILEGES ON anjidb.* TO 'anji'@'%';
>>FLUSH PRIVILEGES;
----creating user only for only single database read only  access  -->you can manage
>>mysql -h <your-rds-endpoint> -u <your-username> -p
>>CREATE USER 'anji'@'%' IDENTIFIED BY '<your-password>';
>>CREATE DATABASE anjidb;
>>GRANT SELECT ON anjidb.* TO 'anji'@'%';
>>FLUSH PRIVILEGES;

112-what is phpmyadmin

>>phpmyadmin is database management tol(GUI)
>>phpmyadmin is a free software tool written in PHP that is intende to handle the administration of a MQSQL or MariaDB database server. you can use phpmyadmin to perform most administration tasks including creating a database, running quires and adding user accounts 
first install httpd server then 
>>cd /var/www/html 
>>wget phpmyadmin
>>tar -xzvf phpmyadmin
>>cp -Rf phpmyadmin/* .
>>install php package 8.0 or something
>>systemctl restart httpd

#!/bin/bash
sudo apt update -y 
sudo apt install apache2 -y 
echo "<h1>Server Details</h1><p><strong>Hostname:</strong>$(hostname)</p><p><strong>IP Address:</strong>$(hostname -I | cut -d "" -f1)</p>"> /var/www/html/index.html
sudo systemctl restart apache2 
sudo chkconfig apache2 on -->instead you can use syste use chkconfig also 

cloud alarams 

1-ok  -->means this alarm is not triggred 
2-insufficient data   -->it dont have sufficient data for montoring 
2-Alarm  -->means that triggred for an data 

cat /etc/passwd 
apt-gt install httpd -y 
useradd anji   -->whenever you create a user automatically created group 
uderdel anji 
groupadd sampi 
cat /etc/group 
ls /home/   -->see list of users in your system 
cat /etc/passwd 
getent passwd 
getent group 
cat /etc/shadow   -->see credeintial of user the password will be encrypted md5 algorithms
passwd anji   -->ask to enter password
id    -->if you logged in user root it will show 
id anji    -->find the user-id and group-id 
userid of root is 0

1-if i want add specific userid is possible to users 

>>yes it possible by default userid will start from users 1000 
>>adduser -u 1223 anji  -->it create anji user with user-id 1223 
>>anji:x:1234:1234::/home/anji:/bin/bash
Username (anji):
Password Placeholder (x):  -->Similar to the previous explanation, the "x" indicates that the actual password hash is stored in the /etc/shadow file.
User ID (1234):
Group ID (1234):
User Information (::):
Home Directory (/home/anji):
Login Shell (/bin/bash):
Login Shell (/sbin/nologin):  -->This field specifies the user's default shell. However, in this case, it's set to /sbin/nologin. The /sbin/nologin shell is used to prevent the user from logging into the 
system. It's often employed for service accounts or accounts that are not meant for interactive login.

last   -->to see login users list 
w  -->also login user details and ip addess login time 
su - switch to users (su anji) like this 
visudo or vi /etc/visudoers  -->you can add user permission 

2-iam created anji user and created password and anji user can login but he cant able to install any packages what is the problems

>>so when you created the user anji or sampi you give password you enable password authentication and everything but he is not able add or install any packages means that user not added visudo or sudoers
file need to add that user and give permission then he can access or install with the permission
>>visudo 

anji ALL=(ALL)  NOPASSWD: ALL 

:wq

3-i want to create a user for custom directory is it possible 

>>yes its possible 
>>useradd -m -d /opt/anji anji -->it will create /opt/anji directory

4-how to create a user account expire while creating a user 

>>adduser -3 2024/01/23 anji  -->it will be that date only he will account valid 
>>chage -l anji  -->will show the password or acount expire details 
>>chage -m 10 anji  -->he have to chnage the password every 10 days 
>>chage -M 30 anji    -->password expiry 
>>cat /etc/login.defs  --> here you can user id details and groupid details also 
>>passwd --lock anji 

5-is it possible to change the userid and groupid start from 1000 right can i change 100 is it possible 

>>yes its possible you cna chnage user-id and group-id and password expiry dates and when user created home directory are not 
>>vi /etc/login.defs 

6-so while deleting a user that user is deleted but not the home directory under user directory

>>you can use userdel --remove anji  -->it will delete the anji user and directory while we created the user that time created 
>>if you use userdel anji --it will not dleete that directory 

7-types of groupes in linux 

1-primary 
2-secondary 

>>groups   -->list the groups or provides group information
id anji -->list user-i and group also 
group anji  -->it will also show  

559-what bases s3 will charge 

1-how much data we are storing
2-how many (PUT/GET) operations happen like upload and download 
3-Data Retrivals based 

560-i have anji778 bucket stored in standard then moved to standars-IA after a week my mind is changed I want the object to standard storage it possible

>>Yes, it is possible to move objects back from "standard-IA" (Infrequent Access) storage to "standard" storage in cloud services like AWS or Azure. Here’s how you can typically accomplish this:

Console Method:

Log in to your AWS Management Console.
Navigate to the Amazon S3 dashboard.
Select the bucket anji778 where your objects are stored.
Navigate to the folder or object you want to move.
Select the object and choose "Actions" or right-click to see options.
Choose "Change storage class."
Select "Standard" as the desired storage class.
Confirm the change.
Using AWS CLI:

You can use the AWS CLI (aws s3 cp or aws s3 mv) with the --storage-class parameter to specify STANDARD for moving objects back to standard storage.
