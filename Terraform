==================================================================Terraform and packer=====================================================================================

1 - what is terraform

>>Terraform version 1.4.0
>>Terraform it is open-source terrform is IAAC(Infrastructure as a code) tool used to create resources on cloud providers like AWS, Azure, GCP so on. 
>> For example if you want to create an ec2 instance using Terraform rather than going to the management console to click on service you will write a code 
>>using terraform is easy to manage the infrastructure you can reuse the code to create multiple resources you can also store the code on a version control system like
GitHub code commit bitbucket to keep track of your changes in resources 
>> Terraform is not easy but effective
>> When any changes happen .tfstate will save the data
>>tfstate is very important
>>you can apply single module if you want 

Advantages:-
1-Automated infra CRUD(create read update delete)
2-Terraform code is easy to understand and easy to use
3-Version control(we can save the code in git and easy to collaborate the other teammates)
4-terraform have an idempotant
5-Consistent Infra(using terraform code you can create similar infra into different environment without duplicate)
6-Config can be split into multiple files (variables)
7-Inventory management
8-Cost optimization
9-Modular Infra(DRY -->Dont Repeat Yourself)
10-importing resource is easy
Dis-Advantages:-
1-Bugs will not be fixed (fast)
2-new service might not be available(when aws developing an service when he relases aws services then he can develop the terraform )
3-his is a third-party tool (there is no support at all times and all the new services not availble)

#>>terraform is an infrastructure as a tool that allows you to build infrastructure and change versions,safely and efficently
#>>Terraform is a tool for building, changing and versioning infrastructure safely and efficiently. Terraform can manage existing and popular cloud service providers
#as well as custom in-house solutions.Configuration files describe to Terraform the components needed to run a single application or your entire datacenter

2-is it terraform cloud aganstics

>>terraform is cloud aganstic  -->yes terraform will work every cloud but the code is difference that why we calling cloud aganstics

3-what is proprietory

>>it is like not a open-source when you using terraform you advised some code to then can verfiy and give resopnce 
but in aws your rasing an ticket but its take some time to contact and its not accept your advise this proprietory

4-what is the differnce between argument refernce and attribute refernce in terraform

>>In Terraform, attributes and arguments are two important concepts used to define and configure resources.

>>Arguments: Arguments are used to define the configuration for a resource. They are the input values that define the desired state of the resource. For example, when defining an 
EC2 instance, arguments might include the instance type, AMI ID, security group, and key pair.
>>Attributes: Attributes are the output values that are returned by a resource after it has been created or updated. They represent the current state of the resource and can be used
by other resources or modules. For example, an EC2 instance might have attributes such as the public IP address, private IP address, and instance ID.
>>In general, arguments are used to define the desired state of a resource, while attributes are used to retrieve the current state of a resource. By using attributes and arguments in
Terraform, you can define and manage your infrastructure in a more modular and reusable way.

5-how to deploy dev environment and prod environment in a single folder 

>>yes you can use workspace usinng you can deploy the devand prod environmet by chnaging code in dev.tfvars and prod.tfvars by chnaging workspace context
there is chance to overwrite the environmet in case if you do mistake so need to be carefully. either you can seprate folders is good in production environment
>>terraform apply --var-file prod.tfvars --auto-approve       -->it will execute the prod.tfvars in prod workspace
>>terraform apply --var-file dev.tfvars --auto-approve        -->it will execute the dev.tfvars in dev workspace

6-what is splat syntax in terraform and element ,functions,

>>splat syntax if the resource has a count attributes set you can access individual attributes with a zero-based 
index such as ${aws_instance.web.0.id} you can also use the splat syntax to get a list of attributes ${aws_instance.web.*.id}

>>if you want use subnet_id = aws_subnet.public-subnets[0].id   first subnets then like this it will satrt 0 until you mention like this ${count.index +1}" it will start 1 then 2 then 3 
 
>>functions using we can do duplicate the code
>>element idicates when you give the index in terraform it indicates starts from 0 
>>index is count

7-what is concat

>>concat takes two or more lists and combines them into a single list.

--How do I merge two lists in Terraform?

>>You can use terraform concat() function to combine multiple lists into a single list. concat() takes two or more lists and combines them into a single list.

variable "bucket_prefix" {
  default = "my-bucket-prefix"
}

variable "dynamic_value" {
  default = "123"
}

variable "bucket_suffix" {
  default = "mysuffix"
}

resource "aws_s3_bucket" "example_bucket" {
  bucket = "${concat(var.bucket_prefix, "-", var.dynamic_value, "-", var.bucket_suffix)}"

  acl    = "private"
  # other AWS S3 bucket configurations...
}

8-Define null resource in Terraform.

>>null_resource is used to run the resources using which present the resource using we can create the new resouces
>>A null resource is basically something that doesn't create anything on its own, but you can use it to define provisioners blocks. They also have a “trigger” attribute, which can 
be used to recreate the resource, hence to rerun the provisioner block if the trigger is hit.

>>if your using null resource your trigger first time then it will trigger the instance that remote exec or local exec will trigger if you do second time it will trigger but will not execute . why means 
either id or remote exec file need to change eitheriwise it will not work 
>>ex:-

resource "aws_instance" "web" {
  ami                    = data.aws_ami.centos8.id
  instance_type          = "t3.micro"
  vpc_security_group_ids = [aws_security_group.allow_tls.id]

  tags = {
    Name        = "web"
    environment = "DEV"
  }

}

resource "null_resource" "provision" {

  triggers = {
    instance_id = aws_instance.web.id
  }

  provisioner "remote-exec" {
    connection {
      host     = aws_instance.web.public_ip
      user     = "centos"
      password = "DevOps321"
    }

    inline = [
      "echo Helo"
    ]
  }
}

9-what is local-exec and remote-exec

>>local-exec provisioners. The local-exec using we can retrive or take information from the terraform deployment code and see. where you are executed 
terraform we can use this in local-exec command and write some details to see without going to aws you can see your system terraform folder

>>remote-exec provisioner is used it will login to your server run your commands inside the server and execute that files inside the server

10-what is for_each and count how to use in terraform and how many types of variables and what is variables 

>>count is using create multiple resources instances according to the count
>>The count argument is used to determine the amount of instances to create for a particular resource. The count argument can be used in both a module as well as every resource type
>>count is good if there is not change in the list index and changes are sequentical/leanier. any changes in the existing index will for the resource to recreate

resource "aws_subnet" "public-subnets" {
    count = "${var.environment == "prod" ? 3 : 2}"
    vpc_id = "$element{aws_vpc.default.id}"
    cidr_block = "${element(var.public-cidrs, count.index)}"
    availability_zone = "${element(var.azs, count.index)}"

    tags = {
        Name = "$element{aws_vpc.default.tags.name}-public-subnet-${count.index + 1}"
    }
}

ex;-
docdb = {
  main = {
    vpc_name            = "main"
    subnets_name        = "db"
    engine_version      = "4.0.0"
    number_of_instances = 1
    instance_class      = "db.t3.medium"
  }
}

rds = {
  main = {
    vpc_name            = "main"
    subnets_name        = "db"
    engine              = "aurora-mysql"
    engine_version      = "5.7.mysql_aurora.2.11.1"
    number_of_instances = 1
    instance_class      = "db.t3.small"
  }
}

module "docdb" {
  source = "github.com/raghudevopsb70/tf-module-docdb"
  env    = var.env

  for_each            = var.docdb
  subnet_ids          = lookup(lookup(lookup(lookup(module.vpc, each.value.vpc_name, null), "private_subnet_ids", null), each.value.subnets_name, null), "subnet_ids", null)
  vpc_id              = lookup(lookup(module.vpc, each.value.vpc_name, null), "vpc_id", null)
  allow_cidr          = lookup(lookup(lookup(lookup(var.vpc, each.value.vpc_name, null), "private_subnets", null), "app", null), "cidr_block", null)
  engine_version      = each.value.engine_version
  number_of_instances = each.value.number_of_instances
  instance_class      = each.value.instance_class
}

module "rds" {
  source = "github.com/raghudevopsb70/tf-module-rds"
  env    = var.env

  for_each            = var.rds
  subnet_ids          = lookup(lookup(lookup(lookup(module.vpc, each.value.vpc_name, null), "private_subnet_ids", null), each.value.subnets_name, null), "subnet_ids", null)
  vpc_id              = lookup(lookup(module.vpc, each.value.vpc_name, null), "vpc_id", null)
  allow_cidr          = lookup(lookup(lookup(lookup(var.vpc, each.value.vpc_name, null), "private_subnets", null), "app", null), "cidr_block", null)
  engine              = each.value.engine
  engine_version      = each.value.engine_version
  number_of_instances = each.value.number_of_instances
  instance_class      = each.value.instance_class
}

this can be solved by for_each bcoz it uses key rather than

>>for_each using you can create multiple instances accroding to the map, or set of strings

11-what is modules in terraform 

>>A Terraform module is a set of Terraform configuration files in a single directory. Even a simple configuration consisting of a single directory with
one or more .tf files is a module. When you run Terraform commands directly from such a directory, it is considered the root module.

>>if you created modules anyone can use this why because module will be same you need to change the values what need to change you can pass that modules file to anyone then that person can change
the values they can use variables to pass the values 
>>modules is using we can use code many times
>>modules using you can source the files you can use github repository also
>>modules using ./main.tf file another tab also
>>it is repetable code.one code you can use number of times

module "virtual_network" {
source = "./modules/virtual_network"
// other variables... #here you can mention what changes you need  
}

12-what is terraform life cycle, Imports , terraform cloud 

---Life cycle:-
>>create_before_destroy
--if your created an s3 or any system but your created wrong bucket or wrong vpc this logs have store in vpc flow logs
but you dont want lose that dat on that you use create_before_destroy and then your write a code to
copy the files to some other bucket store the data 

>>prevent_destroy
--if your createdan s3 bucket are any vpc or any ec2 system or anything no one want to delete you can use 
this prevent_destory 

lifecycle {
prevent_destory = true
}

>>ignore_changes
--if deployed some instance using terraform but your changed some of the tags or some data change 
maually but in terraform you need to use ignore_changes then the the cant destory what you changed manually

lifecycle {
ignore changes = [
 tags,enable_dns_hostname
]
}

13-what is terraform Imports?

>>terraform Imports:-
---when we have mistkaley or already deployed server in ec2 any service you need import that service
using terraform command you can take the code command to copy the the code code into .tf file

commands:-

>>terraform import aws_instance.importing "instance-id from the aws"
>>terraform state list
>>terraform state show aws_instance.importing          -->you will get code copy and paste below your code page

resource "aws_instance" "importing" {

paste that copy that data

}

14-what is terraform-cloud

>>What is Terraform Cloud? Terraform Cloud is HashiCorp's managed service offering. It eliminates the need for unnecessary tooling and
documentation for practitioners, teams, and organizations to use Terraform in production. Provision infrastructure in a remote environment 
that is optimized for the Terraform workflow.

What is Terraform cloud used for?
Terraform Cloud enables infrastructure automation for provisioning, compliance, and management of any cloud, datacenter, and service

15-what is packer

>>packer is used to create only images
>>Packer is an open-source VM image creation tool from Hashicorp. 
It helps you automate the process of Virtual machine image creation on the cloud and on-prem virtualized environments.

16-what is terraform dry-run

>>Dry runs help you identify trouble spots, discover sections you may not fully understand, and get a better understanding of how everything fits together.
It's also a good way to find out if you don't fully understand how an activity is supposed to be conducted or how the virtual tools function.

17-What are the most useful Terraform commands?

terraform init - initializes the current directory
terraform refresh - refreshes the state file
terraform output - views Terraform outputs
terraform apply - applies the Terraform code and builds stuff
terraform destroy - destroys what has been built by Terraform
terraform graph - creates a DOT-formatted graph
terraform plan - a dry run to see what Terraform will do

18-What are some of the built-in provisioners available in Terraform?

Salt-masterless Provisioner
Remote-exec Provisioner
Puppet Provisioner
Local-exec Provisioner
Habitat Provisioner
File Provisioner
Chef Provisioner

19-What is terraform taint and untaint?

>>the commands comes under terrafomr forcing recreation of resources
>>terraform taint command manually marks a terraform-managed reosuce as tainted forcing it to be destroyed and recreated on the next apply
>>terraform untaint command manually umarks a terraform managed resource as tainted restoring it as the primary instance in the state

>>terraform state list
>>terraform taint "resource-name"
>>terraform taint aws_instance.my-ec2-vm-new
>>terraform plan
>>terraform apply --auto-approve
>>terraform untaint aws_instance.my-ec2-vm-new
>>terraform plan
>>terraform state mv anji.sh sampi.sh

20-what is the use of provisoners in packer and terraform

>>Terraform Provisioners are used for executing scripts or shell commands on a local or remote machine as part of resource creation/deletion. 
They are similar to “EC2 instance user data” scripts that only run once on the creation and if it fails terraform marks it tainted

21-What is a tuple Terraform?

>>A tuple is a strongly typed collection of one or more values. So, for example, we could define a tuple of three values: string , number , number , 
or two values: string ,string . Once a tuple is defined, it always has to contain the number of values defined in that tuple.

22-what is .tfstate in terraform

>>when your deploying any resources it will save your code in .tfstate file if any changes happen in code it will go and check the .tfstate file and it will change 
what need to chnage the resources. it will idempotant so it update the resource what changed only that resources change not changed all the rsources

23-is it possible to execute the dev.tfvars and prod.tfvars

>>No if you executed first dev.tfvars and later add prod.tfvars it will think need to change the .tfvars and destroy your first dev.tfvars and add into prod.tfvars 
so dont use this method you can use the instead of WorkSpaces. if your using workspace but inside prod workspace if your using the dev.tfvars  it will destroy the 
prod environment so be careful when you using the workspace

24-how you can give to other devlopers terraform.tfstate

>>when we deploying the code onthat time we saving the terraform.tfstate file in s3 bucket or any version control system then we can give that terraform.tfstae to
other developer he can easily understand the situvation and he can do chnages then the state file update when we can changes on that time we can use that
terraform.tfstate to avoid the complications and any errors or misunderstandings. so when the runing the backend it will run first backend so its give an error
need to add access-key and secret-key in your system. it will not take the access and secret key in your var.tf

25-what is lookup in terraform

>>The lookup() function can be used to lookup a particular value inside of a map , given its key and if the given key does not exist, the given default value is 
returned instead: lookup(map, key, default)

26-what is triggers

>>A map of values which should cause this set of provisioners to re-run values are mean to interpolated refernace varibales to attributes of other resources

27-what is DNS_Hostnames

>>For example, when a Web address (URL) is typed into a browser, a DNS query is made to learn an IP address of a Web server associated with that name. 
Using the www.example.com URL, example.com is the domain name, and www is the hostname. DNS resolution maps www.example.com into an IP address (such as 192.168.2.10)

28-what is variable

>>giving name to storage location. variable is a great way to define centrally controlled reusable values

29-what is terraform data-sources

>>Terraform data sources let you dynamically fetch data from APIs or other Terraform state backends. Examples of data sources include machine image IDs from a 
cloud provider or Terraform outputs from other configurations.

data "aws_instance" "anji" {
  instance_id = "i-78vula7*jkba"
}

output "instance" {
  value = data.aws_instance.anji.public_ip
} ##this will fecth the public ip of your instance only

30-what is terraform functions

>>The Terraform language includes a number of built-in functions that you can call from within expressions to transform and combine values. 
The general syntax for function calls is a function name followed by comma-separated arguments in parentheses: max(5, 12, 9)map

1-concat
2-element
3-join
4-map
5-slice
6-format
7-file
8-lookup
9-replace
10-max
11-min
12-timeadd
13-floor

31-what is dynamic blocks in terraform

>>Terraform provides the dynamic block to create repeatable nested blocks within a resource. A dynamic block is similar to the for expression. Where for creates 
repeatable top-level resources, like VNets, dynamic creates nested blocks within a top-level resource, like subnets within a VNet.

32-what is teraform lock file

>>terraform. lock. hcl , and this name is intended to signify that it is a lock file for various items that Terraform caches in the . terraform subdirectory of your
working directory. Terraform automatically creates or updates the dependency lock file each time you run the terraform init command.
>>it .terraform.lock.hcl holdes the version of AWS code and hashes version

33-what is terraform count

>>To manage several of the same resources, you can use either count or for_each , which removes the need to write a separate block of code for each one.

34-what is Terraform state locking 

>>any two developers working on same state you an able lok that file then second person get an error to avoid duplications. your mentiong in the backend file like 
s3 where s3 storing on that your mentioning and that will dynamodb lock the partition key must be LockID
>>you can loack that file 1 hour or 2 hours also example 
terraform {
  backend "s3" {
    bucket         = "your-s3-bucket"
    key            = "path/to/terraform.tfstate"
    region         = "your-aws-region"
    dynamodb_table = "terraform_lock_table"
    encrypt        = true
    acl            = "private"
  }
}

locals {
  lock_duration_hours = 1
}

35-what is terrafomr depends-on

>> Using depends_on you cna create the resources like you want create the vpc then after need to create the subnet then you can use the depends-on then it will
create when vpc created after it will create the reaming resource
depends_on = {
aws_vpc.default
}

36-when you created vpc and route tables using terrform and the data will be stored .tfstate file some one came and delete the route table it will recreate route table one more time

>>yes it will recreate the route table the data is stored in .tfstate so when some one deleted it will automatically created 

37-if you want apply one one instance or one module only how can you apply in terraform

>>terraform apply --target=module.vpc

38-my clinet is asking to setup same infra in 3 or 4 regions using same code and dont duplicate at a time need to execute what we can do

>>you cna do this way 
provider "aws" {
 region = "ap-south-1"
 region = "us-east-1"
 region = "us-east-1"
} 
>>if you want use ami means you need to use true or false because ami will be differnet in other regions

variable "ami" {
  description = "AMIs by region"
  default = {
    ap-south-1 = "ami-024c319d5d14b463e"
    us-east-1 = "ami-0149b2da6ceec4bb0"
  }
}

39-is this possible to do same configuration in another aws account

>>yes it possible to do using variables

40-You have a Terraform configuration file that defines an infrastructure deployment. However, there are multiple instances of the same resource that need to be created.How would you modify the configuration file to achieve this..??

>>To create multiple instances of the same resource in Terraform, you can utilize the count or for_each meta-arguments in your configuration file.

count:-
resource "aws_instance" "example" {
  count = 3

  # Configuration for each instance
  ami           = "ami-abc123"
  instance_type = "t2.micro"

  # Unique tags for each instance
  tags = {
    Name = "Instance ${count.index}"
  }
}

for_each:-
variable "instances" {
  type = map(object({
    ami           = string
    instance_type = string
  }))
  default = {
    "instance1" = {
      ami           = "ami-abc123"
      instance_type = "t2.micro"
    },
    "instance2" = {
      ami           = "ami-def456"
      instance_type = "t2.small"
    }
  }
}

resource "aws_instance" "example" {
  for_each = var.instances

  # Configuration for each instance
  ami           = each.value.ami
  instance_type = each.value.instance_type

  # Unique tags for each instance
  tags = {
    Name = each.key
  }
}

41-What exactly is Sentinel? Can you provide few examples where we can use for Sentinel policies..?

>>Sentinel is a policy-as-code framework developed by HashiCorp. It enables the implementation of fine-grained, automated policy enforcement for infrastructure provisioning and
deployment using Terraform. Sentinel policies help enforce security, compliance, and operational best practices.
>>Sentinel is terraform cloud or enterprise will provide restrict some types like instance types tags cost optimization easy way to build your infrastructure

1-Security Policies
2-Compliance Policies
3-Cost Optimization
4-Naming Conventions
5-Governance and Best Practices

42-How do you manage sensitive data in Terraform, such as API keys or passwords..?

1-Input Variables with Sensitive Flag
2- Environment Variables
3-External Secret Management Tools
4-Terraform Cloud: If you're using Terraform Cloud, you can set sensitive variables directly in the workspace. These variables are stored securely and only provided to runs as needed.
5-Vault: For a more sophisticated solution, you could use HashiCorp's Vault. Vault is a tool for securely managing secrets, and it integrates with Terraform.
	
43-Below command will destroy everything that is being created in the infrastructure. Tell us how would you save any particular resource while destroying the complete infrastructure.

>>When executing the terraform destroy command, it will remove all the resources created by Terraform and destroy the infrastructure. 
If you want to save a particular resource from being destroyed, you can use Terraform's resource lifecycle management feature.
>>To save a specific resource, you can modify the resource block in your Terraform configuration file by adding the lifecycle block and setting the prevent_destroy argument to true. 
This prevents Terraform from destroying that specific resource during the execution of terraform destroy
resource "aws_instance" "example" {
  # Resource configuration...

  lifecycle {
    prevent_destroy = true
  }
}

44-What are the main competitors of terraform list and describe them.

1-AWS CloudFormation
2-Azure Resource Manager (ARM) Templates
3-Google Cloud Deployment Manager
4-Ansible
5-Puppet

45-Define purpose of basic Terraform commands which you’ll use often

1-terraform init: Initializes a Terraform working directory. It downloads the necessary provider plugins and sets up the backend configuration.
2- terraform init -upgrade: Upgrades the Terraform modules and providers to the latest available versions, ensuring you have the most up-to-date features and bug fixes.
3-terraform plan: Creates an execution plan by comparing the current state of the infrastructure with the desired state defined in your Terraform configuration. 
It shows you the changes that Terraform will make without actually applying them.
4-terraform apply: Applies the changes defined in your Terraform configuration to create, modify, or delete resources. It prompts for confirmation before making any modifications.
5-terraform validate: Validates the syntax and configuration of your Terraform files. It checks for any errors or warnings in your code without making any changes to your infrastructure.
6-terraform fmt: Formats your Terraform configuration files to ensure consistent and readable code style. 
It automatically rearranges and indents the code according to the Terraform style guide.
7-terraform destroy: Destroys all the resources created by Terraform, effectively tearing down your infrastructure. It prompts for confirmation before removing the resources.

46-What is the Resource..??

>>In Terraform, a resource is a single entity in your infrastructure that you want to manage using Terraform. It could be a virtual machine, a database, a network interface, or any other
component that you want to provision, configure, or manage. A resource is defined using a Terraform configuration block that specifies the resource type, attributes, and dependencies.

47-What is Desired and Current State..?

>>In the context of infrastructure management tools like Terraform, the desired state refers to the configuration or specification of how the infrastructure should look like. 
It defines the intended state of resources, such as servers, networks, and databases, in terms of their properties, relationships, and settings.

>>On the other hand, the current state represents the actual state of the infrastructure at a given point in time. It reflects the existing resources, their attributes, and the 
relationships between them.

48-what are the functions you are using

>>joint
>>lookup
>>file

49-Is Terraform is mutable or immutable

>>terraform is mutable >>A mutable object can be changed after it's created, and an immutable object can't.

50-what is HCL(Hashicorp Conffiguration Language)

>>Hashicorp Configuration Language

51-what is terraform state mv command in 

>>>>terraform state mv command is used to rename existing resources and move a resource into a module and move a module into a module
>>if you were to just rename a resourcce or move it to another module and run terraform terraform apply will destroy and recraete the resource.
state mv allows you to just change the reference so you ccan avoid a create and destroy action

>>terraform state mv packet_device.worker packet_device.helper

52-what is terraform locking versions in different people in same team

>>your developed infrastructure using AWS version 3.74.0 all the infrastructure developed you pasted the code in github .tfstate some other person came and downloaded and the person is
intialized the code the AWS version 4.1.0 it may get an error right
so we can overcome this by using terraform locking version
>>if another person using different version this will make errors code will not work properly
>>if your not shared this lock.hcl file to other persons that may get error you need to validate the documentation everytime

terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "3.47.0"
      version = ">=3.47.0"   ##it will use latest version
      version = "<=3.47.0"   ##it will use 3.47.0 below version
      version = "~>3.47.0"   ##any one in the range 3.47.99 something it will not go or work this up versions 3.48.0 

    }
  }
}

53-your state is locked another person how he can use that file statefile

>>you can use certain locck time for backend file
terraform {
  backend "s3" {
    bucket         = "your-state-bucket"
    key            = "path/to/your/terraform.tfstate"
    region         = "us-east-1"  # Replace with your AWS region
    encrypt        = true
    dynamodb_table = "your-locking-table"
    lock_duration  = 1800  # Optional: Set lock duration (in seconds)
  }
}
or else 
Release Lock: After they are done, they should release the lock by running terraform destroy or terraform state unlock followed by terraform apply to ensure that the state file is not left locked.
>>terraform state unlock lock-id-1234567890    -->you will see the lock id in state file  -->The Terraform lock ID is typically displayed in the error message when you try to access a locked state file.
For example, if you try to run the terraform apply command and the state file is locked, you will see an error message like this: 
>>terraform state unlock -force lock-id-1234567890  

54-What would you do if you were working in a team and your Terraform operations were consistently failing due to state lock errors?

>>Consistent state lock errors imply that another operation is already in progress, or a previous operation failed and left the state locked. You can use the terraform force-unlock command to manually 
unlock the state. However, be careful, as forcing an unlock can potentially corrupt the state.

55-How would you securely manage a Terraform state file in a production environment?

>>For a production environment, the state file should be stored in a remote backend that supports encryption at rest. The backend should also support locking to prevent concurrent state operations.
The state file contains sensitive data, so it should be treated with the same level of care as any sensitive data.

56-What is the impact on the state file when you run terraform destroy?

>>The terraform destroy command destroys all resources managed by Terraform in the state file and then updates the state file to indicate that no resources are managed. After running this command,
the state file will still exist, but it will not contain any managed resources.

57-Explain a scenario where you might need to use terraform state rm and its implications.

>>The terraform state rm command removes items from the Terraform state. It does not destroy the actual infrastructure resource, just removes it from Terraform management. 
You might use this command when a resource has been manually deleted, and you need to reconcile the state file to match the actual infrastructure.

58-In what scenario might you need to use terraform state list and what information does it provide?

>>The terraform state list command is used to list all resources in the current state file. This can be helpful if you need to get an overview of all resources managed by Terraform, 
especially in large deployments.

59-If you accidentally deleted your Terraform state file, how would you recover?

>>If versioning is enabled on the remote state backend, you can recover a deleted state file by restoring the most recent version. If you're not using versioning or a remote backend, 
it's significantly harder to recover unless you have a backup of the state file. This highlights the importance of state file backups and using versioned remote backends.

60-Can you explain how the terraform taint command affects the state file?

>>The terraform taint command marks a resource as tainted within the state file. A tainted resource will be destroyed and recreated during the next terraform apply.
This can be useful if you know a specific resource has an issue and needs to be recreated.

61-Describe a situation where you'd use a map or list variable in Terraform and provide an example.

>>Map and list variables are useful in Terraform when we need to pass complex data structures as input to modules or to define repeated similar resources. 
For instance, let's say we are creating multiple Azure storage accounts with different names and locations. We can use a map variable for this purpose.

variable "storage_accounts" {
description = "Map of all storage accounts and their properties" type = map(object({
name  = string location = string
}))
default = {

account1 = { name = "account1", location = "West Europe" } account2 = { name = "account2", location = "North Europe" }
}

}
resource "azurerm_storage_account" "example" { for_each = var.storage_accounts
name	= each.value.name location	= each.value.location
resource_group_name = azurerm_resource_group.example.name account_tier	= "Standard"
account_replication_type = "GRS"

}

62-how you can pass the output value to another resource

# Define your AWS provider configuration here
provider "aws" {
  region = "us-east-1" # Replace with your desired AWS region
}

# Create an EC2 instance
resource "aws_instance" "example" {
  ami           = "ami-XXXXXXXXXXXXXXXXX" # Replace with your desired AMI ID
  instance_type = "t2.micro"              # Replace with your desired instance type
  # Add any other instance configuration here
}

terraform init
terraform apply

# ...

output "public_ip" {
  value = aws_instance.example.public_ip
}
terraform apply

# Define your AWS provider configuration here
provider "aws" {
  region = "us-east-1" # Replace with your desired AWS region
}

# Create a Route 53 record set
resource "aws_route53_record" "example" {
  zone_id = "YOUR_ZONE_ID" # Replace with your Route 53 hosted zone ID
  name    = "example.com" # Replace with your desired DNS name
  type    = "A"
  ttl     = "300"         # Replace with your desired TTL
  records = [aws_instance.example.public_ip]
}

63-Imagine you have different variable values for different stages (dev, test, prod) of your infrastructure. How would you structure your Terraform configurations to make it easy to apply changes to a
specific stage?

>>One common way to handle this is to use separate Terraform workspace for each environment and have a corresponding variable file for each workspace. 
You can use workspace-specific variable files (dev.tfvars, test.tfvars, etc.) and apply them with the -var-file flag.

64-You are managing a complex infrastructure with many interdependent resources. How would you use output variables to expose necessary data (like IP addresses, DNS names, etc.) for use in other
configurations or modules?

>>Output variables can be used to expose data from one module to another, or to the root module. By using outputs, the necessary data will be displayed in the console after running terraform apply. 
If the state is stored remotely, the output values can also be queried using terraform output command.

65-What is the difference between local values and input variables in Terraform? When might you use one over the other?

>>Both local values and input variables in Terraform can be used to assign a name to an expression, so it can be reused. The main difference is that input variables are parameters for a module, 
while local values are only within the module where they are defined. You might use local values for temporary complex objects or when a value is repeated many times, 
to make the configuration more DRY (Don't Repeat Yourself).

66-You run terraform apply, and it fails with an error that the resource you're trying to create already exists. What could be the cause, and how would you solve it?

>>This issue usually arises when the Terraform state file does not reflect the actual infrastructure. The resource might have been manually created or managed by a different Terraform state file. 
One way to resolve this is by importing the resource into the current Terraform state using terraform import.

67-After running terraform plan, you notice that Terraform intends to recreate a resource even though you haven't made any changes to its configuration. What might cause this, and how would you handle it?

>>This could be due to some default values in the resource configuration that Terraform doesn't correctly identify as unchanged, or external changes that Terraform picked up during refresh.
To understand why Terraform wants to recreate the resource, you can use the -detailed-exitcode flag with terraform plan which will provide more information about what's changing.

68-You've made some changes to your Terraform configuration, but when you run terraform plan, it's showing a much larger set of changes than you expected. How would you determine what's causing these extra changes?

>>It could be due to a number of factors like implicit dependencies, changes in default values, etc. You could examine the output of terraform plan to see which resources it's intending to change and why.
You might also find the terraform graph command useful to visualize dependencies between resources.

69-You're running terraform apply in an automated CI/CD pipeline, and it fails with a timeout error while waiting for a resource to be created. What steps could you take to address this issue?

>>This could be addressed by adjusting the timeout settings in your Terraform configuration, using the timeouts block. You could also investigate whether there are any issues with the cloud provider 
that's causing resource creation to take longer than expected.

70-A Terraform module you're using from the Terraform Registry is causing errors during terraform apply. What steps can you take to investigate and resolve this issue?

>>You can review the module's documentation and source code to try to understand what might be causing the issue. Check the module's issue tracker for similar problems reported by others.
If you can't find a solution, consider raising a new issue on the tracker.

71-Terraform fails during planning with an error message "Error: Reference to undeclared resource". How would you fix this?

>>This error occurs when your configuration refers to a resource that Terraform doesn't know about. This could be due to a typo in the resource name, or the resource not being declared in your configuration.
Ensure that the referred resource is correctly declared in your configuration.

72-When applying a Terraform configuration, you receive an error that a required attribute is missing, but you see it defined in your configuration. What could be happening here?

>>This might be due to the attribute being in the wrong block or being incorrectly named in your configuration. Verify that the attribute is correctly named and placed according to the resource's documentation.
Also, check for syntax errors or incorrect variable references.

73-You made changes to your Terraform configuration, but when you run terraform plan, it doesn't detect any changes. What might be happening?

>>This could be due to a syntax error in your configuration causing Terraform to not recognize your changes, or your changes might not actually result in any changes to the infrastructure.
Check your configuration for syntax errors and verify that your changes should result in changes to the infrastructure.

74-When working with multiple Terraform workspaces, you receive an error that a resource already exists. How would you investigate this?

>>Check to make sure you're in the correct workspace by running terraform workspace show. Resources can overlap between workspaces if they're not differentiated in some way. 
Consider using the workspace name in your resource names to avoid collisions between workspaces.

75-Can you describe a scenario where you might need to manually edit a Terraform state file? What are the risks?

>>Manually editing a Terraform state file is highly discouraged. The state file contains crucial information about the infrastructure managed by Terraform, and any discrepancies between the 
actual infrastructure and the state file can cause significant problems. In cases where manual intervention is unavoidable, such as when a resource has been manually deleted, terraform taint or
terraform import are safer alternatives to direct state manipulation.

76-How does Terraform handle state locking, and how does it prevent conflicts in a team environment?

>>State locking is a feature of Terraform that locks the state file when an operation is being performed, preventing others from running Terraform commands that could interfere. 
This prevents conflicts in the state file and ensures consistency. State locking is available with certain backends that support it, such as the S3 backend when used with DynamoDB.

77-If you are working on multiple separate Terraform projects, how can you ensure that the state files don't get mixed up?

>>To manage the state files for separate projects, you can use separate backends for each project or use the workspace feature of Terraform. Workspaces allow you to have separate state files in the 
same backend, effectively separating resources between environments or projects.

78-Describe a situation where you would use remote state in Terraform.

>>Remote state is particularly useful when working as part of a team or when you need to keep the state of your infrastructure secure yet accessible. Remote state allows the state data to be stored 
in a remote data store, such as AWS S3, Azure Blob Storage, Google Cloud Storage, and others. This ensures everyone on the team has access to the latest state of the infrastructure.

79-What command would you use to pull the latest state file, and why might you need to do this?

>>You can use the terraform refresh command to update the local state file against the real resources. This is useful to ensure the state file accurately represents the real- world resources, especially 
when changes might have been made outside of Terraform.

80-What strategies can you use to mitigate the risk of state file corruption or loss?

>>Using a remote backend that supports versioning can help prevent loss or corruption of the state file. Versioning allows you to roll back to a previous version if something goes wrong. Additionally, 
regular backups of the state file are also a good practice to recover from accidental deletion or corruption.

81-How would you handle a situation where your local Terraform state is out of sync with the actual infrastructure?

>>If the local state is out of sync, you might use the terraform refresh command to update the local state file with the actual infrastructure's state. 
This command queries the providers for each resource's actual status and updates the state file accordingly.

83-You need to install or copy specific software on an AWS EC2 instance after it's created. How would you achieve this using provisioners?

variable "amis" {
    description = "AMIs by region"
    default = {
      us-east-1 = "ami-97785bed" # ubuntu 14.04 LTS
		  us-east-2 = "ami-f63b1193" # ubuntu 14.04 LTS
		  us-west-1 = "ami-824c4ee2" # ubuntu 14.04 LTS
		  us-west-2 = "ami-f2d3638a" # ubuntu 14.04 LTS
    }
}
aws_region = "us-east-1"
resource "aws_instance" "private-instances" {
    count = "${var.environment == "prod" ? 3 : 1}"
    ami = "${lookup(var.amis, var.aws_region, "ap-south-1")}"
    instance_type = "t2.micro"
    key_name = "chefkeypair"
    subnet_id = "${element(aws_subnet.private-subnets.*.id, count.index)}"
    vpc_security_group_ids = ["${aws_security_group.allow_all.id}]
    #associate_public_ip_address = true
    tags = {
        Name = "private-server-${count.index+1}"
    }
}
resource "null_resource" "execute" {
depends_on = [aws_instance.public-instances]
    connection {
        type = "ssh"
        host = aws_instance.foo.public_ip
        user = "ubuntu"
        private_key = "${file("test.pem")}"
    }
    provisioner "file" {
    source      = "anji.sh"
    destination = "/tmp/anji.sh"
  }

  provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/anji.sh",
      "/tmp/anji.sh",
    ]
  }

    provisioner "local-exec" {
    command = "echo '${aws_instance.foo.public_ip}' > instance_ip.txt"
    }
    provisioner "remote-exec" {
        inline = [
        "sleep 60",
        "sudo apt-get update -y",
        "sleep 20",
        "sudo curl https://get.docker.com | bash",
        "docker pull mongo", 
        "sudo docker run -d -p 27017:27017 -e username=mongoadmin -e mongo_password=password -e mango_database=test  mongo",
        "docker ps",
        ]
    }  
}

84-how to open all ports incomming ports using terraform 

ingress = [
  for port in [20, 80, 443, 8080, 9000] : {
    description   = "inbound rules"
    from_port     = port
    to_port       = port
    protocol      = "tcp"
    cidr_blocks   = ["0.0.0.0/0"]
    ipv6_cidr_blocks = []
    prefix_list_ids = []
    security_groups = []
    self            = false  
  }
] 


ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  } 
single port only 
ingress {
    description = "http"
    from_port = 80
    to_port = 80 
    protocol = "tcp"
    cidr_blocks = ["0.0.0.0/0"]

}
egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1" 
    cidr_blocks = ["0.0.0.0/0"]
  }

85-how to configure secuirty group of instance anji system will access the sampi instance using secuirty grup using terrfform

# aws_setup.tf

provider "aws" {
  region = "your_aws_region"
}

# Security Group for Anji instance
resource "aws_security_group" "anji_security_group" {
  name        = "anji-se"
  description = "Security group for Anji instance"

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# EC2 Instance for Anji
resource "aws_instance" "anji_instance" {
  ami           = "your_ami_id"  # Specify the AMI ID for your region
  instance_type = "t2.micro"
  key_name      = "your_key_pair_name"  # Replace with your key pair name

  security_group = [aws_security_group.anji_security_group.id]

  tags = {
    Name = "anji-instance"
  }
}

# Security Group for Sampi instance
resource "aws_security_group" "sampi_security_group" {
  name        = "sampi-se"
  description = "Security group for Sampi instance"

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    security_groups = [aws_security_group.anji_security_group.id]
  }
}

# EC2 Instance for Sampi
resource "aws_instance" "sampi_instance" {
  ami           = "your_ami_id"  # Specify the AMI ID for your region
  instance_type = "t2.micro"
  key_name      = "your_key_pair_name"  # Replace with your key pair name

  security_group = [aws_security_group.sampi_security_group.id]

  tags = {
    Name = "sampi-instance"
  }
}

////////////////////////////////////////////////////////////Second-type-multiple-security-groups//////////////////////////////////////////////////////////////////////

resource "aws_security_group_rule" "http_ingress_rule" {

  type              = "ingress"
  from_port         = 80
  to_port           = 80
  protocol          = "tcp"
  cidr_blocks       = ["0.0.0.0/0"]
}

resource "aws_security_group_rule" "ssh_ingress_rule" {
 
  type              = "ingress"
  from_port         = 22
  to_port           = 22
  protocol          = "tcp"
  cidr_blocks       = ["aws_security_group.sampi_security_group.id"]  # Replace <sampi_system_IP> with the actual IP address of Sampi system
}
# Security Group for Sampi instance
resource "aws_security_group" "sampi_security_group" {
  name        = "sampi-se"
  description = "Security group for Sampi instance"

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks       = ["0.0.0.0/0"]
  }
}
##sampi system only take ssh possible 
resource "aws_instance" "anjis_instance" {
  ami           = "ami-xxxxxxxxxxxxxxxxx"
  instance_type = "t2.micro"
  associate_public_ip_address = false
  # Other instance configuration parameters...

  vpc_security_group_ids = [
    aws_security_group.http_ingress_rule.id,
    aws_security_group.ssh_ingress_rule.id,
    # Add other security groups as needed
  ]
}
##sampi system
resource "aws_instance" "sampi_instance" {
  ami           = "ami-xxxxxxxxxxxxxxxxx"
  instance_type = "t2.micro"
  associate_public_ip_address = true
  # Other instance configuration parameters...
  security_group = [aws_security_group.sampi_security_group.id]  
}

86-why IAC  -->Infrastructure As Code 

>>faster time to release 
>>Improved consistency,less configuration drift
>>faster, more efficient development
>>Lower costs and more time developing and fewer operations 

87-what are the terraform module types 

1-root module 
2-child module 
3-publish module 
4-

88-how to attach multiple security groups to ec2 inatnce 

vpc_security_group_ids = [
    "sg-xxxxxxxxxxxxxxxxx",  # Security Group 1
    "sg-yyyyyyyyyyyyyyyyy",  # Security Group 2
    "sg-zzzzzzzzzzzzzzzzz"   # Security Group 3
]

89-you have three system you need to get the public ip as output 

output "public_ip" {
  value = aws_instance.web.*.public_ip ##this will print two or three  public ips 
}

90-you need two instance one is t2micro another is t3.micro this need to execute single code using terraform

resource "aws_instance" "example_instance" {
  count = length(var.component)
  ami           = "ami-xxxxxxxxxxxxxxxx" # specify your AMI ID
  instance_type = "t2.micro"             # specify the instance type
  security_groups = [var.security_group_id]

  tags = {
    Name = var.component[count.index]#this will give cart and catalogue as name first one 
     
  }
}


variable "component" {
  value = ["cart", "catalogue"]
}


resource "aws_instance" "web" {
  for_each  = var.component
  ami           = "ami-xxxxxxxxxxxxxxxx" # specify your AMI ID
  instance_type = each.value.instance_type             # specify the instance type
  security_groups = [var.security_group_id]
  
  tags = {
    Name = each.value["name"]
  }
}

variable "component" {
  default {
    cart = {
      name = "cart"
      instance_type = "t3.micro"
    }
    catalogue = {
      name = "catalogue"
      instance_type = "t2.micro"
    }
  }
}

output "publicip" {
  value =aws_instance.web["cart"].public_ip ##this will get only cart public ip 
}

output "publicip" {
  value = {
    for k, v in aws_instance.web : k => v.public_ip  
    #it wil fecth the aws_instance.web under it will check the k=cart then check the public ip same for k=catalogue will be show the public ip 
  }
} 


variable "instances" {
  default = [
    {
      name          = "cart"
      ami           = "xxxxxxxxxxxxxxxxxxxx"
      instance_type = "t3.micro"
    },
    {
      name          = "catalogue"
      ami           = "xxxxxxxxxxxxxxxxxxxx"
      instance_type = "t2.micro"
    },
  ]
}

resource "aws_instance" "web" {
  for_each =  var.instances

  ami           = each.value.ami
  instance_type = each.value.instance_type

  tags = {
    Name = each.key
  }
}

if your using modules then like this 

mkdir module 
cd module 
vi main.tf 

resource "aws_instance" "web" {
  ami                = *************************
  instance_type      = var.instance_type

  tags = {
    Name = var.name 
  }

}
variable instance_type {}
variable name {}

:wq

cd ..
vi main.tf 

variable "component" {
  default = {
    cart = {
      name          = "cart"
      instance_type = "t3.micro"
    }
    catalogue = {
      name          = "catalogue"
      instance_type = "t2.micro" 
    }
  }
}

module "ec2" {
  source   = "./module"

  for_each   = var.component
  instance_type = each.value.instance_type
  name = each.value.name
}

output "publicip" {
  value = {
    for k, v in module.ec2 : k => v["ec2"].public_ip  
    #it wil fecth the aws_instance.web under it will check the k=cart then check the public ip same for k=catalogue will be show the public ip 
  }
} 

91-what is terraform local 

>>the big expression need to be used in multiple places and locals can help you keeping it one place and you refer with another simple variable 

locals {
  instance_type  = { for k, v in var.components : k => v.instance_type }
  name           = { for k, v in var.components : k => v.name }
}

output "instance_type" {
  value = local.instance_type
}

output "name" {
  value = local.name 
}

92-how to connect ec2 instance using terraform provisioners

resource "aws_instance" "example_instance" {
  ami             = "ami-xxxxxxxxxxxxxxxx" # Replace with your AMI ID
  instance_type   = "t2.micro"
  key_name        = "your_key_pair_name"   # Replace with your key pair name
  security_group_ids = ["sg-xxxxxxxxxxxxxxxx"] # Replace with your security group ID

  # Other instance configurations...

  provisioner "remote-exec" {

    inline = [
      "echo 'Hello, this command is running on the instance.'",
      "sudo apt-get update",
      # Add any other commands you want to execute on the instance
    ]

    connection {
      type     = "ssh"  ##you can username and password also 
      user     = "ubuntu" # Replace with your instance's default user
      private_key = file("path/to/your/private-key.pem") # Replace with the path to your private key
      host     = self.public_ip # Terraform's self variable to get the public IP of the instance
    }
  }
}

resource "null_resource" "provision" {
    triggers = {
      instance_id = aws_instance.example_instance.id  ##this will if the system restart or terminate then it will connect or do it again  
    }
      provisioner "remote-exec" {

    inline = [
      "echo 'Hello, this command is running on the instance.'",
      "sudo apt-get update",
      # Add any other commands you want to execute on the instance
    ]

    connection {
      type     = "ssh"  ##you can username and password also 
      user     = "ubuntu" # Replace with your instance's default user
      private_key = file("path/to/your/private-key.pem") # Replace with the path to your private key
      host     = self.public_ip # Terraform's self variable to get the public IP of the instance
    }
  }
}  

93-how to configure write a code to two differnet AWS account

aws configure --profile anji(prod)
access_key: *************************
secret_key: *************************
region: *****************

aws configure --profile sampi(dev)
access_key: *************************
secret_key: *************************
region: *****************
provider "aws" {
    alias = "prod"
    region = "us-east-1"  ##it will be first account so 
    profile = "anji(prod)"
}

provider "aws" {
    alias = "dev"  ##you need to give this compulsory with this it cant work 
    region = ap-south-1
    profile = sampi(dev)
}    

resource "aws_instance" "ec2_main" {
  provider        = aws.prod 
  ami             = var.ami_apsoutheast
  instance_type   = "t2.micro"
  key_name        = "linux-sea-key"
  security_groups = ["ansible-sg"]
  tags = {
    Name    = "account-main",
    Project = "multiprovider",
    Region  = "ap-southeast-1"
  }
}
resource "aws_instance" "ec2_dev" {
  provider        = aws.dev
  ami             = var.ami_useast
  instance_type   = "t2.micro"
  key_name        = "linux-useast-key"
  security_groups = ["ansible-sg"]
  tags = {
    Name    = "account-dev",
    Project = "multiprovider",
    Region  = "ap-southeast-1"
  }
}

variable "ami_apsoutheast" {
  type    = string
  default = "ami-0af2f764c580cc1f9"
}

variable "ami_useast" {
  type    = string
  default = "ami-00c39f71452c08778"
}
resource "aws_s3_bucket" {
    provider = aws.prod 
    bucket = "anji159258"
    acl    =  "public-read"
    lifecycle {
        prevent_destroy = true 
    }  

}  ##this is anji account automatically taken 

resource "aws_s3_bucket" {
    provider = aws.dev 
    bucket = "anji1592589"
    acl    =  "public-read"
    lifecycle {
        prevent_destroy = true 
    }  

}  

94-your using modules you need to print the public ip address of all the instances 

#it will print the ip address of all inside ec2 module 
output "publicip" {
  value = module.ec2
}

95-what is terraform locals 

>>In Terraform, locals are used to declare variables that are used within a module to simplify expressions or reuse values. Locals are evaluated during the Terraform graph building phase and can be useful for
making configurations more readable and maintainable. Here's a simple example of using locals in a Terraform configuration.
>>you can store localy that refer anywhere fro example public_ip printed 10 places rather than you can store in local call that using output 

# main.tf

provider "aws" {
  region = "us-east-1"
}

locals {
  resource_prefix = "myapp"
}

resource "aws_s3_bucket" "example_bucket" {
  bucket = "${local.resource_prefix}-s3-bucket"
  acl    = "private"
}

resource "aws_dynamodb_table" "example_table" {
  name           = "${local.resource_prefix}_dynamodb_table"
  read_capacity  = 5
  write_capacity = 5
  hash_key       = "id"
  attribute {
    name = "id"
    type = "N"
  }
}

96-can i use inside the aws_instance provision and outside also example 

resource "aws_instance" "web" {
    ami                   = "xxxxxxxxxxxxxxxxxxxxxx"
    instance_type         = "t2.micro"
    #it will create the 2 instance 
    count                 = 2
    ##it will create two instances based on the components you mentiod if write frontend in components it will create three   
    #first way these see below varaibles
    count                 = lenth(var.components) ## 

    tags = {
        #it will give the name one instance name is cart another instance name is catalogue  
        Name = var.components[count.index]
    }


  provisioner "remote-execc" {
    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = ec2-user  
      private_key = "${file("test.pem")}" 
    }

    inline = [
      "false"
    ]
  }
}    

#outside youcan use null resource outsie of the instance

resource "aws_instance" "web" {
    ami                   = "xxxxxxxxxxxxxxxxxxxxxx"
    instance_type         = "t2.micro"
    #it will create the 2 instance 
    count                 = 2
    ##it will create two instances based on the components you mentiod if write frontend in components it will create three   
    #first way these see below varaibles
    count                 = lenth(var.components) ## 

    tags = {
        #it will give the name one instance name is cart another instance name is catalogue  
        Name = var.components[count.index]
    }
}

resource "null_resource" "execute" {
depends_on = [aws_instance.public-instances]
    connection {
        type = "ssh"
        host = aws_instance.web[1].public_ip  ##it will connect the first system only 
        user = "ubuntu"
        private_key = "${file("test.pem")}"
    }
    provisioner "file" {
    source      = "anji.sh"
    destination = "/tmp/anji.sh"
  }
}

97-your created 6 subnets but you want choose only one subnet for ec2 instance

>>using like this 
subnet_id  = aws_subnet.main.*.id[0] #this will select the first subnet 

98-how you can configure lookup in terraform 

vpc = {
  main = {
    cidr_block        = "10.0.0.0/16"
    availability_zone = ["us-east-1a", "us-east-1b"]
    public_subnets = {
      public = {
        name        = "public"
        cidr_block  = ["10.0.0.0/24", "10.0.1.0/24"]
        internet_gw = true
      }
    }
    private_subnets = {
      web = {
        name       = "web"
        cidr_block = ["10.0.2.0/24", "10.0.3.0/24"]
        nat_gw     = true
      }
      app = {
        name       = "app"
        cidr_block = ["10.0.4.0/24", "10.0.5.0/24"]
        nat_gw     = true
      }
      db = {
        name       = "db"
        cidr_block = ["10.0.6.0/24", "10.0.7.0/24"]
        nat_gw     = true
      }
    }

docdb = {
  main = {
    vpc_name            = "main"
    subnets_name        = "db"
    engine_version      = "4.0.0"
    number_of_instances = 1
    instance_class      = "db.t3.medium"
  }
}


module "docdb" {
  source = "github.com/raghudevopsb70/tf-module-docdb"
  env    = var.env

  for_each            = var.docdb   ##its like take all the values from docdb only 
  subnet_ids          = lookup(lookup(lookup(lookup(module.vpc, each.value.vpc_name, null), "private_subnet_ids", null), each.value.subnets_name, null), "subnet_ids", null)  ##private_subnets will takeing the
                               output of your vpc then each.value.subnets taking from docdb then subnet_ids will be output 
  vpc_id              = lookup(lookup(module.vpc, each.value.vpc_name, null), "vpc_id", null)
  allow_cidr          = lookup(lookup(lookup(lookup(var.vpc, each.value.vpc_name, null), "private_subnets", null), "app", null), "cidr_block", null)
  engine_version      = each.value.engine_version
  number_of_instances = each.value.number_of_instances
  instance_class      = each.value.instance_class
}
>>see the video of 52 and time 52mins  

99-what is try in terrafrom 

>>see video 60 15 mins  

100-if you use this like this count = var.listerner == ? 1 : 0

1->true
0->false   -->its basically true or false only if 1 means that condition execute if 0 menas that condition will not execute 

101-how you can install node exporter in any server is created using auto-scaling while using terafrom 

resource "aws_launch_configuration" "example" {
  name = "example_config"

  image_id = "ami-12345678"  # Replace with your AMI ID
  instance_type = "t2.micro" # Replace with your instance type

  lifecycle {
    create_before_destroy = true
  }

  user_data = <<-EOF
              #!/bin/bash
              curl -LO https://example.com/install_node_exporter.sh
              chmod +x install_node_exporter.sh
              ./install_node_exporter.sh
              EOF

  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_autoscaling_group" "example" {
  desired_capacity     = 2
  max_size             = 4
  min_size             = 2
  launch_configuration = aws_launch_configuration.example.id

  # Rest of your autoscaling configuration...
}

vi install_node_exporter.sh

#!/bin/bash

# Node Exporter version to install (update accordingly)
NODE_EXPORTER_VERSION="1.2.3"

# Download and install Node Exporter
curl -LO "https://github.com/prometheus/node_exporter/releases/download/v${NODE_EXPORTER_VERSION}/node_exporter-${NODE_EXPORTER_VERSION}.linux-amd64.tar.gz"
tar -xvf "node_exporter-${NODE_EXPORTER_VERSION}.linux-amd64.tar.gz"
sudo mv "node_exporter-${NODE_EXPORTER_VERSION}.linux-amd64/node_exporter" /usr/local/bin/

# Create a non-privileged user for Node Exporter
sudo useradd -rs /bin/false node_exporter

# Create a systemd service for Node Exporter
sudo tee /etc/systemd/system/node_exporter.service > /dev/null <<EOF
[Unit]
Description=Node Exporter
Wants=network-online.target
After=network-online.target

[Service]
User=node_exporter
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=default.target
EOF

# Reload systemd and start Node Exporter
sudo systemctl daemon-reload
sudo systemctl enable node_exporter
sudo systemctl start node_exporter

:wq

102-what is tfswitch 

>>tfswitch is a command-line tool that allows you to switch between different versions of Terraform seamlessly. It simplifies managing multiple Terraform versions on your system and ensures that you can easily
switch between them based on your project requirements.
>>For example, if you have projects that require Terraform version 0.14 and others that require version 0.15, TFSwitch enables you to switch between these versions effortlessly. By using TFSwitch, 
you can ensure that you're using the correct Terraform version for each project without the need to uninstall or reinstall different versions manually.
# Install tfswitch
brew install warrensbox/tap/tfswitch

# List available Terraform versions
tfswitch --list

# Switch to a specific version (e.g., 0.14.0)
tfswitch 0.14.0

# Check the currently active Terraform version
terraform version

# Run Terraform commands
terraform init
terraform plan
terraform apply

103-what happen if my terraform statefile deleted how to terminate the instance without going to the aws console

>>If your Terraform state file is deleted, it becomes challenging to manage your AWS resources using Terraform because Terraform relies on the state file to understand the current state of your infrastructure.
However, you can still manually terminate the instances in your AWS account without using the AWS Management Console by following these steps:

aws ec2 describe-instances
aws ec2 terminate-instances --instance-ids <instance-id>
aws ec2 delete-security-group --group-id <security-group-id>

104-types of variables in terraform

1-string
2-number
3-bool
4-list
5-set
6-map

105-if you lost statefile how we can achive this or recovery this 

>>if you lost the statefile you need to go what are the resource you have created then import command using then import it will create the statefile like what resource you created 
>>so its headache task instead you can use version control then you can recovery 
>>when you used backend statefile s3 bucket your executinng terraform using that file it will lock until your script complete and it wont allow the parallel execution
>>when you created the statefile using s3 and you given terraform code to another one he can terraform init then using that backend file then he can write new code or any changes do in previous code also work 

106-if you have prod.tfvars and dev.tfvars how to use 

>>For example you prod. tfvars and dev.tfvars then you can terraform apply --var-file=prod.tfvars
>>Prod.tfvars file under one region or production server details dev.tfvars under development details you can segergate your data or code to divide the environment
>>Also but use different folders for any confusion

>>suppose you created the anji.tf and variable.tf file and you passwd the variables in prod.tfvars and my manager come and say you need to create the same infrastructure with differnt name i will create 
vinod.tfvars and i will change the vaues accroding to that you can good to go no need to change the values in variable.tf so its easy to use anji.tf and variable.tf you can change anything in .tfvars file
>>terraform apply --var-file=vindo.tfvars -auto-approve  
>>if your not given terrform apply but not given any .tfvars and your passed that .tfvars it will ask like this igw_name and vpc_name so you need to pass that in .tfvars file or variables.tf   
>>if you executed same prod.tfvars and dev.tfvars in a single terrafrom will think need to destroy old thinks and create new environment because of .state file why means in a folder only have one .state file
he think need to modify those things  

>>Terraform is cloud against but coming different cloud code will change if you take ping or git any environment it will work same command also same

107-Iam Downloaded my code to github and .state file iam executing prod.tfvars and dev.tfvars my environment is destroying and creating new why could tell me 

>>because of this reason if you executed same prod.tfvars and dev.tfvars in a single terrafrom will think need to destroy old thinks and create new environment because of .state file why means in a folder 
only have one .state file he think need to modify those things  
>>dont use this method there is chance to overwrite the environemnt.instead you can use workspace or separate folders 
>>terraform workspae show 
>>terraform workspace new anji  -->it will create anji 
>>terraform workspace new sampi  -->it will create sampi 
>>Terraform workspace list 
default 
anji 
sampi *
>>terraform apply --var-file=prod.tfvars --auto-approve 
>>terraform workspace select anji   -->your going anji 
>>terraform apply --var-file=dev.tfvars --auto-approve   -->like this you can execute instead a single enviorment 

108-iam createing a bucket and paste the code backend to upload the.tfstaste file but when initilizing the code it gives an error why 

>>because of credentailas your passing the credeintial in .tfvars or variable file but when you creating the backend it will check local if you caonfigure local means it will work if youe not configure 
your credeintial in local but your using credeintial in .tfvars it will error 

aws config   -->here enter you credeintial it will not give any error 

vi prof.tfvars 

aws_acces_key  = XXXXXXXXXXXXXXXXXXXXXXXXX
aws_secret_key = XXXXXXXXXXXXXXXXXXXXXXXXX   -->if you giving like this it will give error 

109-what is terraform console 

>>it is like console to execute your code or practice the commands of functions or 
>>see video sreeharsha video 19 

110-what is element in terraform

>>terraform is element will use to retriaves single from a list 
element(list, index)

Ex:-
resource "aws_subnet" "public-subnets" {
    count = "${length(var.public-cidrs)}"
    vpc_id = "${aws_vpc.default.id}"
    cidr_block = "${element(var.public-cidrs, count.index)}"
    availability_zone = "${element(var.azs, count.index)}"

    tags = {
        Name = "${aws_vpc.default.id}-private-subnet-${count.index +1}"
    }
}

variable "azs" {
  description = "Run the EC2 Instances in these Availability Zones"
  default = ["us-east-1a", "us-east-1b", "us-east-1c"]
}

variable "public-cidrs" {
  description = "Run the EC2 Instances in these Availability Zones"
  default = ["10.1.1.0/24", "10.1.2.0/24", "10.1.3.0/24"]
}

111-your using this    subnet_id = "${element(aws_subnet.private-subnets.*.id, count.index)}" bu i need 2nd subnet how we can 

>>yes there is chance you can do that 

resource "aws_subnet" "private-subnets" {
    count = "${length(var.private-cidrs)}"
    vpc_id = "${aws_vpc.default.id}"
    cidr_block = "${element(var.private-cidrs, count.index + 1)}"
    availability_zone = "${element(var.azs, count.index)}"

    tags = {
        Name = "${aws_vpc.default.id}-public-subnet-${count.index + 1}"
    }
}

>>subnet_id = "${element(aws_subnet.private-subnets.1.id, count.index)}"   -->it will take 2 subnets 

by default it will take or else. if your used this means you will start from 1 
resource "aws_subnet" "private-subnets" {
    count = "${length(var.private-cidrs)}"
    vpc_id = "${aws_vpc.default.id}"
    cidr_block = "${element(var.private-cidrs, count.index + 1)}"
    availability_zone = "${element(var.azs, count.index)}"

    tags = {
        Name = "${aws_vpc.default.id}-public-subnet-${count.index + 1}"
    }
}

>>subnet_id = "${element(aws_subnet.private-subnets.2.id, count.index)}"  
>>subnet_id = "${element(aws_subnet.private-subnets.*.id, count.index)}"    -->all subnets 

>>this is called splat syntax 

112-how to use your userdata in terraform 

user_data = <<-EOF 
            #!/bin/bash
            yum update -y 
            yum install -y nginx 
EOF  
  
113-how to use lookup in terrafrom

variable "amis" {
    description = "AMIs by region"
    default = {
      us-east-1 = "ami-97785bed" # ubuntu 14.04 LTS
		  us-east-2 = "ami-f63b1193" # ubuntu 14.04 LTS
		  us-west-1 = "ami-824c4ee2" # ubuntu 14.04 LTS
		  us-west-2 = "ami-f2d3638a" # ubuntu 14.04 LTS
    }
}

resource "aws_instance" "public-instances" {
    count = "${var.environment  == "prod" ? 3 : 1}"     # if production environmet=prod it will deploy 3 otherwise it will deploy 1 only
    ami = "${lookup(var.amis, var.aws_region, "us-east-1")}"      #it will select the region based ami
    instance_type = "t2.micro"
    key_name = "chefkeypair"
    subnet_id = "${element(aws_subnet.public-subnets.*.id, count.index)}"
    vpc_security_group_ids = ["${aws_security_group.allow_all.id}"]
    associate_public_ip_address = true
   
    tags = {
        Name = "public-server-${count.index+1}"

    }
}     ##it will take  us-east-1 region ami only like this you can mention 

114-if you run null resource it will it will execute without changing naything if you want run null resource second it will not work why means you need to change the code or modify the code or you can use 
taint then you can execute the null resource terraform taint null_resource.cluster[0] ##if you have multiple means you can use that [0] if you one only dont use [0] directly taken 

115-local exec will fetch the data from server and remote exec will login into server and execute the commands  

116-what is locals in Terraform

>>Terraform local is used to express the value local system or local module only  
>>terraform module is repeatable code use many times 

# Input Variables
# AWS Region
variable "aws_region" {
  description = "Region in which AWS Resources to be created"
  type = string
  default = "us-east-1"  
}
# Environment Variable
variable "environment" {
  description = "Environment Variable used as a prefix"
  type = string
  default = "dev"
}
# Business Division
variable "business_divsion" {
  description = "Business Division in the large organization this Infrastructure belongs"
  type = string
  default = "HR"
}

# Define Local Values in Terraform
locals {
  owners = var.business_divsion
  environment = var.environment
  name = "${var.business_divsion}-${var.environment}"
  common_tags = {
    owners = local.owners
    environment = local.environment     
  }
}

variable "vpc_name" {
  description = "VPC Name"
  type = string 
  default = "myvpc"
}

module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  #version = "2.78.0"
  #version = "~> 2.78"
  version = "5.2.0"  

  # VPC Basic Details 
  #the name will be like this  HR-dev-myvpc
  name = "${local.name}-${var.vpc_name}"
  name= local.name  ##this also work you can use ffirst one or this also work
  cidr = var.vpc_cidr_block
  azs             = var.vpc_availability_zones
  public_subnets  = var.vpc_public_subnets
  private_subnets = var.vpc_private_subnets  

  tags = local.common_tags #like this owner = HR , environemnt = dev 
  vpc_tags = local.common_tags  #like this owner = HR , environemnt = dev 

}  

117-I have code and i want to deploy 3 different environments how would you do that 

1-Use 3 times in different folders and apply 
2-Use .tfvars with workspaces 
3-Use modules:-
mkdir modules
cd modules
mkdir terraform_infra
cd terraform_infra
vi main.tf 
vi variable.tf 
vi output.tf 
cd ../..
vi main.tf 

provider "aws" {}

#Create Infra for Testing Env 

module "testing-for-DEV" {
  source = "./modules/terraform_infra"

  Vpc_cidr   = "10.1.0.0/16"
  key_name   = "anjitest"
  enviorment = "dev"
}

#Create Infra for Testing Env 

module "testing-for-PROD" {
  source = "./modules/terraform_infra"
}

#Create Infra for Testing Env 

module "testing-for-TEST" {
  source = "./modules/terraform_infra"
}

118-your or someone can and added tags or anything in aws console but that is not ther in terraform.state file so you if you run the code
 it will remove if you want keep that means you can ignore changes in terraform lifecycle 

119-packer validate packer.json
packer inspect packer.json 
packer build packer.json 

120-no need to move or copy the path you can directly install terraform in linux 
>>cd /usr/local/bin/ 
>>wget https://
>>cd ../../../
>>Terraform -v 

121-how to change directory while cloning a git moving to another directory

122-build after any output file is artifact like if your build packer then you will get ami that is artifact if your building java then you will get .jar or .war file that is also artifact 

123-how you can you use for_each concept in terraform

>>for example you have this kind of variables

variable "component" {
  default = {
    cart = {
      Name          =  "cart"
      instance_type =  "t3.micro"  
    }
    catalogue = {
      Name          = "catalogue"
      instance_type = "t2.micro"
    }
  }
}

resoures "aws_instance" "anji" {
  # it will count the components how many and creat that much instead of using count you can use for_each
  for_each          = var.components
  ami               = hja0ijka-0np0
  #it will take the components inside cart will t3.micro and catalogue will be t2.micro 
  instance_type     = each.value.instance_type 

  tags = {
    Name = each.value["name"] #use this also work each.value.name 
  }
}

output "publicip" {
  #you need cart or catalogue you can use like this 
  value = aws_instance.anji["cart"].public_ip
  #you need both means you can use this 
  value = {
    for k, v in aws_instance.anji : k => v.public_ip
  }
}
the best practice in terraform use module dont change your every time resource use .tfvars or variables . use always iterate modules not resources

124-types of provisioners in terraform

1-file
2-local-exec
3-remote-exec :-dont use remote-execc provisioners in inside resource if nay case provisioners failed that will recreate the instance onemore time and rerun when you apply . its not good practice to use
provisioners in resource under use null resource then use remote-exec  
>>if you use null resource it does nothing to do you can use null_resource to provision that will help you because if any changes that null resource will execute other wise wont execute . you can trigger 
>>for example you created the instance and null_resource using provision then it will executed some scenario mu system has terminated while rerun the code it will not execute it will run only and create the
instance only if you want trigger or execute you need to use trigger when to trigger what conditions to trigger any ip changes or instance_id changes  

resource "aws_instance" "anji" {
  ami = shgddvnksjvk
  instance_type = t2.micro 

  tags = {
    Name = "anji"
  }
  #dont use here instead you can use after see the below 
  provisioners "remote-exec" {
    connection {

    }
  } 
}

resource "null_resource" "provision" {
  #if anytime your instance restart or instance_id changes then this trigger will execute the provisioners 
   triggers = {
    instance_id = aws_instance.anji.id 
   }

    provisioners "remote-exec" {
    connection {
      host    = aws_instance.anji.public_ip 
      user    =
      password = 
    }

    inline = [
      "yum update -y",
      "yum install nginx -y"
    ]
  } 
}

125-how can you revert a commit without creating a new commit 

>>git reset can be used to revert a commit without creating a new commit 

126-what is module in terrafrom and types of modules 

>>module is nothing but your going to call group of .tf files its is nothing but module 

1-root module :- your going to call all .tf file from the root location or folder so then it is called root module 
2-child module :- your going to call other modules other modules in the sense from root location to call other modules 
3-publish module :- your going to call any registory either public or private  

127-create a load balancer target group 

resource "aws_lb_target_group" "anji" {
  name     = "anji-target-group"
  protocol = "HTTP"
  port     = 80

  vpc_id = aws_vpc.main.id

  connection_draining {
    timeout_seconds = 30
  }

  health_check {
    interval        = 30
    timeout         = 5
    healthy_threshold = 2
    unhealthy_threshold = 5

    path                = "/"
    port                = 80
    protocol            = "HTTP"
    matcher {
      http_code = "200,301,302"
    }
  }

  # Replace with the actual list of your targets
  target {
    id      = aws_instance.anji_server1.private_ip
    port    = 80
    weight  = 50
  }

  # Add more target blocks as needed
}

resource "aws_lb_target_group" "sampi" {
  name     = "sampi-target-group"
  protocol = "HTTP"
  port     = 80

  vpc_id = aws_vpc.main.id

  connection_draining {
    timeout_seconds = 30
  }

  health_check {
    interval        = 30
    timeout         = 5
    healthy_threshold = 2
    unhealthy_threshold = 5

    path                = "/"
    port                = 80
    protocol            = "HTTP"
    matcher {
      http_code = "200,301,302"
    }
  }

  # Replace with the actual list of your targets
  target {
    id      = aws_instance.sampi_server1.private_ip
    port    = 80
    weight  = 50
  }

  # Add more target blocks as needed
}


resource "aws_lb" "app_lb" {
  name         = "my-app-lb"
  security_groups = [aws_security_group.lb_sg.id]
  subnets        = aws_subnet.public_subnets.*.id

  internal = false

  listener {
    port    = 80
    protocol = "HTTP"

    default_action {
      target_group_arn = aws_lb_target_group.anji.arn
      weight           = 50
    }

    default_action {
      target_group_arn = aws_lb_target_group.sampi.arn
      weight           = 50
    }
  }
}

>>docker run -d -p 82:80 --name anji nginx:v1

128-what is terraform workflows

1-terraform init 
2-terraform validate 
3-terraform plan 
4-apply 
5-destroy 

129-how to comment in terraform

instance_type = t2.micro //this comment either you can use // or #

/* 
this is multiline comments 
you can 
use 
*/

130-what is terraform block in terraform

>>earlier 0.14 version we dont have this this special block used configure some bheaviours . this block can be called in 3 ways 1-terraform block 2-terraform settings block 3-terraform configuration 
>>required version 
>>list of required providers 
>>terraform backend 

terraform {
  required_version = "~> 1.5.7"  ##it will allow 1.5.8 , 1.5.9 and deny 1.6.0, 1.6.1 like this 
  required_providers {
    aws = {
      source = "hashicorp/aws"
        version = "3.47.0"
      version = ">=3.47.0"   ##it will use latest version its not recomended
      version = "<=3.47.0"   ##it will use 3.47.0 below version #its also not recomended
      version = "~>3.0"  ##it will support like this 3.1.0 , 3.2.1   #this is also not recomended
      version = "~>3.47.0"   ##any one in the range 3.47.99 something it will not go or work this up versions 3.48.0 
    }        
  }     
}

aws configure --profile anji(prod)
access_key: *************************
secret_key: *************************
region: *****************

provider "aws" {
  profile = "default" 
  profile = "anji(prod)"  ##if you have multiple access and secret keys means you can choose your profile like this 
  region  = "us-east-1"

}
>>if you install externsion then ctrl + space you will get recomended options accordingly
>>always choose or use offical or verified only dont use community or archived until your need use but modify accroding to you 
>>why iam saying if you use any bug or outdated you will face issue in production

131-when the .tfstate file created

>>it will create the when you applying the terraform code like this 
>>terraform plan apply 
>>terraform apply --auto-approve 
>>terraform plan destroy 

132-what is desired state and current state in terraform

>>desired state is nothing but my configuration files what ever i have define terraform configuration files 
>>current state is going my realworld ec2 or realworld resource on the remote systems

133-how many ways to pass the variables in terraform

>>we can send 10 ways to terraform variables
1-Input variables
2-provide input variables when prompted during terraform plan or apply 
3-override variable values using cli argumnet  -var 
4-provide input variables using terraform.tfvars files 
5-override default variables values using enviorment variables
6-provide input variables using anyname.tfvars   terraform --var-file=prod.tfvars 
7-provide input variables using vpc.auto.tfvars files ##your using like this terraform --var-file=prod.tfvars if you put auto no need to mention like this terraform --var-file=prod.tfvars  terraform apply automatically execute 
8-implement custom validation rules in variables
9-implement complex type constructors like LIst & Map in input variables
10-protect sensitive input variables

134-how to take custom ami id of your aws to terraform

>>Data souce allow data to be fetched or computed for use elsewhere in terraform configuration
>>use of data sources allows a terraform configuration to make use of information defined outside or terraform, or defined by another separate terraform configuration 

provider "aws" {
  region = "your_aws_region"
}

data "aws_ami" "anji_test" {
  most_recent = true 
  name_regex  = "anji-test"
  owner       = "695696876723"
}

resource "aws_instance" "example" {
  ami           = data.aws_ami.anji_test.id
  instance_type = "t2.micro"

  # Other instance configuration settings go here

  tags = {
    Name = "example-instance"
    # Add other tags as needed
  }
}

##taking amazon image latest 

provider "aws" {
  region = "your_aws_region"
}

data "aws_ami" "amazon_linux" {
  most_recent = true
  owners      = ["amazon"]
  filter {
    name = "name"
    values = ["amzn2-ami-hvm-*-gp2"]
  }
  filter {
    name = "root-device"
    values = ["ebs"]
  }
  filter {
    name = "virtualization-type"
    values = ["hvm"]
  }
  filter {
    name = "architecture"
    values = ["x86_64"]
  }  
}

135-can i use the backend file one project to anothe project 

>>yes you can see video of 5 section 4 kalyan reddy 

136-how to use list variables in terraform

variable "instance_type_list" {
  description = "Ec2 Instance Type"
  type = list(string)
  default = ["t3.micro", "t3.small", "t2.large"]  #it will start 0 , 1 , 2
}

resource "aws_instance" "myec2" {
  ami = data.aws_ami.amzlinux2.id 
  instance_type = var.instance_type_list["0"]
 }


137-how to use map variables in terraform

variable "instance_type_list" {
  description = "Ec2 Instance Type"
  type = map(string)
  default = {
    dev = "t3.micro"
    prod = "t3.small" 
    test = "t2.large"
  } 
}

resource "aws_instance" "myec2" {
  count = 5
  ami = data.aws_ami.amzlinux2.id 
  instance_type = var.instance_type_list["prod"]
}

138-how to use for loop in terraform give example using 

variable "instance_type_list" {
  description = "Ec2 Instance Type"
  type = list(string)
  default = ["t3.micro", "t3.small", "t2.large"]  #it will start 0 , 1 , 2
}

resource "aws_instance" "myec2" {
  count = 5
  ami = data.aws_ami.amzlinux2.id 
  instance_type = var.instance_type_list["0"]
}

output "get_dns_name" {
  value = [for instance in aws_instance.myec2: instance.public_dns] ##it will get five instances details

}

#map 

variable "instance_type_list" {
  description = "Ec2 Instance Type"
  type = map(string)
  default = {
    dev = "t3.micro"
    prod = "t3.small" 
    test = "t2.large"
  } 
}

resource "aws_instance" "myec2" {
  count = 5
  ami = data.aws_ami.amzlinux2.id 
  instance_type = var.instance_type_list["prod"]
}

output "get_dns_name" {
  value = {for instance in aws_instance.myec2: instance.id[prod] =>instance.public_dns} ##it will get five instances details
}

#latest 

output"latest_splat_syntax" {
  value = aws_instance.myec2[*].public_dns ##all system deatils #use instead of this .*. use this [*]
  value = aws_instance.myec2[0].public_dns  ##it will atke first instance detail only 
}

139-how to use for_each in terraform 

variable "component" {
  default = {
    cart = {
      Name          =  "cart"
      instance_type =  "t3.micro"  
    }
    catalogue = {
      Name          = "catalogue"
      instance_type = "t2.micro"
    }
  }
}

resoures "aws_instance" "anji" {
  # it will count the components how many and creat that much instead of using count you can use for_each
  for_each          = var.components
  ami               = hja0ijka-0np0
  #it will take the components inside cart will t3.micro and catalogue will be t2.micro 
  instance_type     = each.value.instance_type 

  tags = {
    Name = each.value["name"] #use this also work each.value.name 
  }
}

output "publicip" {
  #you need cart or catalogue you can use like this 
  value = aws_instance.anji["cart"].public_ip
  #you need both means you can use this 
  value = {
    for k, v in aws_instance.anji : k => v.public_ip
  }
}
the best practice in terraform use module dont change your every time resource use .tfvars or variables . use always iterate modules not resources

140-is it possible to check any region support kind of ec2 instance types like t2.micro or t3.micro 

>>yes you can do this using below code 

data "aws_ec2_instance_type_offerings" "anji" {
  filter {
    name = "instance_type"
    values = [t3.micro]
  }
  filter {
    name = "location"
    values = ["us-east-1a"]
    values = ["us-east-1b"]
  }
  location_type = "availability-zone"
}

output "output_v1_1" {
  value = data.aws_ec2_instance_type_offerings.anji.instance_type
}

2nd-
data "aws_ec2_instance_type_offerings" "anji" {
  for_each = toset([ "us-east-1a", "us-east-1b", "us-east-1c"])  ##you can use this type also for_each only support map only   
  filter {
    name = "instance_type"
    values = [t3.micro]
  }
  filter {
    name = "location"
    values = [each.value]

  }
  location_type = "availability-zone"
}

# Get List of Availability Zones in a Specific Region
# Region is set in c1-versions.tf in Provider Block
# Datasource-1
data "aws_availability_zones" "my_azones" {
  filter {
    name   = "opt-in-status"
    values = ["opt-in-not-required"]
  }
}

# Check if that respective Instance Type is supported in that Specific Region in list of availability Zones
# Get the List of Availability Zones in a Particular region where that respective Instance Type is supported
# Datasource-2
data "aws_ec2_instance_type_offerings" "my_ins_type" {
  for_each = toset(data.aws_availability_zones.my_azones.names)
  filter {
    name   = "instance-type"
    values = ["t3.micro"]
  }
  filter {
    name   = "location"
    values = [each.key]
  }
  location_type = "availability-zone"
}


# Output-1
# Basic Output: All Availability Zones mapped to Supported Instance Types
output "output_v3_1" {
  value = {
    for az, details in data.aws_ec2_instance_type_offerings.my_ins_type: az => details.instance_types
  }
}

# Output-2
# Filtered Output: Exclude Unsupported Availability Zones
output "output_v3_2" {
  value = {
    for az, details in data.aws_ec2_instance_type_offerings.my_ins_type: 
    az => details.instance_types if length(details.instance_types) != 0 }
}

# Output-3
# Filtered Output: with Keys Function - Which gets keys from a Map
# This will return the list of availability zones supported for a instance type
output "output_v3_3" {
  value = keys({for az, details in data.aws_ec2_instance_type_offerings.my_ins_type: 
    az => details.instance_types if length(details.instance_types) != 0 })
}


# Output-4 (additional learning)
# Filtered Output: As the output is list now, get the first item from list (just for learning)
output "output_v3_4" {
  value = keys({
    for az, details in data.aws_ec2_instance_type_offerings.my_ins_type: 
    az => details.instance_types if length(details.instance_types) != 0 })[0]
}

# EC2 Instance
resource "aws_instance" "myec2vm" {
  ami = data.aws_ami.amzlinux2.id
  instance_type = var.instance_type
  user_data = file("${path.module}/app1-install.sh")
  key_name = var.instance_keypair
  vpc_security_group_ids = [ aws_security_group.vpc-ssh.id, aws_security_group.vpc-web.id   ]
  # Create EC2 Instance in all Availabilty Zones of a VPC  
  #for_each = toset(data.aws_availability_zones.my_azones.names)
  for_each = toset(keys({for az, details in data.aws_ec2_instance_type_offerings.my_ins_type: 
    az => details.instance_types if length(details.instance_types) != 0 }))
  availability_zone = each.key # You can also use each.value because for list items each.key == each.value
  tags = {
    "Name" = "For-Each-Demo-${each.key}"
  }
}

#best example for for_each in map 

variable "availability_zone_instance_types" {
  type = map(string)
  default = {
    "us-east-1a" = "t2.micro",
    "us-east-1b" = "t3.micro",
    "us-east-1c" = "t3.large"
  }
}

resource "aws_vpc" "example_vpc" {
  cidr_block = "10.0.0.0/16"
  # Add other VPC configurations as needed...
}

resource "aws_subnet" "example_subnet" {
  for_each = var.availability_zone_instance_types

  vpc_id                  = aws_vpc.example_vpc.id
  cidr_block              = cidrsubnet(aws_vpc.example_vpc.cidr_block, 8, each.key)
  availability_zone       = each.key
  map_public_ip_on_launch = true  # Adjust this based on your requirements
  # Add other subnet configurations as needed...
}

resource "aws_instance" "example_instance" {
  for_each = var.availability_zone_instance_types

  ami           = "ami-xxxxxxxxxxxxxxxx"  # Replace with your desired AMI ID
  instance_type = each.value
  subnet_id     = aws_subnet.example_subnet[each.key].id

  # Add other instance configurations as needed...
}

or 

variable "availability_zone_instance_types" {
  type = map(object({
    public_subnet_cidr  = string
    private_subnet_cidr = string
    instance_type       = string
  }))

  default = {
    "us-east-1a" = {
      public_subnet_cidr  = "10.0.1.0/24",
      private_subnet_cidr = "10.0.2.0/24",
      instance_type       = "t2.micro",
    },
    "us-east-1b" = {
      public_subnet_cidr  = "10.0.3.0/24",
      private_subnet_cidr = "10.0.4.0/24",
      instance_type       = "t3.micro",
    },
    "us-east-1c" = {
      public_subnet_cidr  = "10.0.5.0/24",
      private_subnet_cidr = "10.0.6.0/24",
      instance_type       = "t3.large",
    },
  }
}

resource "aws_vpc" "example_vpc" {
  cidr_block = "10.0.0.0/16"
  # Add other VPC configurations as needed...
}

resource "aws_subnet" "public_subnet" {
  for_each = var.availability_zone_instance_types

  vpc_id                  = aws_vpc.example_vpc.id
  cidr_block              = each.value.public_subnet_cidr
  availability_zone       = each.key
  map_public_ip_on_launch = true
  # Add other public subnet configurations as needed...
}

resource "aws_subnet" "private_subnet" {
  for_each = var.availability_zone_instance_types

  vpc_id                  = aws_vpc.example_vpc.id
  cidr_block              = each.value.private_subnet_cidr
  availability_zone       = each.key
  # Add other private subnet configurations as needed...
}

resource "aws_instance" "example_instance" {
  for_each = var.availability_zone_instance_types

  ami           = "ami-xxxxxxxxxxxxxxxx"  # Replace with your desired AMI ID
  instance_type = each.value.instance_type

  subnet_id = aws_subnet.private_subnet[each.key].id  # Deploy instances in private subnets

  # Add other instance configurations as needed...
}

#best example for for_each in list 

variable "availability_zones" {
  type    = list(string)
  default = ["us-east-1a", "us-east-1b", "us-east-1c"]
}

variable "instance_types" {
  type    = list(string)
  default = ["t2.micro", "t3.micro", "t3.large"]
}

resource "aws_vpc" "example_vpc" {
  cidr_block = "10.0.0.0/16"
  # Add other VPC configurations as needed...
}

resource "aws_subnet" "public_subnet" {
  for_each = {
    for idx, az in var.availability_zones : idx => az
  }

  vpc_id                  = aws_vpc.example_vpc.id
  cidr_block              = cidrsubnet(aws_vpc.example_vpc.cidr_block, 8, each.value)
  availability_zone       = each.value
  map_public_ip_on_launch = true
  # Add other public subnet configurations as needed...
}

resource "aws_subnet" "private_subnet" {
  for_each = {
    for idx, az in var.availability_zones : idx => az
  }

  vpc_id     = aws_vpc.example_vpc.id
  cidr_block = cidrsubnet(aws_vpc.example_vpc.cidr_block, 8, each.value)
  availability_zone = each.value
  # Add other private subnet configurations as needed...
}

resource "aws_instance" "example_instance" {
  for_each = {
    for idx, instance_type in var.instance_types : idx => instance_type
  }

  ami           = "ami-xxxxxxxxxxxxxxxx"  # Replace with your desired AMI ID
  instance_type = each.value

  subnet_id = aws_subnet.private_subnet[each.key].id  # Deploy instances in private subnets

  # Add other instance configurations as needed...
}


141-How to make a decision of using the public Registry module?

Understand about Terraform Registry and Modules
We are going to use a VPC Module from Terraform Public Registry
Understand about Authenticity of a module hosted on Public Terraform Registry with HashiCorp Verified Tag
Review the download rate for that module
Review the latest versions and release history of that module
Review our feature needs when using that module and ensure if our need is satisfied use the module else use the standard terraform resource definition appraoch.
Review module inputs, outputs and dependencies too.

142-


#this is block 
terraform {
  ##this is argumnet example tags = {}
  tags = {

  }
}

143-can i use concat instaed anyone in terraform

>>you can use locals for instead of concat for attaching + naming names 

144-iam used multiple modules i have multiple tf.vars how we can execute single command

>>vpc.auto.tfvars 
>>instance.auto.tfvars 
>>security.auto.tfvars  

>>like this you have many is there your using modules and need to execute the same your using auto.tfvars it will take automatically . if you use anji.tfvars or prod.tfvars you need to mention or else your
segergated the code
like this vpc.auto.tfvars only vpc deatils and instance.auto.tfvars only instance deatils like this any isses happen or you can change your code easily going to .tfvars and you can change accroding to you 

>>and your using multiple modules in single file or difference file you need to create the output.tf need to refer in any other module if you want use compulsary is needed either wise you will face so many
issue

145-where you cna see the logs will createing or booting in cloud aws 

>>sudo su -
>>cd /var/log 
>>cat cloud-init-output.log   -->here you can see all the logs 

146-what is trimsuffix in terraform what is the use 

>>if you have internal domain anji.com. or nay thing you can use trim that last . using trimsuffix

>>
module "acm" {
  source = ""
  version = ""
  domain_name = trimsuffix("anji.com.", ".")  ##like this you can use 
 
} 

147-how to get the metric details like need to use metric name reference in terraform

>>you can go the cloudwatch click on metric explorer select which service you want write that in terraform like this 

resource "aws_cloudwatch_metric_alarm" "cpu_utilization" {
  depends_on          = [aws_autoscaling_group.example]
  alarm_name          = "example-cpu-utilization"
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = 300  # 5 minutes
  statistic           = "Average"
  threshold           = 50  # Set your threshold value

}

 metric_name         = "CPUUtilization"
 metric_name         = "HTTPCode_ELB_5XX_Count"
 metric_name         = "HTTPCode_ELB_502_Count"
 metric_name         = "TargetResponseTime"
 metric_name         = "RequestCount"

148-when iam creating s3 bucket using terraform i have some data in s3 bucket delete with terraform using only how we can achive this 

>>when you creating s3 bucket if you mention 

resource "aws_s3_bucket" "mywebsite" {
  bucket = var.bucket_name  
  tags          = var.tags
  force_destroy = true
}

149-what happens if two members in executing same state file in terraform

>>if two members are running terraform at the same time you may run into race conditions as multiple terraform processes make concurrent  updates to the state files, leading to conflicts, data loss, and state
file corruption 
so that why we use state locking in terraform using dynmodb using 
>>not all backeds support state locking aws s3 support state locking
>>when any user executing terraform automatically that state lock until the process complete
>>when you run it will state lock if you complete the release unlock

150-if you have anji.tfstate file under vpc only you have to use this in vpc details in another user another modules to get the details and another sampi.tfstaste need to create is it possibe 

>>yes its possible you have anji.tfstaste under vpc details only so you need to create the alb, secuirty group, ec2 instances need to create this using anji.tfstaste and need to create another sampi.tfstaste
need to create for alb, secutity groups, and ec2 

#this is used to fetch the vpc details in from s3 bucket and pass the details to alb, security groups, ec2 
data "terraform_remote_state" "vpc" {
  backed = "s3"
  config = {
    bucket = anji5413
    key = anji-dev-vpc/anji.tfstaste
    region = us-east-1
  }
}
>>create output for anji.tfstate 

#createing one more sampi.tfstaste for this alb, security groups, ec2 

backed "s3" {
  bucket = "sampi5413"
  key = sampi-dev-ec2/sampi.tfstaste
  region = "us-east-1"

  dynmodb_table = "dev-project2-ec2" 
}

data.terraform_remote_state.vpc.outputs.vpc_id 

resource "aws_security_group" "internet-load" {
  name        = "internet-load"
  description = "Allow all inbound traffic"
  vpc_id      = "data.terraform_remote_state.vpc.outputs.vpc_id"
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  egress {
    from_port       = 0
    to_port         = 0
    protocol        = "-1"
    cidr_blocks     = ["0.0.0.0/0"]
    }
}
>>so first get the data from anji bucket like vpc details using this we can create the alb , security groups and ec2 

##can use local file also here is example for kubernetes the cluster is created then iam using that cluster as datasource then executing commands via terraform

terrafrom {
  required_version = ">= 1.0.0"
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "~> 3.7.0"
    }
    kubernetes = {
      source = "hasicorp/kubernetes"
      version = "~> 2.7"
    }
  }
}

data "terraform_remote_state" "eks" {
  backend = "local"
  config = {
    path = "../../anji/anji.tfstate"  ##you have to give the location where the .tfstate file is there 
  }
} 

provider "aws" {
  region = "us-east-1"

}

#host name and cluster_ca_certificate and token this will be need to write in output in project1 then use this project2  
#then that will be taking from here as a data source then that source will be given to provider of kubernetes

data "aws_eks_cluster" "cluster" {
  name = data.terraform_remote_state.eks.outputs.cluster_id  ##the output will be written in project1 name as cluster_id 
}

data "aws_eks_cluster_auth" "cluster" {
  name = data.terraform_remote_state.eks.outputs.cluster_id  ##the output will be written in project1 name as cluster_id 
}

#connect to your kubernetes server 

provider "kubernetes" {
  host = "data.terraform_remote_state.eks.outputs.cluster_endpoint"
  clusterca_certificate =  "base64decode(data.terraform_remote_state.eks.outputs.cluster_certificate_authority_data)"
  token = data.aws_eks_cluster_auth.cluster.token 
}

151-argumnet_refernce and attribute_refernce are both are similar have some changes 
meta argumnets are count, for_each and provider, lifecycle 

152-what are the terraform blocks 

1-provider block 
2-resoures block 
3-input variables block 
4-output values block 
5-local values block 
6-data source block 
7-modules block 

153-what are terraform top blocks 

1-fundamentals block:-
>>terraform block :- this block can called in 3 ways 1-terraform block 2-terraform stemctl block 3-terraform configuration block and each terraform block can contain a number of settings related to terraform's
behavior.
>>providers block
>>Resource block 
2-Variable block 
>>input variables block 
>>output values block 
>>local values block 
3-calling or Refernce block 
>>data source block 
>>modules block 

154-when the terraform.tfstaste file will create 

>>when you apply the command in terraform. once the terraform execution is complete then that will create automaticlaly

155-is this possible to exclude the any availability_zone using terraform

>>yes its possible you can do 

variable "availability_zone_subnet_instances" {
  type = map(object({
    public_subnet_cidr  = string
    private_subnet_cidr = string
    instance_type       = string
  }))

  default = {
    "us-east-1a" = {
      public_subnet_cidr  = "10.0.1.0/24",
      private_subnet_cidr = "10.0.2.0/24",
      instance_type       = "t2.micro",
    },
    "us-east-1b" = {
      public_subnet_cidr  = "10.0.3.0/24",
      private_subnet_cidr = "10.0.4.0/24",
      instance_type       = "t3.micro",
    },
    "us-east-1c" = {
      public_subnet_cidr  = "10.0.5.0/24",
      private_subnet_cidr = "10.0.6.0/24",
      instance_type       = "t3.large",
    },
    "us-east-1d" = {
      public_subnet_cidr  = "10.0.7.0/24",
      private_subnet_cidr = "10.0.8.0/24",
      instance_type       = "t3.micro",
    },
  }
}

resource "aws_vpc" "example_vpc" {
  cidr_block = "10.0.0.0/16"
  # Add other VPC configurations as needed...
}

resource "aws_subnet" "public_subnet" {
  for_each = var.availability_zone_subnet_instances

  vpc_id                  = aws_vpc.example_vpc.id
  cidr_block              = each.value.public_subnet_cidr
  availability_zone       = each.key
  map_public_ip_on_launch = true
  # Add other public subnet configurations as needed...
}

resource "aws_subnet" "private_subnet" {
  for_each = var.availability_zone_subnet_instances

  vpc_id                  = aws_vpc.example_vpc.id
  cidr_block              = each.value.private_subnet_cidr
  availability_zone       = each.key
  # Add other private subnet configurations as needed...
}

resource "aws_instance" "example_instance" {
  for_each = {
    for az, subnet_info in var.availability_zone_subnet_instances : az => subnet_info
    if az != "us-east-1d" # Exclude us-east-1d
  }

  ami           = "ami-xxxxxxxxxxxxxxxx"  # Replace with your desired AMI ID
  instance_type = each.value.instance_type

  subnet_id = aws_subnet.private_subnet[each.key].id  # Deploy instances in private subnets

  # Add other instance configurations as needed...
}
